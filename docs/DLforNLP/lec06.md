# üìò Lecture 6 ‚Äî Machine Translation, Seq2Seq, and Attention (2024W)

Thomas Lukasiewicz  
Based on slides by Abigail See

---

## 1) –ò—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç: Machine Translation (MT)

MT ‚Äî –∑–∞–¥–∞—á–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–µ—Ä–µ–≤–æ–¥–∞ —Ç–µ–∫—Å—Ç–∞ –º–µ–∂–¥—É —è–∑—ã–∫–∞–º–∏.

### –û—Å–Ω–æ–≤–Ω—ã–µ —ç—Ä—ã

| –ü–µ—Ä–∏–æ–¥ | –ú–µ—Ç–æ–¥ | –û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ |
|---|---|---|
1950s | Early rule-based MT | —Å–ª–æ–≤–∞—Ä–∏ + –≥—Ä–∞–º–º–∞—Ç–∏–∫–∞, —Ä—É—á–Ω—ã–µ –ø—Ä–∞–≤–∏–ª–∞ |
1990s‚Äì2010s | **Statistical Machine Translation (SMT)** | –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–Ω—ã–µ –º–æ–¥–µ–ª–∏ + alignment |
2014‚Äì... | **Neural Machine Translation (NMT)** | seq2seq, attention, transformers |

---

## 2) Pre-Neural MT (–¥–æ –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π)

### –ü—Ä–∞–≤–∏–ª—å–Ω—ã–π –≤—ã–≤–æ–¥:
–ú–¢ –Ω–∞—á–∞–ª–∞—Å—å –¥–∞–≤–Ω–æ ‚Üí –ø—Ä–æ–±–ª–µ–º–∞ —Å–ª–æ–∂–Ω–∞—è ‚Üí —ç–≤–æ–ª—é—Ü–∏—è –º–æ–¥–µ–ª–µ–π –ø–æ—Ç—Ä–µ–±–æ–≤–∞–ª–∞ –¥–µ—Å—è—Ç–∏–ª–µ—Ç–∏–π.

---

## 3) SMT (Statistical Machine Translation)

–ò–¥–µ—è:  
–±—É–¥–µ–º —É—á–∏—Ç—å—Å—è –ø–µ—Ä–µ–≤–æ–¥–∏—Ç—å –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –≤ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –∫–æ—Ä–ø—É—Å–∞—Ö.

### –û—Å–Ω–æ–≤–Ω–∞—è —Ñ–æ—Ä–º—É–ª–∞ SMT
(–∏–∑ Brown et al., 1993)

\[
\hat{e} = \arg\max_e P(e|f) = \arg\max_e P(f|e)P(e)
\]

- \(f\) ‚Äî —Ñ—Ä–∞–∑–∞ –Ω–∞ foreign —è–∑—ã–∫–µ (source)
- \(e\) ‚Äî candidate –ø–µ—Ä–µ–≤–æ–¥–∞ (target)
- \(P(e)\) ‚Äî language model (LM)
- \(P(f|e)\) ‚Äî translation model (TM)

**–ò–Ω—Ç—É–∏—Ü–∏—è:**  
–≤—ã–±–∏—Ä–∞–µ–º –ø–µ—Ä–µ–≤–æ–¥, –∫–æ—Ç–æ—Ä—ã–π:
- –≤–µ—Ä–æ—è—Ç–µ–Ω –∫–∞–∫ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ (LM)
- –≤–µ—Ä–æ—è—Ç–µ–Ω –∫–∞–∫ –ø–µ—Ä–µ–≤–æ–¥ (TM)

---

## 4) –ß—Ç–æ —Ç–∞–∫–æ–µ alignment

- alignment = —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —Å–ª–æ–≤/—Ñ—Ä–∞–∑ source ‚Üí target
- –Ω–µ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –æ–¥–∏–Ω-–∫-–æ–¥–Ω–æ–º—É
- –ø–æ—Ä—è–¥–æ–∫ —Å–ª–æ–≤ –º–æ–∂–µ—Ç –æ—Ç–ª–∏—á–∞—Ç—å—Å—è

–ü—Ä–∏–º–µ—Ä:
EN: I love cats
FR: J' aime les chats

I ‚Üî J'
love ‚Üî aime
cats ‚Üî chats


---

## 5) –ü—Ä–æ–±–ª–µ–º—ã SMT

| –ü—Ä–æ–±–ª–µ–º–∞ | –ü–æ—è—Å–Ω–µ–Ω–∏–µ |
|---|---|
–ú–Ω–æ–≥–æ —Ä—É—á–Ω–æ–≥–æ —Ç—Ä—É–¥–∞ | —Ñ—Ä–∞–∑—ã, —Å–ª–æ–≤–∞—Ä–∏, —ç–≤—Ä–∏—Å—Ç–∏–∫–∏ |
–û–≥—Ä–æ–º–Ω–∞—è —Å–ª–æ–∂–Ω–æ—Å—Ç—å | combinatorial explosion |
–ù–∞–∫–æ–ø–ª–µ–Ω–∏–µ –æ—à–∏–±–æ–∫ | LM + TM + decoder |
–û–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–∞—è –≥–∏–±–∫–æ—Å—Ç—å | –ø–ª–æ—Ö–æ —É—á–∏—Ç—Å—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞–º —è–∑—ã–∫–∞ |
–ù–µ–æ–±–æ–±—â–∞–µ–º–æ—Å—Ç—å | —Ç—Ä–µ–±—É–µ—Ç —Ö–∞–∫–æ–≤ –¥–ª—è –º–æ—Ä—Ñ–æ–ª–æ–≥–∏–∏, reorderings |

**–í—ã–≤–æ–¥:**  
SMT = –º–æ—â–Ω–æ, –Ω–æ —Å–ª–∏—à–∫–æ–º —Å–ª–æ–∂–Ω–æ —Ä—É–∫–∞–º–∏ –ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞—Ç—å.

---

## 6) –ü–µ—Ä–µ—Ö–æ–¥ –∫ Neural MT

–ò–¥–µ—è NMT (Sutskever et al., 2014; Bahdanau et al., 2015):

- end-to-end –æ–±—É—á–µ–Ω–∏–µ
- –µ–¥–∏–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –≤—Å–µ–≥–æ –ø—Ä–æ—Ü–µ—Å—Å–∞
- —É—á–∏—Ç –∏ translation model, –∏ language model –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ
- —É—á–∏—Ç alignment (attention)

---

## 7) Seq2Seq recap (–±–µ–∑ attention)

–ö–ª–∞—Å—Å–∏—á–µ—Å–∫–∞—è —Å—Ö–µ–º–∞ NMT 2014:

**Encoder RNN**
- —á–∏—Ç–∞–µ—Ç –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ source
- –ø–æ–ª—É—á–∞–µ—Ç —Ñ–∏–Ω–∞–ª—å–Ω–æ–µ —Å–∫—Ä—ã—Ç–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ \(h_T\)

**Decoder RNN**
- –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç—Å—è \(h_T\)
- –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç target —Å–ª–æ–≤–æ –∑–∞ —Å–ª–æ–≤–æ–º

–°—Ö–µ–º–∞:

input ‚Üí [Encoder RNN] ‚Üí context vector ‚Üí [Decoder RNN] ‚Üí output


---

## 8) Training NMT

- teacher forcing
- minimize cross-entropy
- batch SGD / Adam
- padding / masking
- (–ø–æ–∑–∂–µ) beam search –¥–ª—è inference

---

## 9) Greedy decoding

–í –¥–µ–∫–æ–¥–µ—Ä–µ –Ω–∞ –∫–∞–∂–¥–æ–º —à–∞–≥–µ –≤—ã–±–∏—Ä–∞–µ–º —Å–ª–æ–≤–æ:

\[
\hat{y}_t = \arg\max p(y_t | y_{<t}, x)
\]

–ü–ª—é—Å—ã:
- –ø—Ä–æ—Å—Ç–æ–π, –±—ã—Å—Ç—Ä—ã–π

–ú–∏–Ω—É—Å—ã:
- **–ª–æ–∫–∞–ª—å–Ω–æ –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π** –≤—ã–±–æ—Ä = —á–∞—Å—Ç–æ –ø–ª–æ—Ö–æ–π –≥–ª–æ–±–∞–ª—å–Ω—ã–π –ø–µ—Ä–µ–≤–æ–¥

---

## 10) –ü–æ—á–µ–º—É greedy –ø–ª–æ—Ö–æ

–ü—Ä–∏–º–µ—Ä:

Model options at step 1:  
"the" (0.40), "a" (0.38), "this" (0.22)


Greedy –≤—ã–±–µ—Ä–µ—Ç "the", –Ω–æ –ø—É—Ç—å —Å "a" –º–æ–∂–µ—Ç –ø—Ä–∏–≤–µ—Å—Ç–∏ –∫ –ª—É—á—à–µ–º—É –∏—Ç–æ–≥–æ–≤–æ–º—É –ø–µ—Ä–µ–≤–æ–¥—É.  
‚Üí –ö–æ–º–±–∏–Ω–∞—Ç–æ—Ä–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å ‚Äî –Ω—É–∂–µ–Ω –ø–æ–∏—Å–∫.

---

## 11) –ü–æ–ª–Ω—ã–π –ø–µ—Ä–µ–±–æ—Ä (exhaustive search)

–¢–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏: —Ä–∞—Å—Å–º–æ—Ç—Ä–µ—Ç—å –≤—Å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è.  
–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏: **–Ω–µ–≤–æ–∑–º–æ–∂–Ω–æ**, —ç–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–π —Ä–æ—Å—Ç.

---

## 12) Beam Search (–æ—Å–Ω–æ–≤–∞ NMT inference)

–ë–∞–ª–∞–Ω—Å:
- —Å–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–æ–ø-k –≥–∏–ø–æ—Ç–µ–∑ –≤–º–µ—Å—Ç–æ –æ–¥–Ω–æ–π

\[
k = \text{beam size} \quad (–æ–±—ã—á–Ω–æ 4‚Äì10)
\]

–ò–¥–µ—è:
- –Ω–∞ –∫–∞–∂–¥–æ–º —à–∞–≥–µ –æ—Å—Ç–∞–≤–ª—è–µ–º k –ª—É—á—à–∏—Ö —á–∞—Å—Ç–∏—á–Ω—ã—Ö –ø–µ—Ä–µ–≤–æ–¥–æ–≤
- –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º –∏—Ö —Ä–∞–∑–≤–∏–≤–∞—Ç—å

–î–∞—ë—Ç:
- –ª—É—á—à–µ, —á–µ–º greedy
- –Ω–∞–º–Ω–æ–≥–æ –±—ã—Å—Ç—Ä–µ–µ –ø–æ–ª–Ω–æ–≥–æ –ø–µ—Ä–µ–±–æ—Ä–∞

---

## 13) Beam Search —Å—Ö–µ–º–∞

Step 1: choose top k first tokens  
Step 2: expand each  
Step 3: keep top k combined  
...  
Stop when <EOS> reached  


---

## 14) Beam width trade-off

| –ú–∞–ª—ã–π beam | –ë–æ–ª—å—à–æ–π beam |
|---|---|
–±—ã—Å—Ç—Ä–µ–µ | –º–µ–¥–ª–µ–Ω–Ω–µ–µ |
—Ö—É–∂–µ –∫–∞—á–µ—Å—Ç–≤–æ | –ª—É—á—à–µ |
–º–æ–∂–µ—Ç –ø—Ä–æ–ø—É—Å—Ç–∏—Ç—å –ª—É—á—à–∏–π –ø—É—Ç—å | diminishing returns |

beam > 10 —Ä–µ–¥–∫–æ –ø–æ–º–æ–≥–∞–µ—Ç

---

## 15) –û—Ü–µ–Ω–∫–∞ NMT ‚Äî BLEU

BLEU (Papineni et al., 2002) = n-gram precision + penalty for too-short outputs

**–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ (–∏–∑ —Å–ª–∞–π–¥–æ–≤):**

\[
p_n = \frac{\# \text{matched n-grams}}{\# \text{candidate n-grams}}
\]

–í–µ—Å:
\[
w_i = \frac{1}{2^i}
\]

Brevity penalty:
\[
\beta = e^{\min(0, 1 - \frac{len_{ref}}{len_{MT}})}
\]

BLEU:
\[
BLEU = \beta \prod_{i=1}^{k} p_i^{w_i}
\]

–ò–Ω—Ç—É–∏—Ü–∏—è:
- —Å—á–∏—Ç–∞–µ–º —Å–æ–≤–ø–∞–¥–∞—é—â–∏–µ n-–≥—Ä–∞–º–º—ã
- —à—Ç—Ä–∞—Ñ—É–µ–º —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ –ø–µ—Ä–µ–≤–æ–¥—ã

---

## 16) BLEU ‚Äî –≤–∞–∂–Ω—ã–µ —Å–≤–æ–π—Å—Ç–≤–∞

| –°–≤–æ–π—Å—Ç–≤–æ | –ó–Ω–∞—á–µ–Ω–∏–µ |
|---|---|
–ù–µ –∏–¥–µ–∞–ª—å–Ω–∞—è –º–µ—Ç—Ä–∏–∫–∞ | –Ω–æ —Ä–∞–±–æ—Ç–∞–µ—Ç —Ö–æ—Ä–æ—à–æ |
–ö–æ—Ä—Ä–µ–ª–∏—Ä—É–µ—Ç —Å –∫–∞—á–µ—Å—Ç–≤–æ–º | –æ—Å–æ–±–µ–Ω–Ω–æ –Ω–∞ –±–æ–ª—å—à–∏—Ö –∫–æ—Ä–ø—É—Å–∞—Ö |
–ù–µ —Ç—Ä–µ–±—É–µ—Ç —Ä–µ—Ñ–µ—Ä–µ–Ω—Å–∞ per-token | —Ç–æ–ª—å–∫–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ n-gram —á–∞—Å—Ç–æ—Ç |
–ë—ã–ª–∞ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–æ–º –¥–æ COMET / BLEURT | —Å–µ–π—á–∞—Å –≤—Å—ë –µ—â—ë –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è |

---


## 17) –ü–æ—á–µ–º—É –Ω—É–∂–µ–Ω Attention –≤ Seq2Seq

Seq2Seq –±–µ–∑ attention:
- encoder –ø—Ä–µ–≤—Ä–∞—â–∞–µ—Ç –≤—Å—ë –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –≤ **–æ–¥–∏–Ω –≤–µ–∫—Ç–æ—Ä**
- decoder –∏—Å–ø–æ–ª—å–∑—É–µ—Ç **—Ç–æ–ª—å–∫–æ –æ–¥–∏–Ω** –∫–æ–Ω—Ç–µ–∫—Å—Ç

–ü—Ä–æ–±–ª–µ–º–∞:
- –¥–ª–∏–Ω–Ω—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è ‚Üí encoder —Ç–µ—Ä—è–µ—Ç –¥–µ—Ç–∞–ª–∏
- —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ä–∞–∑–º–µ—Ä –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ = –±—É—Ç—ã–ª–æ—á–Ω–æ–µ –≥–æ—Ä–ª—ã—à–∫–æ

**–ò–¥–µ—è Attention:**
> –Ω–∞ –∫–∞–∂–¥–æ–º —à–∞–≥–µ decoder —Å–∞–º —Ä–µ—à–∞–µ—Ç, –Ω–∞ –∫–∞–∫–∏–µ —á–∞—Å—Ç–∏ –≤—Ö–æ–¥–Ω–æ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è —Å–º–æ—Ç—Ä–µ—Ç—å

---

## 18) Attention = –º—è–≥–∫–æ–µ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ (soft alignment)

–í SMT –±—ã–ª–∏ **alignment —Ç–∞–±–ª–∏—Ü—ã**.  
Attention = *–Ω–µ–π—Ä–æ–Ω–Ω–æ–µ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ*.

–ù–∞ —à–∞–≥–µ t decoder —Å–º–æ—Ç—Ä–∏—Ç –Ω–∞ encoder hidden states:

\[
h_1, \ldots, h_T
\]

–°—á–∏—Ç–∞–µ—Ç –≤–µ—Å–∞ –≤–Ω–∏–º–∞–Ω–∏—è:
\[
\alpha_{t,i} = \text{attention weight –Ω–∞ i-–π —Ç–æ–∫–µ–Ω source}
\]

---

## 19) –§–æ—Ä–º—É–ª—ã Bahdanau Attention (Additive Attention)

**Score function**
\[
e_{t,i} = v_a^\top \tanh(W_s s_{t-1} + W_h h_i)
\]

–≥–¥–µ
- \(s_{t-1}\) ‚Äî hidden decoder-–∞
- \(h_i\) ‚Äî hidden encoder-–∞
- \(v_a, W_s, W_h\) ‚Äî –æ–±—É—á–∞–µ–º—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã

**Softmax**
\[
\alpha_{t,i} = \frac{\exp(e_{t,i})}{\sum_{j} \exp(e_{t,j})}
\]

**Context**
\[
c_t = \sum_i \alpha_{t,i} h_i
\]

**Decoder update**
\[
s_t = \text{RNN}(y_{t-1}, s_{t-1}, c_t)
\]

---

## 20) –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è Attention

–ù–∞ —à–∞–≥–µ t decoder ¬´—Å–º–æ—Ç—Ä–∏—Ç¬ª –Ω–∞ —Ä–∞–∑–Ω—ã–µ —á–∞—Å—Ç–∏ source:

SRC: I saw the small dog  
ATT: 0.1 0.8 0.0 0.05 0.05  


‚Üí –º–æ–¥–µ–ª—å —Ñ–æ–∫—É—Å–∏—Ä—É–µ—Ç—Å—è –Ω–∞ **saw**

–°–ª–µ–¥—É—é—â–µ–µ —Å–ª–æ–≤–æ –≤ target –∑–∞–≤–∏—Å–∏—Ç –æ—Ç —Ç–æ–≥–æ, –∫—É–¥–∞ —Å–º–æ—Ç—Ä–∏–º.

---

## 21) –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –≤–Ω–∏–º–∞–Ω–∏—è

I saw the small dog  
0.02 0.75 0.03 0.10 0.10  


–ß–∞—Å—Ç–æ –∏–∑–æ–±—Ä–∞–∂–∞–µ—Ç—Å—è heatmap-–æ–º:
- —Å—Ç—Ä–æ–∫–∏ ‚Üí target —Å–ª–æ–≤–∞
- —Å—Ç–æ–ª–±—Ü—ã ‚Üí source —Å–ª–æ–≤–∞
- —è—Ä–∫–æ—Å—Ç—å ‚Üí attention weight

---

## 22) –ú–µ—Ö–∞–Ω–∏–∫–∏ Attention (–≤–∞—Ä–∏–∞–Ω—Ç—ã score)

| –¢–∏–ø | –§–æ—Ä–º—É–ª–∞ | –ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π |
|---|---|---|
Dot-product | \(e_{t,i} = s_{t-1}^\top h_i\) | –±—ã—Å—Ç—Ä–æ, –±–µ–∑ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ |
General | \(e_{t,i} = s_{t-1}^\top W h_i\) | –ø–∞—Ä–∞–º–µ—Ç—Ä–∏–∑–æ–≤–∞–Ω–Ω—ã–π dot |
Additive (Bahdanau) | \(v^\top\tanh(W_s s_{t-1} + W_h h_i)\) | –ª—É—á—à–µ –¥–ª—è —Ä–∞–∑–Ω–æ–π —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ |

> Dot-product = –ø–æ–∑–∂–µ —Å—Ç–∞–Ω–µ—Ç –æ—Å–Ω–æ–≤–æ–π **Scaled Dot-Product Attention** –≤ Transformers.

---

## 23) –ü–æ—á–µ–º—É attention –≤–∞–∂–µ–Ω –¥–ª—è NMT (–∏ –Ω–µ —Ç–æ–ª—å–∫–æ)

| –ë–µ–∑ Attention | C Attention |
|---|---|
–û–¥–∏–Ω –∫–æ–Ω—Ç–µ–∫—Å—Ç | –ö–æ–Ω—Ç–µ–∫—Å—Ç –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π |
–ü–ª–æ—Ö –¥–ª—è –¥–ª–∏–Ω–Ω—ã—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π | –ì–∏–±–∫–æ —Å–º–æ—Ç—Ä–∏—Ç –Ω–∞ –Ω—É–∂–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã |
–¢—Ä—É–¥–Ω—ã–µ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è | Alignment ¬´–∏–∑ –∫–æ—Ä–æ–±–∫–∏¬ª |
–ë–æ–ª—å—à–µ –æ—à–∏–±–æ–∫ | –°–∏–ª—å–Ω–æ –ª—É—á—à–µ BLEU |

Attention ‚Üí **–ø—Ä–æ—Ä—ã–≤ –≤ NMT**.

---

## 24) Connection to Self-Attention (–∑–∞—á–∏–Ω –Ω–∞ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã)

Bahdanau attention:
- encoder ‚Üí hidden states
- decoder —Å–º–æ—Ç—Ä–∏—Ç –Ω–∞ encoder

Self-attention (Transformers):
- –∫–∞–∂–¥—ã–π —Ç–æ–∫–µ–Ω —Å–º–æ—Ç—Ä–∏—Ç –Ω–∞ **–≤—Å–µ** —Ç–æ–∫–µ–Ω—ã
- –∏ –≤ encoder, –∏ –≤ decoder

\[
\text{Self-Attention}(X) = \text{Attention}(X, X, X)
\]

–≠—Ç–∞ –ª–µ–∫—Ü–∏—è ‚Äî –º–æ—Å—Ç —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤.

---

## 25) –î–≤–∞ –≤–∏–¥–∞ attention –≤ seq2seq

| –í–∏–¥ | –ß—Ç–æ —Å–º–æ—Ç—Ä–∏—Ç –∫—É–¥–∞ | –ü—Ä–∏–º–µ—Ä |
|---|---|---|
Encoder-Decoder attention | decoder ‚Üí encoder states | Bahdanau, Luong, Transformers |
Self-attention | –≤–Ω—É—Ç—Ä–∏ –æ–¥–Ω–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ | Transformers, BERT, GPT |

---

## 26) Luong Attention (dot-product style)

–ë–æ–ª–µ–µ –ø—Ä–æ—Å—Ç–æ–π/–±—ã—Å—Ç—Ä—ã–π –≤–∞—Ä–∏–∞–Ω—Ç:

\[
e_{t,i} = s_t^\top h_i
\]

–∫–æ–Ω—Ç–µ–∫—Å—Ç:
\[
c_t = \sum_i \alpha_{t,i} h_i
\]

decoder concat:
\[
\tilde{s}_t = \tanh(W [s_t ; c_t])
\]

Luong vs Bahdanau:
- Bahdanau additive ‚Üí –º–æ—â–Ω–µ–µ
- Luong dot ‚Üí –±—ã—Å—Ç—Ä–µ–µ, –ø–æ–ø—É–ª—è—Ä–Ω–µ–µ

---

## 27) "Global vs Local" attention (–∏–∑ —Å–ª–∞–π–¥–æ–≤)

| Global | Local |
|---|---|
—Å–º–æ—Ç—Ä–∏—Ç –Ω–∞ –≤—Å–µ —Ç–æ–∫–µ–Ω—ã | —Å–º–æ—Ç—Ä–∏—Ç –Ω–∞ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–µ –æ–∫–Ω–æ |
–¥–æ—Ä–æ–∂–µ | –¥–µ—à–µ–≤–ª–µ |
–ª—É—á—à–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ | —Ö—É–∂–µ –Ω–∞ –¥–ª–∏–Ω–Ω—ã—Ö —Ñ—Ä–∞–∑–∞—Ö |

---

## 28) Training NMT with Attention

–¢–µ –∂–µ —à–∞–≥–∏:
- teacher forcing
- cross-entropy
- masking / batching
- Adam
- gradient clipping

–ù–∏—á–µ–≥–æ —Å–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫–∏ –æ—Å–æ–±–µ–Ω–Ω–æ–≥–æ ‚Äî –ø—Ä–æ—Å—Ç–æ –¥–æ–±–∞–≤–∏–ª–∞—Å—å attention-–≥–æ–ª–æ–≤–∞.

---

## 29) –ü–æ—á–µ–º—É Attention ‚â† Transformers (–ø–æ–∫–∞)

–ó–¥–µ—Å—å:
- RNN + attention
- attention –ø–æ–º–æ–≥–∞–µ—Ç RNN

–í Transformers:
- **–Ω–µ—Ç RNN**
- –≤—Å—ë ‚Äî attention + FFN + layer norm + residuals


## 30) Copying words from source (Copy mechanism idea)

–ò–Ω–æ–≥–¥–∞ –Ω—É–∂–Ω–æ **—Å–∫–æ–ø–∏—Ä–æ–≤–∞—Ç—å** —Å–ª–æ–≤–æ –∏–∑ –≤—Ö–æ–¥–∞ –≤ –≤—ã—Ö–æ–¥:
- –∏–º–µ–Ω–∞
- —á–∏—Å–ª–∞
- —Ä–µ–¥–∫–∏–µ/–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã
- —Å–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫–∏–µ —Ç–µ—Ä–º–∏–Ω—ã

Attention —É–∂–µ –ø–æ–º–æ–≥–∞–µ—Ç ¬´—É–∫–∞–∑—ã–≤–∞—Ç—å¬ª –Ω–∞ —Å–ª–æ–≤–æ,  
–ø–æ—ç—Ç–æ–º—É –º–æ–¥–µ–ª—å –º–æ–∂–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –º–µ—Ö–∞–Ω–∏–∑–º –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏—è:

- –≤–º–µ—Å—Ç–æ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –Ω–æ–≤–æ–≥–æ —Å–ª–æ–≤–∞
- –ø—Ä–æ—Å—Ç–æ –∫–æ–ø–∏—Ä—É–µ—Ç —Ç–æ, –∫—É–¥–∞ —Å–º–æ—Ç—Ä–∏—Ç attention

–≠—Ç–æ –æ—Å–æ–±–µ–Ω–Ω–æ –≤–∞–∂–Ω–æ –¥–ª—è:
- low-resource —è–∑—ã–∫–æ–≤
- –¥–ª–∏–Ω–Ω—ã—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
- —Ä–µ–¥–∫–∏—Ö —Å–ª–æ–≤

–ü–æ–∑–∂–µ: **Pointer networks / CopyNet / Transformers copy behavior**

## 31) Why sequence-level training is hard

Language is **combinatorial**:
- –æ—á–µ–Ω—å –º–Ω–æ–≥–æ –≤–æ–∑–º–æ–∂–Ω—ã—Ö –≤—ã—Ö–æ–¥–æ–≤
- –Ω–µ –≤—Å–µ–≥–¥–∞ –æ–¥–Ω–∞ ¬´–ø—Ä–∞–≤–∏–ª—å–Ω–∞—è¬ª —Ñ—Ä–∞–∑–∞
- –º–µ—Ç—Ä–∏–∫–∏ (BLEU) –Ω–µ –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏—Ä—É–µ–º—ã

–û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å **–Ω–∞ —Ç–æ–∫–µ–Ω–∞—Ö**,  
–æ—Ü–µ–Ω–∏–≤–∞–µ–º **–Ω–∞ —É—Ä–æ–≤–Ω–µ –ø–æ–ª–Ω–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏**.

Mismatch:
> cross-entropy vs BLEU = train-test objective gap

## 32) Exposure Bias (–∏–∑ –ª–µ–∫—Ü–∏–∏)

–ü—Ä–æ–±–ª–µ–º–∞:

–í –æ–±—É—á–µ–Ω–∏–∏ decoder –ø–æ–ª—É—á–∞–µ—Ç **–ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã** (teacher forcing).  
–í –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–µ ‚Äî —Å–≤–æ–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è.

–ï—Å–ª–∏ –Ω–∞ —à–∞–≥–µ t –º–æ–¥–µ–ª—å –æ—à–∏–±–ª–∞—Å—å, –æ—à–∏–±–∫–∏ –Ω–∞–∫–∞–ø–ª–∏–≤–∞—é—Ç—Å—è.

–≠—Ç–æ –Ω–∞–∑—ã–≤–∞–µ—Ç—Å—è **exposure bias**.

–í–∏–¥–µ–ª–∏ —É–∂–µ —Ä–∞–Ω—å—à–µ. –ü—Ä–æ–±–ª–µ–º–∞ –≤–∞–∂–Ω–∞ –≤ MT.

## 33) Scheduled Sampling (Bengio et al., 2015)

–ò–¥–µ—è: –ø–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ **–∑–∞–º–µ–Ω—è—Ç—å teacher forcing** –Ω–∞ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –º–æ–¥–µ–ª–∏.

–ò–Ω–æ–≥–¥–∞ –¥–∞—ë–º decoder:
- ground-truth —Ç–æ–∫–µ–Ω
- –∏–Ω–æ–≥–¥–∞ ‚Äî —Ç–æ–∫–µ–Ω –º–æ–¥–µ–ª–∏

–¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º:
- –æ–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å —Ä–∞–±–æ—Ç–∞—Ç—å —Å–æ —Å–≤–æ–∏–º–∏ –æ—à–∏–±–∫–∞–º–∏
- —É–º–µ–Ω—å—à–∞–µ–º exposure bias

–ù–æ:
- –º–æ–∂–µ—Ç –ø—Ä–∏–≤–æ–¥–∏—Ç—å –∫ –Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
- –µ—â—ë –æ–¥–Ω–∞ —ç–≤—Ä–∏—Å—Ç–∏–∫–∞

## 34) Rare words & UNK problem

–ü—Ä–æ–±–ª–µ–º–∞ NMT —Ä–∞–Ω–Ω–∏—Ö –ª–µ—Ç:
- —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–ª–æ–≤–∞—Ä—å
- —Ä–µ–¥–∫–∏–µ —Å–ª–æ–≤–∞ ‚Üí `<UNK>`

–†–µ—à–µ–Ω–∏—è:
- subword units (BPE, WordPiece)
- byte/char models
- copy/point mechanisms

–°–µ–≥–æ–¥–Ω—è: **BPE ‚Üí —Å—Ç–∞–Ω–¥–∞—Ä—Ç** (–¥–æ byte-level —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤).

## 35) Coverage problem (Attention issue)

–ü—Ä–æ–±–ª–µ–º–∞:

Attention –º–æ–∂–µ—Ç ¬´–∑–∞–ª–∏–ø–∞—Ç—å¬ª –Ω–∞ –æ–¥–Ω–æ–º —Å–ª–æ–≤–µ  
‚Üí –ø–æ–≤—Ç–æ—Ä—è–µ—Ç –µ–≥–æ —Å–Ω–æ–≤–∞ –∏ —Å–Ω–æ–≤–∞

–∏–ª–∏ –Ω–∞–æ–±–æ—Ä–æ—Ç ‚Äî –ø—Ä–æ–ø—É—Å–∫–∞–µ—Ç —Å–ª–æ–≤–∞.

–≠—Ç–æ –Ω–∞–∑—ã–≤–∞–µ—Ç—Å—è **coverage problem**.

> –º–æ–¥–µ–ª—å –Ω–µ ¬´–ø–æ–º–Ω–∏—Ç¬ª, –∫–∞–∫–∏–µ —Å–ª–æ–≤–∞ —É–∂–µ –ø–µ—Ä–µ–≤–µ–ª–∞

–ü–æ–∑–∂–µ —Ä–µ—à–∞–µ—Ç—Å—è —á–µ—Ä–µ–∑:
- coverage vectors
- penalties
- transformer positional bias

## 36) Repetition & Hallucination

**Repetition**  
–ú–æ–¥–µ–ª—å –ø–æ–≤—Ç–æ—Ä—è–µ—Ç —á–∞—Å—Ç–∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è:

the cat the cat the cat is sitting  

**Hallucination**

–ú–æ–¥–µ–ª—å ¬´–ø—Ä–∏–¥—É–º—ã–≤–∞–µ—Ç¬ª —Ç–µ–∫—Å—Ç, –∫–æ—Ç–æ—Ä–æ–≥–æ –Ω–µ—Ç –≤–æ –≤—Ö–æ–¥–µ.

–≠—Ç–æ –≤–∞–∂–Ω–æ –¥–ª—è MT –∏ –¥–ª—è LLM (—Ç–æ—Ç –∂–µ —ç—Ñ—Ñ–µ–∫—Ç).

## 37) Length bias in beam search

Beam search –æ–±—ã—á–Ω–æ **–ø—Ä–µ–¥–ø–æ—á–∏—Ç–∞–µ—Ç –∫–æ—Ä–æ—Ç–∫–∏–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è**,  
–ø–æ—Ç–æ–º—É —á—Ç–æ –∫–∞–∂–¥–æ–µ –Ω–æ–≤–æ–µ —Å–ª–æ–≤–æ —Å–Ω–∏–∂–∞–µ—Ç —Å—É–º–º–∞—Ä–Ω—É—é –ª–æ–≥-–≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å.

–†–µ—à–µ–Ω–∏–µ:
- length normalization
- length penalty (–∫–∞–∫ –≤ Transformer MT)

## 38) Length penalty (–∏–∑ –ª–µ–∫—Ü–∏–∏)

–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è, —á—Ç–æ–±—ã –Ω–µ –Ω–∞–∫–∞–∑—ã–≤–∞—Ç—å –¥–ª–∏–Ω–Ω—ã–µ –ø–µ—Ä–µ–≤–æ–¥—ã.

–ü—Ä–æ—Å—Ç–∞—è –≤–µ—Ä—Å–∏—è:

\[
\frac{1}{T^\alpha}
\]

–∏–ª–∏ (Transformer style):

\[
LP = \frac{(5+|Y|)^\alpha}{(5+1)^\alpha}
\]

–≥–¥–µ \(\alpha > 0\)

\[
\frac{\log P(Y|X)}{LP}
\]

\( \alpha \approx 0.6 \)

## 39) Summary of MT issues

| –ü—Ä–æ–±–ª–µ–º–∞ | –ü–æ—è—Å–Ω–µ–Ω–∏–µ |
|---|---|
Exposure bias | –æ–±—É—á–∞–µ–º—Å—è –Ω–∞ teacher forcing, –∏–Ω—Ñ–µ—Ä–µ–Ω—Å –¥—Ä—É–≥–æ–π |
Length bias | beam search –ø—Ä–µ–¥–ø–æ—á–∏—Ç–∞–µ—Ç –∫–æ—Ä–æ—Ç–∫–∏–µ –æ—Ç–≤–µ—Ç—ã |
Coverage issue | attention –º–æ–∂–µ—Ç –∑–∞–ª–∏–ø–∞—Ç—å –∏–ª–∏ –ø—Ä–æ–ø—É—Å–∫–∞—Ç—å |
Repetitions | –º–æ–¥–µ–ª—å –ø–æ–≤—Ç–æ—Ä—è–µ—Ç —Ñ—Ä–∞–∑—ã |
Hallucinations | –º–æ–¥–µ–ª—å –≤—ã–¥—É–º—ã–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç |
Rare words | –Ω—É–∂–Ω–∞ subword —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è / copy |

---

