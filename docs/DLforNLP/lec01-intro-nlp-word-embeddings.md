---
layout: default
title: Lecture 1 ‚Äî Introduction to NLP and Word Embeddings I

---
# üìò Lecture 1 ‚Äî Introduction to NLP and Word Embeddings I

> **–ö—É—Ä—Å:** Deep Learning for NLP  
> **–¢–µ–º–∞:** –í–≤–µ–¥–µ–Ω–∏–µ –≤ NLP, —Ä–∞—Å–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –∏ Word2Vec  
> **–§–æ—Ä–º–∞—Ç:** Markdown —Å LaTeX-—Ñ–æ—Ä–º—É–ª–∞–º–∏ (GitHub Math Support)

---

## 1) –ß—Ç–æ —Ç–∞–∫–æ–µ NLP (Natural Language Processing)

**NLP (–æ–±—Ä–∞–±–æ—Ç–∫–∞ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞)** ‚Äî –æ–±–ª–∞—Å—Ç—å –Ω–∞ —Å—Ç—ã–∫–µ **computer science**, **artificial intelligence**, **machine learning** –∏ **linguistics**, —Ü–µ–ª—å –∫–æ—Ç–æ—Ä–æ–π ‚Äî —á—Ç–æ–±—ã –∫–æ–º–ø—å—é—Ç–µ—Ä—ã –º–æ–≥–ª–∏ **–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å, –ø–æ–Ω–∏–º–∞—Ç—å –∏ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å** —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–π —è–∑—ã–∫.

**–ü—Ä–∏–º–µ—Ä—ã –∑–∞–¥–∞—á (applications):**
- –ü–æ–∏—Å–∫ –∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ (**search, document classification**)
- –î–∏–∞–ª–æ–≥–∏/–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç—ã (**dialog systems, virtual assistants**)
- –ú–∞—à–∏–Ω–Ω—ã–π –ø–µ—Ä–µ–≤–æ–¥ (**machine translation, MT**)
- –†–µ—á—å (**automatic speech recognition, ASR**; **text-to-speech, TTS**)
- –ê–Ω–∞–ª–∏—Ç–∏–∫–∞ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ (**sentiment analysis**), —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ (**topic modeling**)

---

## 2) –ü–æ—á–µ–º—É —è–∑—ã–∫ —Å–ª–æ–∂–Ω—ã–π –¥–ª—è –º–∞—à–∏–Ω?

- –Ø–∑—ã–∫ **—Å–∏–º–≤–æ–ª–∏—á–µ—Å–∫–∏–π (symbolic)**, –∞ —Ñ–∏–∑–∏—á–µ—Å–∫–∏–µ —Å–∏–≥–Ω–∞–ª—ã –∏ –º–æ–∑–≥–æ–≤—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è ‚Äî **–Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã–µ (continuous)**.  
- **–ù–µ–æ–¥–Ω–æ–∑–Ω–∞—á–Ω–æ—Å—Ç—å (ambiguity)**, –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏ —Ñ–æ–Ω–æ–≤—ã–µ –∑–Ω–∞–Ω–∏—è –º–∏—Ä–∞.  
  –ü—Ä–∏–º–µ—Ä: *‚ÄúI made her duck.‚Äù* ‚Äî –º–Ω–æ–≥–æ–∑–Ω–∞—á–Ω–æ—Å—Ç—å –ª–µ–∫—Å–∏–∫–∏ –∏ —Å–∏–Ω—Ç–∞–∫—Å–∏—Å–∞.
- –ë–æ–ª—å—à–æ–π —Å–ª–æ–≤–∞—Ä—å ‚Üí **—Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ—Å—Ç—å (sparsity)** –ø—Ä–∏ –¥–∏—Å–∫—Ä–µ—Ç–Ω–æ–º –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–∏ (one-hot).

---

## 3) –ü–æ—á–µ–º—É Deep Learning (DL) –ø–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è NLP

- –ü–µ—Ä–µ—Ö–æ–¥ –æ—Ç **—Ä—É—á–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (hand-engineered features)** –∫ **–æ–±—É—á–∞–µ–º—ã–º –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è–º (learned representations)**.  
- DL —Å—Ç–∞–ª –≤–æ–∑–º–æ–∂–µ–Ω –±–ª–∞–≥–æ–¥–∞—Ä—è –±–æ–ª—å—à–∏–º –¥–∞–Ω–Ω—ã–º, **GPUs**, –∏ –Ω–æ–≤—ã–º –º–æ–¥–µ–ª—è–º (**RNN/LSTM ‚Üí Transformers**), –∞ —Ç–∞–∫–∂–µ **transfer learning** (BERT, GPT).
- –£–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –æ–±—É—á–∞–µ–º–∞—è —Ä–∞–º–∫–∞ **end-to-end** –æ—Ç —Ç–æ–∫–µ–Ω–æ–≤ –∫ –∑–∞–¥–∞—á–µ.

---

## 4) –ß—Ç–æ —Ç–∞–∫–æ–µ word embeddings (–≤–µ–∫—Ç–æ—Ä–Ω—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è —Å–ª–æ–≤)

**Distributional hypothesis (—Ä–∞—Å–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–∞—è –≥–∏–ø–æ—Ç–µ–∑–∞):**  
*–°–ª–æ–≤–∞, –≤—Å—Ç—Ä–µ—á–∞—é—â–∏–µ—Å—è –≤ –ø–æ—Ö–æ–∂–∏—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞—Ö, –∏–º–µ—é—Ç –ø–æ—Ö–æ–∂–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è.*

–í–º–µ—Å—Ç–æ –∏–Ω–¥–µ–∫—Å–∞ —Å–ª–æ–≤–∞ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ–º –µ–≥–æ **–≤–µ–∫—Ç–æ—Ä–æ–º** –≤ $\mathbb{R}^d$:

\[
\text{word} \rightarrow \mathbf{v} \in \mathbb{R}^d
\]

- –ü–æ—Ö–æ–∂–∏–µ –ø–æ —Å–º—ã—Å–ª—É —Å–ª–æ–≤–∞ –∏–º–µ—é—Ç –±–ª–∏–∑–∫–∏–µ –≤–µ–∫—Ç–æ—Ä—ã (**cosine similarity**).  
- –£–ª—É—á—à–∞—é—Ç –æ–±–æ–±—â–µ–Ω–∏–µ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å **one-hot**.

---

## 5) Word2Vec: –æ—Å–Ω–æ–≤–Ω—ã–µ –∏–¥–µ–∏ –∏ —Ä–µ–∂–∏–º—ã

**–¶–µ–ª—å:** –ò–∑–≤–ª–µ–∫–∞—Ç—å —Ä–∞—Å–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω—É—é —Å–µ–º–∞–Ω—Ç–∏–∫—É –∏–∑ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ —Å–æ–≤–º–µ—Å—Ç–Ω—ã—Ö –ø–æ—è–≤–ª–µ–Ω–∏–π —Å–ª–æ–≤.

–î–≤–∞ —Ä–µ–∂–∏–º–∞:
- **CBOW (Continuous Bag-of-Words)** ‚Äî –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º —Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω–æ–µ —Å–ª–æ–≤–æ –ø–æ –æ–∫—Ä—É–∂–µ–Ω–∏—é (–∫–æ–Ω—Ç–µ–∫—Å—Ç—É).
- **Skip-gram** ‚Äî –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º —Å–ª–æ–≤–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –ø–æ —Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω–æ–º—É —Å–ª–æ–≤—É.

–û–±–æ–∑–Ω–∞—á–µ–Ω–∏—è:
- $w_t$ ‚Äî —Å–ª–æ–≤–æ –≤ –ø–æ–∑–∏—Ü–∏–∏ $t$  
- $C$ ‚Äî —Ä–∞–∑–º–µ—Ä –æ–∫–Ω–∞ (**context window**)  
- $V$ ‚Äî —Ä–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è (**vocabulary size**)  
- $d$ ‚Äî —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ (**embedding dimension**)  
- $\mathbf{v}_w \in \mathbb{R}^d$ ‚Äî ¬´–≤—Ö–æ–¥–Ω–æ–π¬ª –≤–µ–∫—Ç–æ—Ä —Å–ª–æ–≤–∞ $w$  
- $\mathbf{u}_w \in \mathbb{R}^d$ ‚Äî ¬´–≤—ã—Ö–æ–¥–Ω–æ–π¬ª –≤–µ–∫—Ç–æ—Ä —Å–ª–æ–≤–∞ $w$

---

## 6) –§–æ—Ä–º—É–ª—ã Word2Vec

### 6.1 CBOW (–ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º —Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω–æ–µ —Å–ª–æ–≤–æ –ø–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É)

–ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–µ –º–Ω–æ–∂–µ—Å—Ç–≤–æ –¥–ª—è –ø–æ–∑–∏—Ü–∏–∏ $t$:
\[
\text{Context}(t) = \{\,w_{t-C}, \ldots, w_{t-1},\; w_{t+1}, \ldots, w_{t+C}\,\}
\]

–ê–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –≤–µ–∫—Ç–æ—Ä (—Å—Ä–µ–¥–Ω–µ–µ ¬´–≤—Ö–æ–¥–Ω—ã—Ö¬ª –≤–µ–∫—Ç–æ—Ä–æ–≤):
\[
\mathbf{h} = \frac{1}{2C}\sum_{\substack{-C \le j \le C \\ j \ne 0}} \mathbf{v}_{w_{t+j}}
\]

–ú–æ–¥–µ–ª—å–Ω–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ —Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω–æ–≥–æ —Å–ª–æ–≤–∞ $w_t$ (softmax):
\[
p(w_t \mid \text{Context}) =
\frac{\exp\!\left(\mathbf{u}_{w_t}^{\top}\mathbf{h}\right)}{\sum_{w=1}^{V} \exp\!\left(\mathbf{u}_{w}^{\top}\mathbf{h}\right)}
\]

–§—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å (–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ –ª–æ–≥-–ø—Ä–∞–≤–¥–æ–ø–æ–¥–æ–±–∏–µ) —Å—É–º–º–∏—Ä—É–µ—Ç—Å—è –ø–æ –¥–∞—Ç–∞—Å–µ—Ç—É.

---

### 6.2 Skip-gram (–ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –ø–æ —Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω–æ–º—É —Å–ª–æ–≤—É)

–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –Ω–∞–±–ª—é–¥–∞—Ç—å —Å–ª–æ–≤–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ $w_{t+j}$ –ø—Ä–∏ —Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω–æ–º $w_t$:
\[
p(w_{t+j} \mid w_t) =
\frac{\exp\!\left(\mathbf{u}_{w_{t+j}}^{\top}\mathbf{v}_{w_t}\right)}
{\sum_{w=1}^{V} \exp\!\left(\mathbf{u}_{w}^{\top}\mathbf{v}_{w_t}\right)}
\]

–°—É–º–º–∞—Ä–Ω–æ–µ –ª–æ–≥-–ø—Ä–∞–≤–¥–æ–ø–æ–¥–æ–±–∏–µ (–º–∞–∫—Å–∏–º–∏–∑–∏—Ä—É–µ–º):
\[
\mathcal{L}_{\text{SG}} =
\sum_{t=1}^{T} \sum_{\substack{-C \le j \le C \\ j \ne 0}}
\log p(w_{t+j} \mid w_t)
\]

---

### 6.3 –£—Å–∫–æ—Ä–µ–Ω–∏–µ: Negative Sampling (NS)

–ü–æ–ª–Ω—ã–π softmax –¥–æ—Ä–æ–≥ –ø—Ä–∏ –±–æ–ª—å—à–æ–º $V$. **Negative Sampling (NS)** –∑–∞–º–µ–Ω—è–µ—Ç –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—é –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–æ ¬´–Ω–µ–≥–∞—Ç–∏–≤–Ω—ã—Ö¬ª –ø—Ä–∏–º–µ—Ä–æ–≤ ($K \ll V$).

–î–ª—è –ø–∞—Ä—ã $(w_i, w_o)$ (input, output) –æ–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ–º:
\[
\log \sigma\!\left(\mathbf{u}_{w_o}^{\top}\mathbf{v}_{w_i}\right)
\;+\;
\sum_{k=1}^{K}
\mathbb{E}_{w_k \sim P_n(w)}
\Big[\log \sigma\!\left(-\,\mathbf{u}_{w_k}^{\top}\mathbf{v}_{w_i}\right)\Big]
\]
–≥–¥–µ $\sigma(x)=\frac{1}{1+e^{-x}}$ ‚Äî —Å–∏–≥–º–æ–∏–¥–∞, $P_n(w)$ ‚Äî —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –¥–ª—è –æ—Ç–±–æ—Ä–∞ –Ω–µ–≥–∞—Ç–∏–≤–æ–≤.

---

### 6.4 –ö–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ (cosine similarity)

–î–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π –±–ª–∏–∑–æ—Å—Ç–∏:
\[
\cos(\mathbf{a}, \mathbf{b}) =
\frac{\mathbf{a}^{\top}\mathbf{b}}{\lVert \mathbf{a}\rVert_2 \,\lVert \mathbf{b}\rVert_2}
\]

---

## 7) –ü–æ—á–µ–º—É –≤–µ–∫—Ç–æ—Ä—ã ¬´—Ä–∞–±–æ—Ç–∞—é—Ç¬ª –≤–µ–∑–¥–µ

- **–ï–¥–∏–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è** (–≤–µ–∫—Ç–æ—Ä—ã/—Ç–µ–Ω–∑–æ—Ä—ã) —É–¥–æ–±–µ–Ω –¥–ª—è —Å–ª–æ–≤, –º–æ—Ä—Ñ–æ–ª–æ–≥–∏–∏, —Å–∏–Ω—Ç–∞–∫—Å–∏—Å–∞, —Å–µ–º–∞–Ω—Ç–∏–∫–∏, –∏ –¥–ª—è –≤—ã—Å–æ–∫–æ—É—Ä–æ–≤–Ω–µ–≤—ã—Ö –∑–∞–¥–∞—á (sentiment, QA, MT, dialogue).  
- **–û–±—É—á–∞–µ–º–æ—Å—Ç—å –∏ –ø–µ—Ä–µ–Ω–æ—Å** (transfer): –æ–¥–∏–Ω —Ä–∞–∑ –æ–±—É—á–µ–Ω–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ —É–ª—É—á—à–∞—é—Ç –º–Ω–æ–∂–µ—Å—Ç–≤–æ –∑–∞–¥–∞—á.  
- **–°–Ω–∏–∂–∞—é—Ç –ø–æ—Ç—Ä–µ–±–Ω–æ—Å—Ç—å –≤ hand-crafted features**.

---

## 8) CBOW vs Skip-gram ‚Äî –∫–æ–≥–¥–∞ —á—Ç–æ

- **CBOW:** –±—ã—Å—Ç—Ä–µ–µ, —Å–≥–ª–∞–∂–∏–≤–∞–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç—ã, –ª—É—á—à–µ –Ω–∞ —á–∞—Å—Ç—ã—Ö —Å–ª–æ–≤–∞—Ö.  
- **Skip-gram:** –ª—É—á—à–µ –Ω–∞ —Ä–µ–¥–∫–∏—Ö —Å–ª–æ–≤–∞—Ö, –¥–µ—Ç–∞–ª—å–Ω–µ–µ –º–æ–¥–µ–ª–∏—Ä—É–µ—Ç –æ–∫—Ä–µ—Å—Ç–Ω–æ—Å—Ç–∏, –Ω–æ –¥–æ—Ä–æ–∂–µ.

---

## 9) –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –∑–∞–º–µ—Ç–∫–∏

- **–û–∫–Ω–æ (context window)**: —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ $C$ –∏–ª–∏ —Å–ª—É—á–∞–π–Ω–æ–µ –¥–æ $C_\text{max}$.  
- **–ü–æ–¥–≤—ã–±–æ—Ä–∫–∞ —á–∞—Å—Ç—ã—Ö —Å–ª–æ–≤ (subsampling)**: —É–º–µ–Ω—å—à–∞–µ—Ç —à—É–º –æ—Ç —Å–≤–µ—Ä—Ö—á–∞—Å—Ç—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤.  
- **NS –ø–∞—Ä–∞–º–µ—Ç—Ä $K$**: –æ–±—ã—á–Ω–æ 5‚Äì20.  
- **–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è/–æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è**: SGD / Adam, –æ–±—É—á–µ–Ω–∏–µ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —ç–ø–æ—Ö.

---

## 10) –ú–∏–Ω–∏-—à–ø–∞—Ä–≥–∞–ª–∫–∞ —Ñ–æ—Ä–º—É–ª (–∫–æ–ø–∏–ø–∞—Å—Ç–∞ –¥–ª—è GitHub)

**Softmax (–æ–±—â–∞—è —Ñ–æ—Ä–º–∞):**
\[
\operatorname{softmax}(\mathbf{z})_i = \frac{e^{z_i}}{\sum_j e^{z_j}}
\]

**CBOW –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å:**
\[
p(w_t \mid \text{Context}) =
\frac{\exp\!\left(\mathbf{u}_{w_t}^{\top}\mathbf{h}\right)}{\sum_{w=1}^{V} \exp\!\left(\mathbf{u}_{w}^{\top}\mathbf{h}\right)}
\]

**Skip-gram –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å:**
\[
p(w_{t+j} \mid w_t) =
\frac{\exp\!\left(\mathbf{u}_{w_{t+j}}^{\top}\mathbf{v}_{w_t}\right)}
{\sum_{w=1}^{V} \exp\!\left(\mathbf{u}_{w}^{\top}\mathbf{v}_{w_t}\right)}
\]

**Negative Sampling (–æ–¥–Ω–∞ –ø–∞—Ä–∞ + K –Ω–µ–≥–∞—Ç–∏–≤–æ–≤):**
\[
\log \sigma\!\left(\mathbf{u}_{w_o}^{\top}\mathbf{v}_{w_i}\right)
+
\sum_{k=1}^{K}
\mathbb{E}_{w_k \sim P_n(w)} \Big[\log \sigma\!\left(-\,\mathbf{u}_{w_k}^{\top}\mathbf{v}_{w_i}\right)\Big]
\]

**Cosine similarity:**
\[
\cos(\mathbf{a}, \mathbf{b}) =
\frac{\mathbf{a}^{\top}\mathbf{b}}{\lVert \mathbf{a}\rVert_2 \,\lVert \mathbf{b}\rVert_2}
\]

---

## 11) –ß–µ–≥–æ –∑–Ω–∞—Ç—å –∫ —Å–ª–µ–¥—É—é—â–µ–π –ª–µ–∫—Ü–∏–∏

- –†–∞–∑–Ω–∏—Ü–∞ **CBOW vs Skip-gram** (—Å–∫–æ—Ä–æ—Å—Ç—å/—Ä–µ–¥–∫–∏–µ —Å–ª–æ–≤–∞)  
- –ß—Ç–æ —Ç–∞–∫–æ–µ **distributional hypothesis**  
- –ö–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç **softmax** –∏ **negative sampling**  
- –ü–æ—á–µ–º—É embeddings –ª—É—á—à–µ **one-hot**  
- –ë–∞–∑–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏: **cosine similarity**

---

## 12) –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ

- Mikolov et al. (2013): *Efficient Estimation of Word Representations in Vector Space*  
- Mikolov et al. (2013): *Distributed Representations of Words and Phrases and their Compositionality*  
- –ü—Ä–∞–∫—Ç–∏–∫–∞: Gensim / PyTorch —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è Skip-gram/CBOW —Å NS

---

### üìÇ –†–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è

