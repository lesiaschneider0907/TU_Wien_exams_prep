# üìò Lecture 5 ‚Äî Vanishing Gradients and Fancy RNNs (2024W)

–õ–µ–∫—Ç–æ—Ä: Thomas Lukasiewicz  
–û—Å–Ω–æ–≤a –Ω–∞ —Å–ª–∞–π–¥–∞—Ö Abigail See

---

## 1) –ß—Ç–æ –∏–∑—É—á–∞–µ–º –≤ —ç—Ç–æ–π –ª–µ–∫—Ü–∏–∏

- –ü–æ—á–µ–º—É —É RNN **–∑–∞—Ç—É—Ö–∞—é—Ç** (vanish) –∏ **–≤–∑—Ä—ã–≤–∞—é—Ç—Å—è** (explode) –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã
- –ò–Ω—Ç—É–∏—Ü–∏—è + –º–∞—Ç–µ–º–∞—Ç–∏–∫–∞ –ø—Ä–æ–±–ª–µ–º—ã
- –ü–æ—á–µ–º—É —ç—Ç–æ –∫—Ä–∏—Ç–∏—á–Ω–æ –¥–ª—è RNN-LM
- –ú–µ—Ç–æ–¥—ã –±–æ—Ä—å–±—ã
  - Gradient clipping
  - LSTM
  - GRU
- –ß—Ç–æ –æ–±—â–∞—è –ø—Ä–æ–±–ª–µ–º–∞ –Ω–µ —Ç–æ–ª—å–∫–æ RNN (ResNets, Highway networks)
- Bidirectional RNNs
- Multi-layer RNNs

---

## 2) –ò–Ω—Ç—É–∏—Ü–∏—è: —á—Ç–æ —Ç–∞–∫–æ–µ vanishing gradient

**Vanishing gradient** = –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã —Å—Ç–∞–Ω–æ–≤—è—Ç—Å—è –ø–æ—á—Ç–∏ –Ω—É–ª–µ–≤—ã–º–∏ –ø—Ä–∏ –æ–±—Ä–∞—Ç–Ω–æ–º —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–∏ –ø–æ –≤—Ä–µ–º–µ–Ω–∏.

–í RNN:

\[
h_t = f(W x_t + U h_{t-1})
\]

–í–æ –≤—Ä–µ–º—è BPTT:

\[
\frac{\partial L}{\partial h_{t-k}}
 = \frac{\partial h_t}{\partial h_{t-1}}
   \cdots
   \frac{\partial h_{t-k+1}}{\partial h_{t-k}}
\]

–≠—Ç–æ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ **–º–Ω–æ–∂–µ—Å—Ç–≤–∞ –º–∞—Ç—Ä–∏—Ü –∏ –ø—Ä–æ–∏–∑–≤–æ–¥–Ω—ã—Ö –∞–∫—Ç–∏–≤–∞—Ü–∏–∏**.  

–ï—Å–ª–∏ –∏—Ö –Ω–æ—Ä–º—ã < 1 ‚Üí –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ —Å—Ç—Ä–µ–º–∏—Ç—Å—è –∫ **0** ‚Üí **–≥—Ä–∞–¥–∏–µ–Ω—Ç –∏—Å—á–µ–∑–∞–µ—Ç**.

---

## 3) –ò–Ω—Ç—É–∏—Ü–∏—è —á–µ—Ä–µ–∑ –º–µ—Ç–∞—Ñ–æ—Ä—É

- –ü—Ä–µ–¥—Å—Ç–∞–≤—å, —Ç—ã —á–∏—Ç–∞–µ—à—å –∫–Ω–∏–≥—É –∏ –ø–æ–º–Ω–∏—à—å —Ç–æ–ª—å–∫–æ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è.
- –í—Å—ë, —á—Ç–æ –±—ã–ª–æ 20 —Å—Ç—Ä–∞–Ω–∏—Ü –Ω–∞–∑–∞–¥, **—Ç–µ—Ä—è–µ—Ç—Å—è**.
- –¢–∞ –∂–µ –ø—Ä–æ–±–ª–µ–º–∞ –≤ RNN: –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –∏–∑ –ø—Ä–æ—à–ª–æ–≥–æ ¬´—Å—Ç–∏—Ä–∞–µ—Ç—Å—è¬ª.

---

## 4) –í–∏–∑—É–∞–ª—å–Ω–∞—è –∏–Ω—Ç—É–∏—Ü–∏—è (–∏–∑ —Å–ª–∞–π–¥–æ–≤)

### RNN forward

h0 ‚Üí h1 ‚Üí h2 ‚Üí h3 ‚Üí ‚Ä¶ ‚Üí hT

### BPTT backward

grad_T ‚Üí grad_(T-1) ‚Üí grad_(T-2) ‚Üí ‚Ä¶ ‚Üí grad_0


–ï—Å–ª–∏ –∫–∞–∂–¥—ã–π —à–∞–≥ ¬´—Å–∂–∏–º–∞–µ—Ç¬ª –≥—Ä–∞–¥–∏–µ–Ω—Ç ‚Üí  
–∫ –º–æ–º–µ–Ω—Ç—É, –∫–æ–≥–¥–∞ –º—ã –¥–æ—Ö–æ–¥–∏–º –Ω–∞–∑–∞–¥, –æ—Å—Ç–∞–ª–æ—Å—å –ø–æ—á—Ç–∏ **0**.

---

## 5) –ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –ø—Ä–∏—á–∏–Ω–∞

–ï—Å–ª–∏ –¥–ª—è –ø—Ä–æ—Å—Ç–æ—Ç—ã —Ä–∞—Å—Å–º–æ—Ç—Ä–µ—Ç—å **–ª–∏–Ω–µ–π–Ω—É—é RNN**:

\[
h_t = W h_{t-1}
\]

–¢–æ–≥–¥–∞:

\[
h_t = W^t h_0
\]

–ì—Ä–∞–¥–∏–µ–Ω—Ç –ø—Ä–æ–ø–æ—Ä—Ü–∏–æ–Ω–∞–ª–µ–Ω \(W^t\).  
–°–æ–±—Å—Ç–≤–µ–Ω–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ \(\lambda\) –º–∞—Ç—Ä–∏—Ü—ã \(W\):

- –µ—Å–ª–∏ \(|\lambda| < 1\) ‚Üí \(\lambda^t \to 0\) ‚Üí **vanishing**
- –µ—Å–ª–∏ \(|\lambda| > 1\) ‚Üí \(\lambda^t \to \infty\) ‚Üí **exploding**

---

## 6) –ü–æ—á–µ–º—É —ç—Ç–æ —Å–µ—Ä—å—ë–∑–Ω–æ

### –ú–æ–¥–µ–ª—å –Ω–µ –º–æ–∂–µ—Ç:
- –ø–æ–º–Ω–∏—Ç—å –¥–∞–ª—å–Ω–∏–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏
- –æ–±—É—á–∏—Ç—å—Å—è —Å—Ç—Ä—É–∫—Ç—É—Ä–µ —è–∑—ã–∫–∞ (—Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–∏—è, –≥—Ä–∞–º–º–∞—Ç–∏–∫–∞)
- –ø—Ä–∞–≤–∏–ª—å–Ω–æ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞—Ç—å long-range syntax

**–≠—Ç–æ —É–±–∏–≤–∞–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ LM/MT –Ω–∞ –¥–ª–∏–Ω–Ω—ã—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è—Ö.**

---

## 7) –ü—Ä–∏–º–µ—Ä —ç—Ñ—Ñ–µ–∫—Ç–∞ (Linzen et al., 2016)

–ó–∞–¥–∞—á–∞: –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç—å –≥–ª–∞–≥–æ–ª, —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω—ã–π —Å –ø–æ–¥–ª–µ–∂–∞—â–∏–º:

> *The keys to the cabinet **are** on the table.*

RNN **–±–µ–∑ LSTM** —á–∞—Å—Ç–æ –¥–µ–ª–∞–µ—Ç –æ—à–∏–±–∫—É:

> *The keys to the cabinet **is** ‚Ä¶*

‚Üí —Ç—Ä–µ–±—É–µ—Ç ¬´–¥–ª–∏–Ω–Ω–æ–π –ø–∞–º—è—Ç–∏¬ª, –Ω–æ RNN –µ—ë —Ç–µ—Ä—è–µ—Ç.

---

## 8) Exploding gradients

–ö–æ–≥–¥–∞ \(|\lambda| > 1\), –≥—Ä–∞–¥–∏–µ–Ω—Ç —Å—Ç–∞–Ω–æ–≤–∏—Ç—Å—è –æ–≥—Ä–æ–º–Ω—ã–º.

–°–∏–º–ø—Ç–æ–º—ã:
- loss = NaN
- –≤–µ—Å–∞ ‚Üí ‚àû
- –æ–±—É—á–µ–Ω–∏–µ —Ä–∞–∑–≤–∞–ª–∏–≤–∞–µ—Ç—Å—è

–†–µ—à–µ–Ω–∏–µ:
- **gradient clipping**
- **–ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏**
- **–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏**

---

## 9) Gradient clipping

–ò–¥–µ—è: –æ–≥—Ä–∞–Ω–∏—á–∏—Ç—å L2 –Ω–æ—Ä–º—É –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞.

\[
g := \frac{g}{\|g\|} \text{ if } \|g\| > \tau
\]

PyTorch:

```python
torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
```

**–≠—Ç–æ must-use –¥–ª—è RNN.**

## 10) –ö–∞–∫ –∏—Å–ø—Ä–∞–≤–∏—Ç—å vanishing gradients?

| –ü–æ–¥—Ö–æ–¥ | –ß—Ç–æ –¥–µ–ª–∞–µ—Ç |
|---|---|
**LSTM** | –æ—Ç–¥–µ–ª—å–Ω–∞—è –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω–∞—è –ø–∞–º—è—Ç—å (cell state) + –≥–µ–π—Ç—ã |
**GRU** | —É–ø—Ä–æ—â—ë–Ω–Ω—ã–π LSTM, –º–µ–Ω—å—à–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ |
**Gradient clipping** | –∑–∞—â–∏—Ç–∞ –æ—Ç exploding gradients |
**Orthogonal initialization** | —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç —Å–ø–µ–∫—Ç—Ä –º–∞—Ç—Ä–∏—Ü—ã, –ø–æ–º–æ–≥–∞–µ—Ç –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞–º –Ω–µ –∏—Å—á–µ–∑–Ω—É—Ç—å |
**Layer normalization** | —Å—Ç–∞–±–∏–ª–∏–∑–∏—Ä—É–µ—Ç –¥–∏–Ω–∞–º–∏–∫—É RNN |
**Residual/Skip connections** | –ø–æ–∑–≤–æ–ª—è–µ—Ç –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞–º –ø—Ä–æ—Ö–æ–¥–∏—Ç—å –Ω–∞–ø—Ä—è–º—É—é |
**Better activations** | ReLU, GELU –≤–º–µ—Å—Ç–æ tanh –∏–Ω–æ–≥–¥–∞ –ø–æ–º–æ–≥–∞–µ—Ç |

–ò–¥–µ—è: **—Å–æ–∑–¥–∞—Ç—å –ø—É—Ç—å, –ø–æ –∫–æ—Ç–æ—Ä–æ–º—É –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –Ω–µ —É–º–∏—Ä–∞—é—Ç**.

---

## 11) –ò—Å—Ç–æ—Ä–∏—è —Ä–µ—à–µ–Ω–∏—è –ø—Ä–æ–±–ª–µ–º—ã

- 1994 ‚Äî Bengio –∏ –¥—Ä. –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –ø—Ä–æ–±–ª–µ–º—É vanishing gradients
- 1997 ‚Äî **LSTM** (Hochreiter & Schmidhuber)
- –ø–æ–∑–∂–µ ‚Üí GRU (Cho et al., 2014)

**LSTM = –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞, —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ —Å–æ–∑–¥–∞–Ω–Ω–∞—è –ø—Ä–æ—Ç–∏–≤ –∑–∞—Ç—É—Ö–∞–Ω–∏—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤.**


## 12) –ü–æ—á–µ–º—É –æ–±—ã—á–Ω—ã–π RNN –∑–∞–±—ã–≤–∞–µ—Ç

\[
h_t = \tanh(Wx_t + Uh_{t-1})
\]

–ü—Ä–æ–∏–∑–≤–æ–¥–Ω–∞—è tanh –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∞:
\[
|f'(x)| \le 1
\]

‚Üí –ø—Ä–∏ –º–Ω–æ–≥–æ–∫—Ä–∞—Ç–Ω–æ–º —É–º–Ω–æ–∂–µ–Ω–∏–∏ –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã ‚Üí 0

---

## 13) LSTM –∏–¥–µ—è (–∏–Ω—Ç—É–∏—Ç–∏–≤–Ω–æ)

–î–æ–±–∞–≤–ª—è–µ–º:
- –æ—Ç–¥–µ–ª—å–Ω—É—é **—è—á–µ–π–∫—É –ø–∞–º—è—Ç–∏** \(c_t\)
- **–≥–µ–π—Ç—ã**, –∫–æ—Ç–æ—Ä—ã–µ —Ä–µ—à–∞—é—Ç:
  - —á—Ç–æ –∑–∞–ø–æ–º–Ω–∏—Ç—å
  - —á—Ç–æ –∑–∞–±—ã—Ç—å
  - —á—Ç–æ –≤—ã–¥–∞—Ç—å

**–û—Å–Ω–æ–≤–Ω–∞—è –º—ã—Å–ª—å:**
> –¥–∞–π –º–æ–¥–µ–ª–∏ –∫–∞–Ω–∞–ª, –ø–æ –∫–æ—Ç–æ—Ä–æ–º—É –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –∏–¥—ë—Ç –±–µ–∑ –∑–∞—Ç—É—Ö–∞–Ω–∏—è.

---

## 14) –§–æ—Ä–º—É–ª—ã LSTM

Forget gate:
\[
f_t = \sigma(W_f x_t + U_f h_{t-1} + b_f)
\]

Input gate:
\[
i_t = \sigma(W_i x_t + U_i h_{t-1} + b_i)
\]

Candidate memory:
\[
\tilde{c}_t = \tanh(W_c x_t + U_c h_{t-1} + b_c)
\]

Update memory:
\[
c_t = f_t * c_{t-1} + i_t * \tilde{c}_t
\]

Output gate:
\[
o_t = \sigma(W_o x_t + U_o h_{t-1} + b_o)
\]

Hidden state:
\[
h_t = o_t * \tanh(c_t)
\]

---

## 15) –ü–æ—á–µ–º—É —ç—Ç–æ –ø–æ–º–æ–≥–∞–µ—Ç –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞–º?

–í LSTM:
\[
c_t = f_t * c_{t-1} + ...
\]

–ï—Å–ª–∏ \( f_t \approx 1 \), —Ç–æ–≥–¥–∞
\[
c_t \approx c_{t-1}
\]

‚Üí –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –∏ –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –ø—Ä–æ—Ö–æ–¥—è—Ç **–¥–æ–ª–≥–æ –∏ —Å—Ç–∞–±–∏–ª—å–Ω–æ**  
‚Üí –º–µ–Ω—å—à–µ vanishing

–≠—Ç–æ –Ω–∞–∑—ã–≤–∞–µ—Ç—Å—è **constant error carousel** (CEC).

---

## 16) –ì–µ–π—Ç –∑–∞–±—ã–≤–∞–Ω–∏—è ‚Äî –∫–ª—é—á

\[
f_t = \sigma(W_f x_t + U_f h_{t-1})
\]

–ï—Å–ª–∏ –º–æ–¥–µ–ª—å —Ö–æ—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –ø–∞–º—è—Ç—å:
- \(f_t ‚âà 1\)

–ï—Å–ª–∏ –∑–∞–±—ã—Ç—å:
- \(f_t ‚âà 0\)

–†–∞–Ω—å—à–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏ —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –∑–∞–±—ã–≤–∞–Ω–∏–µ, —Ç–µ–ø–µ—Ä—å **trainable forgetting**.

---

## 17) –ò–Ω—Ç—É–∏—Ü–∏—è –ø–æ –ø—Ä–æ—Å—Ç–æ–º—É

- RNN = –ø–∞–º—è—Ç—å ¬´–æ–¥–Ω–æ —á–∏—Å–ª–æ –º–µ–Ω—è–µ—Ç—Å—è –∫–∞–∂–¥—ã–π —à–∞–≥¬ª
- LSTM = —É —Ç–µ–±—è **–±–ª–æ–∫–Ω–æ—Ç** (cell) –∏ **—Ä—É—á–∫–∞ —Å –∫–Ω–æ–ø–∫–∞–º–∏ ‚Äú–ø–∏—à—É/–Ω–µ –ø–∏—à—É‚Äù**

c_{t-1} ‚îÄ‚îÄ keep? ‚îÄ‚îÄ> c_t
‚ñ≤ write?
‚îÇ
new info



---

## 18) –í–∏–∑—É–∞–ª—å–Ω–æ (–∫–æ—Ä–æ—Ç–∫–∞—è ASCII)

    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ forget gate ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
h_{t-1} ‚Üí ‚îê ‚Üì
x_t ‚Üí [ gates ] ‚Üí combine ‚Üí cell ‚Üí out ‚Üí h_t
‚Üë ‚îÇ
‚îî‚îÄ input gate




---

## 19) LSTM vs GRU (–æ–±–∑–æ—Ä)

| | LSTM | GRU |
|---|---|---|
–ì–µ–π—Ç–æ–≤ | 3 (i, f, o) + –ø–∞–º—è—Ç—å | 2 (z, r) |
Cell state | ‚úÖ –µ—Å—Ç—å | ‚ùå –Ω–µ—Ç (—Ç–æ–ª—å–∫–æ h) |
–ü–∞–º—è—Ç—å | –ª—É—á—à–µ | –ø–æ—á—Ç–∏ —Ç–∞–∫–∞—è –∂–µ |
–°–∫–æ—Ä–æ—Å—Ç—å | –º–µ–¥–ª–µ–Ω–Ω–µ–µ | –±—ã—Å—Ç—Ä–µ–µ |
–ü–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ | –±–æ–ª—å—à–µ | –º–µ–Ω—å—à–µ |

–û–±–µ —Ä–µ—à–∞—é—Ç –ø—Ä–æ–±–ª–µ–º—É –∑–∞—Ç—É—Ö–∞—é—â–∏—Ö –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤.

---
## 20) GRU ‚Äî Gated Recurrent Unit

–£–ø—Ä–æ—â—ë–Ω–Ω–∞—è –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞ LSTM (Cho et al., 2014).  
–ú–µ–Ω—å—à–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, –±—ã—Å—Ç—Ä–µ–µ —É—á–∏—Ç—Å—è.

**–ì–µ–π—Ç—ã:**
- update gate \(z_t\) ‚Äî —á—Ç–æ –æ—Å—Ç–∞–≤–∏—Ç—å –∏–∑ –ø—Ä–æ—à–ª–æ–≥–æ
- reset gate \(r_t\) ‚Äî —Å–∫–æ–ª—å–∫–æ –ø—Ä–æ—à–ª–æ–π –∏–Ω—Ñ—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å

**–§–æ—Ä–º—É–ª—ã**

\[
z_t = \sigma(W_z x_t + U_z h_{t-1})
\]

\[
r_t = \sigma(W_r x_t + U_r h_{t-1})
\]

\[
\tilde{h}_t = \tanh\big(W_h x_t + U_h (r_t * h_{t-1})\big)
\]

\[
h_t = z_t * h_{t-1} + (1 - z_t) * \tilde{h}_t
\]

**–ò–Ω—Ç—É–∏—Ü–∏—è:**  
–ï—Å–ª–∏ \(z_t ‚âà 1\) ‚Üí –¥–µ—Ä–∂–∏–º —Å—Ç–∞—Ä—É—é –ø–∞–º—è—Ç—å  
–ï—Å–ª–∏ \(z_t ‚âà 0\) ‚Üí –æ–±–Ω–æ–≤–ª—è–µ–º –Ω–æ–≤–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π

---

## 21) BiRNN ‚Äî Bidirectional RNN

**Motivation:** –∫–æ–Ω—Ç–µ–∫—Å—Ç —Å–ª–µ–≤–∞ –∏ —Å–ø—Ä–∞–≤–∞ –ø–æ–º–æ–≥–∞–µ—Ç (–Ω–∞–ø—Ä–∏–º–µ—Ä, –¥–ª—è NER, POS).

\[
\overrightarrow{h}_t = RNN(x_t, \overrightarrow{h}_{t-1})
\]
\[
\overleftarrow{h}_t = RNN(x_t, \overleftarrow{h}_{t+1})
\]

–í—ã—Ö–æ–¥:
\[
h_t = [\overrightarrow{h}_t ; \overleftarrow{h}_t]
\]

**–ü—Ä–∏–º–µ—Ä:**
Sentence: the cat sat
Forward: the ‚Üí cat ‚Üí sat
Backward: sat ‚Üê cat ‚Üê the


–ù–µ–¥–æ—Å—Ç–∞—Ç–æ–∫: –Ω–µ–ª—å–∑—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø—Ä–∏ **–æ–Ω–ª–∞–π–Ω-–∏–Ω—Ñ–µ—Ä–µ–Ω—Å–µ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏**.

---

## 22) Multi-layer (stacked) RNN

–°—Ç—Ä–æ–∏–º –Ω–µ—Å–∫–æ–ª—å–∫–æ RNN-—Å–ª–æ—ë–≤ –¥—Ä—É–≥ –Ω–∞–¥ –¥—Ä—É–≥–æ–º:

x ‚Üí RNN‚ÇÅ ‚Üí h¬π ‚Üí RNN‚ÇÇ ‚Üí h¬≤ ‚Üí ‚Ä¶


–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:
- –≤—ã—à–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞—Ç—å —Å–ª–æ–∂–Ω—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏

–ú–∏–Ω—É—Å—ã:
- –º–µ–¥–ª–µ–Ω–Ω–æ
- —Ç—Ä—É–¥–Ω–µ–µ –æ–±—É—á–∞—Ç—å (–¥–æ –ø–æ—è–≤–ª–µ–Ω–∏—è LayerNorm / residuals)
- –µ—â—ë –±–æ–ª—å—à–µ —Ä–∏—Å–∫ exploding/vanishing ‚Üí –Ω—É–∂–µ–Ω clipping

---

## 23) Dropout –≤ RNN

–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π dropout = –ø–ª–æ—Ö–æ –¥–ª—è RNN (–ª–æ–º–∞–µ—Ç –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å).

**–ü—Ä–∞–≤–∏–ª—å–Ω—ã–π –≤–∞—Ä–∏–∞–Ω—Ç:**  
dropout **–º–µ–∂–¥—É —Å–ª–æ—è–º–∏** (not per-time-step), —Ç.–Ω. *variational dropout*.

PyTorch –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –¥–µ–ª–∞–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ, –µ—Å–ª–∏ —É–∫–∞–∑–∞—Ç—å `dropout` –≤ `nn.LSTM` –∏–ª–∏ `nn.GRU` –∏ `num_layers > 1`.

---

## 24) PyTorch: Bi-LSTM (skeleton)

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class BiLSTMTagger(nn.Module):
    def __init__(self, vocab, emb_dim, hid_dim, n_labels, n_layers=2, dropout=0.3):
        super().__init__()
        self.emb = nn.Embedding(vocab, emb_dim)
        self.lstm = nn.LSTM(
            input_size=emb_dim,
            hidden_size=hid_dim,
            num_layers=n_layers,
            dropout=dropout,
            batch_first=True,
            bidirectional=True
        )
        self.fc = nn.Linear(hid_dim * 2, n_labels)  # *2 because bidirectional

    def forward(self, x):
        e = self.emb(x)             # [B, T, D]
        o, _ = self.lstm(e)         # [B, T, 2H]
        logits = self.fc(o)         # [B, T, C]
        return logits
```

Use case: NER, POS, chunking, sequence tagging.

## 25) –ü–æ—á–µ–º—É RNN (–∏ –¥–∞–∂–µ LSTM/GRU) –ø—Ä–æ–±–ª–µ–º–Ω—ã –Ω–∞ –ø—Ä–∞–∫—Ç–∏–∫–µ

| –ù–µ–¥–æ—Å—Ç–∞—Ç–æ–∫ | –ü–æ—è—Å–Ω–µ–Ω–∏–µ |
|---|---|
**–ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è** | –Ω–µ–ª—å–∑—è –ø–∞—Ä–∞–ª–ª–µ–ª–∏—Ç—å —Ç–æ–∫–µ–Ω—ã ‚Üí –º–µ–¥–ª–µ–Ω–Ω–æ –Ω–∞ –¥–ª–∏–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–∞—Ö |
**–ü–ª–æ—Ö–æ —Å –¥–ª–∏–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º** | –¥–∞–∂–µ LSTM —Ç–µ—Ä—è–µ—Ç –ø–∞–º—è—Ç—å –Ω–∞ –æ—á–µ–Ω—å –¥–ª–∏–Ω–Ω—ã—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—è—Ö |
**–°–ª–æ–∂–Ω–æ –æ–±—É—á–∞—Ç—å** | –Ω—É–∂–Ω–∞ careful –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è, clipping, –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è |
**–ë–æ–ª—å—à–µ —Ç—Ä—é–∫–æ–≤** | truncated BPTT, masking, packing ‚Üí —Å–ª–æ–∂–Ω–µ–µ –ø–∞–π–ø–ª–∞–π–Ω |
**GPU –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –Ω–µ –ø–æ–ª–Ω–æ—Å—Ç—å—é** | –∏–∑-–∑–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ RNN —Ö—É–∂–µ –º–∞—Å—à—Ç–∞–±–∏—Ä—É—é—Ç—Å—è |
**Transformers –ø–æ–±–µ–∂–¥–∞—é—Ç** | self-attention —Ä–∞–±–æ—Ç–∞–µ—Ç –±—ã—Å—Ç—Ä–µ–µ –∏ –ª—É—á—à–µ |

**–ò—Ç–æ–≥:**  
RNN –ø–æ–ª–µ–∑–Ω—ã, –Ω–æ **—É—Å—Ç—É–ø–∏–ª–∏ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞–º** –ø–æ—á—Ç–∏ –≤–æ –≤—Å–µ—Ö NLP-–∑–∞–¥–∞—á–∞—Ö.


## 26) Highway Networks ‚Üí –∏–¥–µ—è skip-connections

–ü–µ—Ä–µ–¥ ResNet –ø–æ—è–≤–∏–ª–∏—Å—å **Highway Networks** (–¥–ª—è RNN —Ç–æ–∂–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏—Å—å).

–ò–¥–µ—è:
\[
y = H(x) * T(x) + x * (1 - T(x))
\]

- \(H(x)\) ‚Äî —Ñ—É–Ω–∫—Ü–∏—è (–Ω–∞–ø—Ä–∏–º–µ—Ä, –Ω–µ–ª–∏–Ω–µ–π–Ω—ã–π —Å–ª–æ–π)
- \(T(x)\) ‚Äî gate (–∫–∞–∫ –≤ LSTM/GRU)
- –ø—Ä–æ–ø—É—Å–∫–∞—é—Ç —á–∞—Å—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ ¬´–∫–∞–∫ –µ—Å—Ç—å¬ª

**–ò–Ω—Ç—É–∏—Ü–∏—è:**
> –µ—Å–ª–∏ —Å–µ—Ç—å –Ω–µ –∑–Ω–∞–µ—Ç, —á—Ç–æ –¥–µ–ª–∞—Ç—å ‚Äî –ø—É—Å—Ç—å —Ö–æ—Ç—è –±—ã –ø–µ—Ä–µ–¥–∞—Å—Ç —Å–∏–≥–Ω–∞–ª –¥–∞–ª—å—à–µ

–≠—Ç–æ –ø–æ–¥–≥–æ—Ç–æ–≤–∏–ª–æ –ø–æ—á–≤—É –¥–ª—è **ResNet skip-connections**.

---

## 27) Layer Normalization –≤ RNN

BatchNorm –ø–ª–æ—Ö–æ —Ä–∞–±–æ—Ç–∞–µ—Ç –≤ RNN ‚Üí –≤—Ä–µ–º—è = –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å, —Ä–∞–∑–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ.

–†–µ—à–µ–Ω–∏–µ: **LayerNorm** (Ba et al., 2016)

\[
\hat{h}_t = \frac{h_t - \mu}{\sigma}
\]

–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤–Ω—É—Ç—Ä–∏ LSTM/GRU –≤ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è—Ö.

–ü–ª—é—Å—ã:
- —Å—Ç–∞–±–∏–ª–∏–∑–∏—Ä—É–µ—Ç –æ–±—É—á–µ–Ω–∏–µ
- –ø–æ–º–æ–≥–∞–µ—Ç –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞–º
- —É—Å–∫–æ—Ä—è–µ—Ç —Å—Ö–æ–¥–∏–º–æ—Å—Ç—å

---

## 28) –ß—Ç–æ –≤—ã–Ω–µ—Å—Ç–∏ –∏–∑ –ª–µ–∫—Ü–∏–∏

| –¢–µ–º–∞ | –°–º—ã—Å–ª |
|---|---|
Vanishing gradients | –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã ‚Üí 0 –ø—Ä–∏ –¥–ª–∏–Ω–Ω—ã—Ö —Ü–µ–ø–æ—á–∫–∞—Ö |
Exploding gradients | –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã ‚Üí ‚àû, –≤–µ—Å—ã diverge |
LSTM/GRU | —Ä–µ—à–∞—é—Ç –ø—Ä–æ–±–ª–µ–º—É –∑–∞—Ç—É—Ö–∞–Ω–∏—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ |
LayerNorm / clipping | —Å—Ç–∞–±–∏–ª–∏–∑–∏—Ä—É—é—Ç –æ–±—É—á–µ–Ω–∏–µ RNN |
Attention | –ª—É—á—à–µ –º–æ–¥–µ–ª—å –ø–∞–º—è—Ç–∏, –≤—ã—Ç–µ—Å–Ω—è–µ—Ç RNN |
Transformers | –±—ã—Å—Ç—Ä–µ–µ –∏ —Ç–æ—á–Ω–µ–µ, –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–∞—Ü–∏—è |

**–ì–ª–∞–≤–Ω–æ–µ:**  
LSTM/GRU ‚âà –º–æ—Å—Ç –º–µ–∂–¥—É –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–º–∏ RNN –∏ attention-based –º–æ–¥–µ–ª—è–º–∏.

---

## 29) –ú–∏–Ω–∏-–≤–æ–ø—Ä–æ—Å—ã –¥–ª—è —Å–∞–º–æ–ø—Ä–æ–≤–µ—Ä–∫–∏

- –ü–æ—á–µ–º—É —É RNN –∑–∞—Ç—É—Ö–∞—é—Ç –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã?
- –ß—Ç–æ —Ç–∞–∫–æ–µ exploding gradient –∏ –∫–∞–∫ –µ–≥–æ —Ñ–∏–∫—Å–∏—Ç—å?
- –ö–∞–∫ LSTM –ø–æ–º–æ–≥–∞–µ—Ç? –ù–∞–∑–æ–≤–∏ gate-—ã.
- –ß–µ–º GRU –æ—Ç–ª–∏—á–∞–µ—Ç—Å—è –æ—Ç LSTM?
- –ó–∞—á–µ–º gradient clipping?
- –ü–æ—á–µ–º—É Transformers –≤—ã—Ç–µ—Å–Ω–∏–ª–∏ RNN?
- –ß—Ç–æ –¥–µ–ª–∞–µ—Ç LayerNorm –≤ RNN?

---

## 30) Quick cheat sheet

- –ò—Å–ø–æ–ª—å–∑—É–π **LSTM/GRU**, –∞ –Ω–µ raw RNN
- –í—Å–µ–≥–¥–∞ –≤–∫–ª—é—á–∞–π **gradient clipping**
- –î–ª—è –º–Ω–æ–≥–æ—Å–ª–æ–π–Ω—ã—Ö RNN ‚Üí **dropout –º–µ–∂–¥—É —Å–ª–æ—è–º–∏**
- –î–ª—è –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –¥–ª–∏–Ω—ã ‚Üí **pack padded sequence**
- –î–ª—è stability ‚Üí **LayerNorm**

---

## 31) –õ–∏—Ç–µ—Ä–∞—Ç—É—Ä–∞

- Hochreiter & Schmidhuber (1997) ‚Äî LSTM
- Cho et al. (2014) ‚Äî GRU
- Bengio et al. (1994) ‚Äî vanishing/exploding gradients
- Ba et al. (2016) ‚Äî LayerNorm
- Sutskever et al. (2014) ‚Äî Seq2Seq
- Vaswani et al. (2017) ‚Äî Transformers (–≤ —Å–ª–µ–¥—É—é—â–µ–π –ª–µ–∫—Ü–∏–∏)

---
