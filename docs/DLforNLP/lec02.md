# üìò Lecture 2 ‚Äî Word Embeddings II (Thomas Lukasiewicz, 2024W)

> **–¢–µ–º–∞:** –æ—Ç word2vec –∫ —Å—á—ë—Ç–Ω—ã–º –º–µ—Ç–æ–¥–∞–º (count-based), SVD, GloVe; –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è (gradient descent/SGD); –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ (intrinsic/extrinsic); –º–Ω–æ–≥–æ–∑–Ω–∞—á–Ω–æ—Å—Ç—å —Å–ª–æ–≤ (polysemy).

---

## 0) –ë—ã—Å—Ç—Ä—ã–π –æ–±–∑–æ—Ä (review) Skip-gram
**–ò–¥–µ—è:** –º–æ–¥–µ–ª—å **skip-gram** —É—á–∏—Ç—Å—è –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—Ç—å —Å–ª–æ–≤–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –ø–æ —Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω–æ–º—É —Å–ª–æ–≤—É, –º–∞–∫—Å–∏–º–∏–∑–∏—Ä—É—è –ø—Ä–∞–≤–¥–æ–ø–æ–¥–æ–±–∏–µ –Ω–∞–±–ª—é–¥–∞–µ–º—ã—Ö –ø–∞—Ä *(center, context)* –≤ —Å–∫–æ–ª—å–∑—è—â–µ–º –æ–∫–Ω–µ (context window).

–û–±–æ–∑–Ω–∞—á–µ–Ω–∏—è:
- $w_t$ ‚Äî —Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω–æ–µ —Å–ª–æ–≤–æ (center word)
- $C$ ‚Äî —Ä–∞–¥–∏—É—Å –æ–∫–Ω–∞ (window size)
- $\mathbf{v}_w$ ‚Äî –≤—Ö–æ–¥–Ω–æ–π –≤–µ–∫—Ç–æ—Ä —Å–ª–æ–≤–∞ $w$ (input embedding)
- $\mathbf{u}_w$ ‚Äî –≤—ã—Ö–æ–¥–Ω–æ–π –≤–µ–∫—Ç–æ—Ä —Å–ª–æ–≤–∞ $w$ (output embedding)
- $V$ ‚Äî —Ä–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è (vocabulary size)

**Skip-gram (softmax):**
\[
p(w_{t+j}\mid w_t)=
\frac{\exp(\mathbf{u}^\top_{w_{t+j}}\mathbf{v}_{w_t})}
{\sum_{w=1}^{V}\exp(\mathbf{u}_w^\top\mathbf{v}_{w_t})}\,,
\quad j\in\{-C,\dots,-1,1,\dots,C\}
\]
\[
\mathcal{L}_{\text{SG}}=\sum_{t=1}^{T}\sum_{\substack{-C\le j\le C\\ j\ne 0}}
\log p(w_{t+j}\mid w_t)
\]

**Negative Sampling (NS):** –∑–∞–º–µ–Ω–∏—Ç—å –ø–æ–ª–Ω—É—é –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—é $V$ –Ω–∞ $K\!\ll\!V$ ¬´–Ω–µ–≥–∞—Ç–∏–≤–æ–≤¬ª (negative samples):
\[
\log\sigma(\mathbf{u}_{w_o}^\top\mathbf{v}_{w_i})
+\sum_{k=1}^{K}\mathbb{E}_{w_k\sim P_n}\big[\log\sigma(-\mathbf{u}_{w_k}^\top\mathbf{v}_{w_i})\big]
\]

---

## 1) –ü—Ä–æ—Å—Ç–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è: Gradient Descent / SGD

### 1.1 –ì—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π —Å–ø—É—Å–∫ (gradient descent)
–ò—â–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã $\theta$ (–≤—Å–µ $\mathbf{v},\mathbf{u}$), –º–∏–Ω–∏–º–∏–∑–∏—Ä—É—é—â–∏–µ —Ñ—É–Ω–∫—Ü–∏—é –ø–æ—Ç–µ—Ä—å $J(\theta)$:
\[
\theta\leftarrow\theta-\eta\,\nabla_\theta J(\theta)
\]
–≥–¥–µ $\eta$ ‚Äî —Å–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è (learning rate).

### 1.2 –°—Ç–æ—Ö–∞—Å—Ç–∏—á–µ—Å–∫–∏–π –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π —Å–ø—É—Å–∫ (SGD)
–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –Ω–∞ –º–∏–Ω–∏-–±–∞—Ç—á–µ (mini-batch) –∏–ª–∏ –æ–¥–Ω–æ–º –ø—Ä–∏–º–µ—Ä–µ:
\[
\theta\leftarrow\theta-\eta\,\nabla_\theta J_i(\theta)
\]
–ü–ª—é—Å—ã: –¥–µ—à—ë–≤–æ –ø–æ –ø–∞–º—è—Ç–∏, –±—ã—Å—Ç—Ä–æ –¥–∞—ë—Ç –ø—Ä–æ–≥—Ä–µ—Å—Å; –ú–∏–Ω—É—Å—ã: —à—É–º–Ω—ã–µ —à–∞–≥–∏ ‚Üí –Ω—É–∂–Ω—ã scheduler/–º–æ–º–µ–Ω—Ç—É–º/Adam, –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π –∫–ª–∏–ø–ø–∏–Ω–≥.

### 1.3 –ü—Å–µ–≤–¥–æ–∫–æ–¥ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è (negative sampling, —É–ø—Ä–æ—â—ë–Ω–Ω–æ)
```python
# w_i = center, w_o = true context, negs = K negative words
score_pos = dot(u[w_o], v[w_i])         # u¬∑v
loss = -log(sigmoid(score_pos))
for w_k in negs:
    loss += -log(sigmoid(-dot(u[w_k], v[w_i])))

# Backprop: grads wrt v[w_i], u[w_o], {u[w_k]}
# v[w_i] <- v[w_i] - lr * dL/dv[w_i]
# u[w_o] <- u[w_o] - lr * dL/du[w_o]
# u[w_k] <- u[w_k] - lr * dL/du[w_k]

---

## 2) –ü–æ—á–µ–º—É –Ω–µ –≤—Å–µ–≥–¥–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç–µ–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏? –°—á—ë—Ç–Ω—ã–µ –º–µ—Ç–æ–¥—ã (count-based)

**–í–æ–ø—Ä–æ—Å:** ¬´–ø–æ—á–µ–º—É –±—ã –Ω–µ –∑–∞—Ö–≤–∞—Ç—ã–≤–∞—Ç—å —Å–æ–≤–º–µ—Å—Ç–Ω—ã–µ –≤—Å—Ç—Ä–µ—á–∞–µ–º–æ—Å—Ç–∏ –Ω–∞–ø—Ä—è–º—É—é?¬ª (co-occurrence counts).  
–ò–¥–µ—è: —Å—Ç—Ä–æ–∏–º **–º–∞—Ç—Ä–∏—Ü—É —Å–æ–≤–º–µ—Å—Ç–Ω—ã—Ö –≤—Å—Ç—Ä–µ—á–∞–µ–º–æ—Å—Ç–µ–π** \(X \in \mathbb{R}^{V \times V}\), –≥–¥–µ \(X_{ij}\) ‚Äî —Å–∫–æ–ª—å–∫–æ —Ä–∞–∑ —Å–ª–æ–≤–æ \(j\) –≤—Å—Ç—Ä–µ—á–∞–ª–æ—Å—å –≤ –æ–∫—Ä–µ—Å—Ç–Ω–æ—Å—Ç–∏ —Å–ª–æ–≤–∞ \(i\) (–æ–∫–Ω–æ ‚Äî context window), –∏–Ω–æ–≥–¥–∞ —Å –≤–µ—Å–æ–º \(1/\text{distance}\).

### 2.1 Window-based co-occurrence
–ü—Ä–∏–º–µ—Ä: –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ *‚Äúthe cat sat on the mat‚Äù*, –æ–∫–Ω–æ \(C=2\). –î–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–ª–æ–≤–∞ —É—á–∏—Ç—ã–≤–∞–µ–º —Å–æ—Å–µ–¥–µ–π –Ω–∞ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–∏ \(\le2\).

### 2.2 –ü—Ä–æ–±–ª–µ–º—ã ¬´—Å—ã—Ä—ã—Ö¬ª —Å—á—ë—Ç—á–∏–∫–æ–≤
- **–í—ã—Å–æ–∫–∞—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å** –∏ **—Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ—Å—Ç—å** (sparsity).  
- –ü–ª–æ—Ö–∞—è –≥–µ–Ω–µ—Ä–∞–ª–∏–∑–∞—Ü–∏—è –¥–ª—è **—Ä–µ–¥–∫–∏—Ö —Å–ª–æ–≤**.  
- –°–≤–µ—Ä—Ö—á–∞—Å—Ç—ã–µ —Ç–æ–∫–µ–Ω—ã (¬´the¬ª, ¬´of¬ª, ‚Ä¶) –¥–æ–º–∏–Ω–∏—Ä—É—é—Ç ‚Äî ¬´—à—É–º¬ª –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏–∫–∏.

---

## 3) –ü–æ–Ω–∏–∂–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ (dimensionality reduction): SVD

–ò–¥–µ—è: –ø—Ä–∏–º–µ–Ω–∏—Ç—å **SVD** (—Å–∏–Ω–≥—É–ª—è—Ä–Ω–æ–µ —Ä–∞–∑–ª–æ–∂–µ–Ω–∏–µ) –∫ \(X\), –ø–æ–ª—É—á–∏—Ç—å –∫–æ–º–ø–∞–∫—Ç–Ω—ã–µ –≤–µ–∫—Ç–æ—Ä—ã.

\[
X \approx U_k\,\Sigma_k\,V_k^\top,
\]
–≥–¥–µ \(U_k, V_k\) ‚Äî –æ—Ä—Ç–æ–Ω–æ—Ä–º–∞–ª—å–Ω—ã–µ, \(\Sigma_k\) ‚Äî –¥–∏–∞–≥–æ–Ω–∞–ª—å–Ω–∞—è —Å —Ç–æ–ø-\(k\) —Å–∏–Ω–≥—É–ª—è—Ä–∞–º–∏.  
**–≠–º–±–µ–¥–¥–∏–Ω–≥ —Å–ª–æ–≤–∞** —á–∞—Å—Ç–æ –±–µ—Ä—É—Ç –∫–∞–∫ —Å—Ç—Ä–æ–∫—É –∏–∑ \(U_k\Sigma_k^{1/2}\) (—ç–º–±–µ–¥–¥–∏–Ω–≥ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ ‚Äî \(V_k\Sigma_k^{1/2}\)).

### 3.1 PPMI –∫–∞–∫ ¬´—Ö–∞–∫¬ª –∫ \(X\)
–í–º–µ—Å—Ç–æ ¬´—Å—ã—Ä—ã—Ö¬ª —á–∞—Å—Ç–æ—Ç –∏—Å–ø–æ–ª—å–∑—É—é—Ç **PPMI** (positive PMI):

\[
\operatorname{PMI}(i,j)=\log\frac{p(i,j)}{p(i)p(j)},\qquad
\operatorname{PPMI}(i,j)=\max\{\operatorname{PMI}(i,j),0\},
\]

–≥–¥–µ \(p(i,j)=\frac{X_{ij}}{\sum_{i'j'} X_{i'j'}}\), \(p(i)=\frac{\sum_j X_{ij}}{\sum_{i'j'} X_{i'j'}}\), \(p(j)=\frac{\sum_i X_{ij}}{\sum_{i'j'} X_{i'j'}}\).

---

## 4) Count-based vs Predictive (—Å–≤—è–∑–∏ –∏ —Ä–∞–∑–ª–∏—á–∏—è)

- **Count-based**: (PPMI-)SVD –Ω–∞–¥ –≥–ª–æ–±–∞–ª—å–Ω—ã–º–∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞–º–∏.  
- **Predictive**: word2vec/SGNS —É—á–∏—Ç –≤–µ–∫—Ç–æ—Ä—ã, **–ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—è** —Å–æ—Å–µ–¥–µ–π.  
- –°–≤—è–∑—å: SGNS –ø—Ä–∏–±–ª–∏–∂—ë–Ω–Ω–æ —Ñ–∞–∫—Ç–æ—Ä–∏–∑—É–µ—Ç **PMI-—Å–¥–≤–∏–≥–∏**; —Ä–∞–∑–ª–∏—á–∞—é—Ç—Å—è –ø—Ä–æ—Ü–µ–¥—É—Ä–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –∏ –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç—å—é.

---

## 5) GloVe (Global Vectors for Word Representation)

**–ò–Ω—Ç—É–∏—Ü–∏—è:** –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å **–≥–ª–æ–±–∞–ª—å–Ω—ã–µ** \(X_{ij}\) –∏ —Å–≤—è–∑–∞—Ç—å **—Ä–∞–∑–Ω–æ—Å—Ç–∏ –≤–µ–∫—Ç–æ—Ä–æ–≤** —Å **–æ—Ç–Ω–æ—à–µ–Ω–∏—è–º–∏ —á–∞—Å—Ç–æ—Ç**. –õ–æ–≥–∞—Ä–∏—Ñ–º —Å–æ–≤–º–µ—Å—Ç–Ω–æ–π —á–∞—Å—Ç–æ—Ç—ã \(\log X_{ij}\) ‚Äî ¬´–µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–∞—è¬ª –≤–µ–ª–∏—á–∏–Ω–∞.

### 5.1 –¶–µ–ª–µ–≤–∞—è —Ñ—É–Ω–∫—Ü–∏—è (loss)
\[
J = \sum_{i,j=1}^{V} f(X_{ij})
\Big(\mathbf{w}_i^\top \tilde{\mathbf{w}}_j + b_i + \tilde b_j - \log X_{ij}\Big)^2,
\]

–≥–¥–µ \(\mathbf{w}_i\) ‚Äî –≤–µ–∫—Ç–æ—Ä —Å–ª–æ–≤–∞, \(\tilde{\mathbf{w}}_j\) ‚Äî –≤–µ–∫—Ç–æ—Ä –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞, \(b_i,\tilde b_j\) ‚Äî —Å–º–µ—â–µ–Ω–∏—è, –∞ –≤–µ—Å–æ–≤–∞—è —Ñ—É–Ω–∫—Ü–∏—è:

\[
f(x)=
\begin{cases}
\left(\dfrac{x}{x_{\max}}\right)^\alpha, & x < x_{\max}\\[4pt]
1, & x \ge x_{\max}
\end{cases}
\quad (\alpha\approx 0.75,\ x_{\max}\approx 100).
\]

### 5.2 –ì—Ä–∞–¥–∏–µ–Ω—Ç—ã (–¥–ª—è –ø–∞—Ä—ã \((i,j)\))
\[
\delta_{ij}=\mathbf{w}_i^\top \tilde{\mathbf{w}}_j + b_i + \tilde b_j - \log X_{ij},
\]
\[
\frac{\partial J}{\partial \mathbf{w}_i}=2\,f(X_{ij})\,\delta_{ij}\,\tilde{\mathbf{w}}_j,\qquad
\frac{\partial J}{\partial \tilde{\mathbf{w}}_j}=2\,f(X_{ij})\,\delta_{ij}\,\mathbf{w}_i,
\]
–∏ –∞–Ω–∞–ª–æ–≥–∏—á–Ω–æ –ø–æ \(b_i,\tilde b_j\).  
–ò—Ç–æ–≥–æ–≤—ã–π –≤–µ–∫—Ç–æ—Ä —Å–ª–æ–≤–∞ —á–∞—Å—Ç–æ –±–µ—Ä—É—Ç –∫–∞–∫ \(\tfrac{1}{2}(\mathbf{w}_i+\tilde{\mathbf{w}}_i)\).

---

## 6) –í—Å–ø–ª—ã–≤–∞—é—â–∏–µ –∑–∞–∫–æ–Ω–æ–º–µ—Ä–Ω–æ—Å—Ç–∏ (emergent patterns)

- **–°–∏–Ω—Ç–∞–∫—Å–∏—á–µ—Å–∫–∏–µ** –∫–ª–∞—Å—Ç–µ—Ä—ã (syntactic).  
- **–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ** –∫–ª–∞—Å—Ç–µ—Ä—ã (semantic), –∞–Ω–∞–ª–æ–≥–∏–∏.  

**–ê–Ω–∞–ª–æ–≥–∏–∏ (3CosAdd):**
\[
\arg\max_{w}\ \cos\!\big(\mathbf{v}_w,\ \mathbf{v}_{b}-\mathbf{v}_{a}+\mathbf{v}_{c}\big).
\]

**–í–∞—Ä–∏–∞–Ω—Ç (3CosMul):**
\[
\arg\max_{w}\ 
\frac{\cos(\mathbf{v}_w,\mathbf{v}_b)\cdot\cos(\mathbf{v}_w,\mathbf{v}_c)}
{\cos(\mathbf{v}_w,\mathbf{v}_a)+\varepsilon}.
\]

---

## 7) –û—Ü–µ–Ω–∫–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ (evaluation)

### 7.1 –í–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ (intrinsic)
- **–ë–ª–∏–∂–∞–π—à–∏–µ —Å–æ—Å–µ–¥–∏** (nearest neighbors; cosine).  
- **–ê–Ω–∞–ª–æ–≥–∏–∏** (word analogies): accuracy@k, –≤–ª–∏—è–Ω–∏–µ \(d\), \(C\), \(K\).  
- **–°—Ö–æ–¥—Å—Ç–≤–æ/—Ä–æ–¥—Å—Ç–≤–æ** (similarity/relatedness): —Ä–∞–Ω–≥–æ–≤–∞—è –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è (Spearman) —Å —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–º–∏ —Å–∫–æ—Ä–∞–º–∏ (WordSim-353, MEN, SimLex-999).

**–ö–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ:**
\[
\cos(\mathbf{a}, \mathbf{b})=\frac{\mathbf{a}^\top\mathbf{b}}{\lVert\mathbf{a}\rVert_2\,\lVert\mathbf{b}\rVert_2}.
\]

### 7.2 –í–Ω–µ—à–Ω–∏–µ (extrinsic)
–ò—Å–ø–æ–ª—å–∑—É–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –∫–∞–∫ –ø—Ä–∏–∑–Ω–∞–∫–∏ –≤ —Ä–µ–∞–ª—å–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö: **NER**, **POS-tagging**, **sentiment**, **parsing**, **QA**, **IR** ‚Äî —Å—á–∏—Ç–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏ –∑–∞–¥–∞—á–∏ (F1/accuracy/BLEU –∏ —Ç.–ø.).

---

## 8) –ú–Ω–æ–≥–æ–∑–Ω–∞—á–Ω–æ—Å—Ç—å (polysemy) –∏ –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø—Ä–æ—Ç–æ—Ç–∏–ø–æ–≤

–û–±—ã—á–Ω—ã–µ word2vec/GloVe –¥–∞—é—Ç **–æ–¥–∏–Ω** –≤–µ–∫—Ç–æ—Ä –Ω–∞ –ª–µ–∫—Å–µ–º—É ‚Üí —Å–º–µ—à–µ–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏–π. –ü–æ–¥—Ö–æ–¥—ã:
- **Global context** (—É—á—ë—Ç —Ç–µ–º—ã/–¥–æ–∫—É–º–µ–Ω—Ç–∞),
- **–ù–µ—Å–∫–æ–ª—å–∫–æ –ø—Ä–æ—Ç–æ—Ç–∏–ø–æ–≤** (multiple prototypes): –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤ –∏ –æ–±—É—á–µ–Ω–∏–µ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤ –Ω–∞ —Å–ª–æ–≤–æ.  
–°–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç ‚Äî **–∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏** (ELMo, BERT): –≤–µ–∫—Ç–æ—Ä –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –æ–∫—Ä—É–∂–µ–Ω–∏—è —Ç–æ–∫–µ–Ω–∞.

---

## 9) –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —Å–æ–≤–µ—Ç—ã (practical tips)

- **–†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å \(d\)**: 100‚Äì300‚Äì768+ (–∫–æ–º–ø—Ä–æ–º–∏—Å—Å –∫–∞—á–µ—Å—Ç–≤–æ/—Ä–µ—Å—É—Ä—Å—ã).  
- **–û–∫–Ω–æ \(C\)**: 2‚Äì10 (–º–∞–ª–æ–µ ‚Üí —Å–∏–Ω—Ç–∞–∫—Å–∏—Å; –±–æ–ª—å—à–æ–µ ‚Üí —Ç–µ–º–∞—Ç–∏–∫–∞).  
- **Negative samples \(K\)**: 5‚Äì20; **subsampling** —á–∞—Å—Ç—ã—Ö —Å–ª–æ–≤.  
- **–°—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏—è**: gradient clipping, early stopping, –∞–¥–µ–∫–≤–∞—Ç–Ω–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è, —Ñ–∏–∫—Å–∏—Ä—É–π seed.  
- **–û—Ü–µ–Ω–∫–∞**: –¥–µ–ª–∞–π –∏ **intrinsic**, –∏ **extrinsic**.

---

## 10) –ú–∏–Ω–∏-–ø—Ä–∞–∫—Ç–∏–∫—É–º (—á—Ç–æ —É–º–µ—Ç—å —Ä—É–∫–∞–º–∏)

**A. PPMI ‚Üí Truncated SVD**  
1) –ü–æ—Å—Ç—Ä–æ–∏—Ç—å \(X\) –ø–æ –æ–∫–Ω—É; 2) –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å –≤ **PPMI**; 3) –ü—Ä–∏–º–µ–Ω–∏—Ç—å **TruncatedSVD** (randomized) –Ω–∞ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ–π –º–∞—Ç—Ä–∏—Ü–µ; 4) –ù–æ—Ä–º–∏—Ä–æ–≤–∞—Ç—å, –ø—Ä–æ–≤–µ—Ä–∏—Ç—å neighbors/analogies.

**B. Skip-gram + NS (—Å–∫–µ–ª–µ—Ç —à–∞–≥–∞)**  
1) –î–ª—è \((\text{center}, \text{context})\) –æ–±–Ω–æ–≤–∏—Ç—å \(\mathbf{v}_{\text{center}}, \mathbf{u}_{\text{context}}\).  
2) –î–ª—è –Ω–µ–≥–∞—Ç–∏–≤–æ–≤ \(w_k\sim P_n\) –æ–±–Ω–æ–≤–∏—Ç—å \(\mathbf{u}_{w_k}\) –∏ \(\mathbf{v}_{\text{center}}\).  
3) –≠–ø–æ—Ö–∏, lr-scheduler, early-stopping.

**C. GloVe (–∏–≥—Ä—É—à–µ—á–Ω—ã–π —à–∞–≥)**  
–î–ª—è –∫–∞–∂–¥–æ–π —Ç—Ä–æ–π–∫–∏ \((i,j,X_{ij})\) –ø–æ—Å—á–∏—Ç–∞—Ç—å \(f(X_{ij})\), \(\delta_{ij}\), —Å–¥–µ–ª–∞—Ç—å —à–∞–≥–∏ –ø–æ \(\mathbf{w}_i,\tilde{\mathbf{w}}_j,b_i,\tilde b_j\); –≤ –∫–æ–Ω—Ü–µ \( (\mathbf{w}_i+\tilde{\mathbf{w}}_i)/2 \).

---

## 11) –ú–∏–Ω–∏-—à–ø–∞—Ä–≥–∞–ª–∫–∞ —Ñ–æ—Ä–º—É–ª

**PMI/PPMI:**
\[
\operatorname{PMI}(i,j)=\log\frac{p(i,j)}{p(i)p(j)},\qquad
\operatorname{PPMI}(i,j)=\max\{\operatorname{PMI}(i,j),0\}.
\]

**SVD (—Ä–∞–Ω–≥-\(k\)) –∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è:**
\[
X\approx U_k\Sigma_k V_k^\top,\quad
\text{word}\approx \text{row}(U_k\Sigma_k^{1/2}),\ 
\text{context}\approx \text{row}(V_k\Sigma_k^{1/2}).
\]

**GloVe loss:**
\[
J=\sum_{i,j} f(X_{ij})
\big(\mathbf{w}_i^\top\tilde{\mathbf{w}}_j+b_i+\tilde b_j-\log X_{ij}\big)^2.
\]

**3CosAdd (–∞–Ω–∞–ª–æ–≥–∏–∏):**
\[
\arg\max_{w}\ \cos\!\big(\mathbf{v}_w,\ \mathbf{v}_{b}-\mathbf{v}_{a}+\mathbf{v}_{c}\big).
\]

**Cosine:**
\[
\cos(\mathbf{a},\mathbf{b})=\frac{\mathbf{a}^\top\mathbf{b}}
{\lVert\mathbf{a}\rVert_2\,\lVert\mathbf{b}\rVert_2}.
\]

---

## 12) –õ–∏—Ç–µ—Ä–∞—Ç—É—Ä–∞ (–¥–ª—è –∑–∞–∫—Ä–µ–ø–ª–µ–Ω–∏—è)

- **GloVe**: Pennington, Socher, Manning (EMNLP 2014).  
- **Levy et al. (2015)**: —Å–≤—è–∑—å count-based –∏ predictive.  
- **Schnabel et al. (2015)**: intrinsic-–æ—Ü–µ–Ω–∫–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤.  
- **Arora et al. (2016/2017/2018)**: PMI-–ø–æ–¥—Ö–æ–¥ –∏ –ª–∏–Ω–µ–π–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ —Å–º—ã—Å–ª–æ–≤.  
- **Yin et al. (2018)**: –æ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤.

---

## üîé –ò—Ç–æ–≥

–õ–µ–∫—Ü–∏—è –≤—ã–≤–æ–¥–∏—Ç –∑–∞ —Ä–∞–º–∫–∏ word2vec: **co-occurrence ‚Üí PPMI ‚Üí SVD**, –≤–≤–æ–¥–∏—Ç **GloVe** –∫–∞–∫ —Ä–µ–≥—Ä–µ—Å—Å–∏—é –∫ \(\log X_{ij}\) —Å –≤–µ—Å–∞–º–∏, –æ–±—Å—É–∂–¥–∞–µ—Ç **SGD/–≥—Ä–∞–¥–∏–µ–Ω—Ç—ã**, **–æ—Ü–µ–Ω–∫—É —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤** (intrinsic/extrinsic) –∏ **–º–Ω–æ–≥–æ–∑–Ω–∞—á–Ω–æ—Å—Ç—å** (polysemy). –≠—Ç–æ –±–∞–∑–∞ –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π (**ELMo/BERT**), –≥–¥–µ –≤–µ–∫—Ç–æ—Ä –∑–∞–≤–∏—Å–∏—Ç –æ—Ç **–∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞** —Ç–æ–∫–µ–Ω–∞.

