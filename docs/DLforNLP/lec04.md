# üìò Lecture 4 ‚Äî Language Models & Recurrent Neural Networks (2024W)

---

## 1) –ß—Ç–æ —Ç–∞–∫–æ–µ Language Modeling (LM)

**Language Model (—è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å)** ‚Äî –º–æ–¥–µ–ª—å, –∫–æ—Ç–æ—Ä–∞—è —É—á–∏—Ç—Å—è –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—Ç—å –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Å–ª–æ–≤.

–§–æ—Ä–º–∞–ª—å–Ω–æ: –¥–ª—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Å–ª–æ–≤  
\[
w_1, w_2, ..., w_T
\]

—è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å –∑–∞–¥–∞—ë—Ç —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ

\[
P(w_1, w_2, ..., w_T)
\]

–ß–∞—â–µ ‚Äî –∞–≤—Ç–æ–≥—Ä–µ—Å—Å–∏–≤–Ω–æ (chain rule):

\[
P(w_1, ..., w_T) = \prod_{t=1}^{T} P(w_t \mid w_{1:t-1})
\]

**–ò–Ω—Ç—É–∏—Ç–∏–≤–Ω–æ:** LM —É—á–∏—Ç—Å—è –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—Ç—å —Å–ª–µ–¥—É—é—â–µ–µ —Å–ª–æ–≤–æ –ø–æ –∏—Å—Ç–æ—Ä–∏–∏.

–ü—Ä–∏–º–µ—Ä:
> *The cat sat on the ...* ‚Üí *mat*

---

## 2) –ì–¥–µ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è LM

- –∞–≤—Ç–æ–¥–æ–ø–æ–ª–Ω–µ–Ω–∏–µ (autocomplete)
- –ø–æ–∏—Å–∫
- —á–∞—Ç-–±–æ—Ç—ã
- —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ä–µ—á–∏
- –º–∞—à–∏–Ω–Ω—ã–π –ø–µ—Ä–µ–≤–æ–¥
- –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞

**–¢—ã –∏—Å–ø–æ–ª—å–∑—É–µ—à—å LM –∫–∞–∂–¥—ã–π –¥–µ–Ω—å.**

---

## 3) n-gram Language Models

–ò–¥–µ—è: –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å —Å–ª–æ–≤–∞ –∑–∞–≤–∏—Å–∏—Ç —Ç–æ–ª—å–∫–æ –æ—Ç –ø–æ—Å–ª–µ–¥–Ω–∏—Ö \(n-1\) —Å–ª–æ–≤:

\[
P(w_t \mid w_{1:t-1}) \approx P(w_t \mid w_{t-n+1:t-1})
\]

–ü—Ä–∏–º–µ—Ä—ã:
- Unigram:    \(P(w_t)\)
- Bigram:     \(P(w_t \mid w_{t-1})\)
- Trigram:    \(P(w_t \mid w_{t-2}, w_{t-1})\)

–û—Ü–µ–Ω–∫–∞ —á–∞—Å—Ç–æ—Ç–æ–π (MLE):

\[
P(w_t \mid w_{t-1}) = \frac{count(w_{t-1}, w_t)}{count(w_{t-1})}
\]

---

### –ü—Ä–æ–±–ª–µ–º—ã n-gram LM

**1) –†–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ—Å—Ç—å (sparsity)**  
–ú–Ω–æ–≥–∏–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –Ω–µ –≤—Å—Ç—Ä–µ—á–∞—é—Ç—Å—è ‚Üí —à–∞–Ω—Å –Ω–æ–ª—å.

**2) –í–∑—Ä—ã–≤ –ø–∞–º—è—Ç–∏ (storage)**  
–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ n-–≥—Ä–∞–º —ç–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–æ —Ä–∞—Å—Ç—ë—Ç.

**3) –ü–ª–æ—Ö–æ–µ –æ–±–æ–±—â–µ–Ω–∏–µ**  
–ù–µ –≤–∏–¥–µ–ª–∏ —Ñ—Ä–∞–∑—É ‚Üí –º–æ–¥–µ–ª—å –ø–∞–¥–∞–µ—Ç.

–ò—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è —Å–≥–ª–∞–∂–∏–≤–∞–Ω–∏—è (smoothing):  
Laplace, Kneser-Ney, back-off, interpolation.

---

## 4) –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ n-gram LM

–ê–ª–≥–æ—Ä–∏—Ç–º:
1) –±–µ—Ä—ë–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ \(n-1\) —Å–ª–æ–≤
2) –≤—ã–±–∏—Ä–∞–µ–º —Å–ª–µ–¥—É—é—â–µ–µ —Å–ª–æ–≤–æ –ø–æ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏
3) –ø–æ–≤—Ç–æ—Ä—è–µ–º

–ü—Ä–æ–±–ª–µ–º–∞ ‚Äî **–∂—ë—Å—Ç–∫–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç, –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –Ω–∞ –ø–∞–º—è—Ç—å, –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏—è**.

---

## 5) Neural Language Models

–ú–æ—Ç–∏–≤–∞—Ü–∏—è: –∏–∑–±–µ–∂–∞—Ç—å –ø—Ä–æ–±–ª–µ–º n-gram, –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å **–≤–µ–∫—Ç–æ—Ä—ã** –∏ **–Ω–µ–π—Ä–æ—Å–µ—Ç–∏**.

–ò–¥–µ—è:

\[
P(w_t \mid w_{1:t-1}) = \operatorname{softmax}(W h_{t-1} + b)
\]

–≥–¥–µ  
- \(h_{t-1}\) ‚Äî —Å–∫—Ä—ã—Ç–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
- –º–æ–¥–µ–ª—å —É—á–∏—Ç **—Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ** —Å–ª–æ–≤

---

### Fixed-window Neural LM (–∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–π —à–∞–≥ –¥–æ RNN)

\[
x_t = [e(w_{t-n+1}); ...; e(w_{t-1})]
\]

\[
h = f(Wx_t + b), \quad
P(w_t) = \operatorname{softmax}(U h + c)
\]

–ú–∏–Ω—É—Å ‚Äî **—Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –æ–∫–Ω–æ** ‚Üí –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç.

---

## 6) RNN ‚Äî Recurrent Neural Networks

–ú–æ—Ç–∏–≤–∞—Ü–∏—è: –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å **–¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–º**, –Ω–µ —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–º.

**–†–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω–∞—è —Ñ–æ—Ä–º—É–ª–∞:**

\[
h_t = f(W_{xh} x_t + W_{hh} h_{t-1} + b)
\]

\[
P(w_t \mid w_{1:t-1}) = \operatorname{softmax}(W_{hy} h_t)
\]

–ì–¥–µ:
- \(x_t = e(w_t)\) ‚Äî embedding —Å–ª–æ–≤–∞
- \(h_t\) ‚Äî —Å–∫—Ä—ã—Ç–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ
- \(f\) ‚Äî –æ–±—ã—á–Ω–æ tanh –∏–ª–∏ ReLU

---

## 7) RNN Language Model

**–ü–æ—à–∞–≥–æ–≤–æ:**
1) –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º \(h_0 = 0\)
2) –ü–æ–ª—É—á–∞–µ–º embedding –≤—Ö–æ–¥–Ω–æ–≥–æ —Å–ª–æ–≤–∞
3) –û–±–Ω–æ–≤–ª—è–µ–º \(h_t\)
4) –ü—Ä–æ–≥–Ω–æ–∑–∏—Ä—É–µ–º —Å–ª–µ–¥—É—é—â–µ–µ —Å–ª–æ–≤–æ —á–µ—Ä–µ–∑ softmax

**–°—Ö–µ–º–Ω–æ**
w_{t-1} ‚Üí Emb ‚Üí x_{t-1} ‚Üí RNN ‚Üí h_{t-1} ‚Üí Softmax ‚Üí P(w_t)


---

## 8) –û–±—É—á–µ–Ω–∏–µ RNN LM

–õ–æ—Å—Å: **Cross-Entropy** –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —à–∞–≥–∞

\[
\mathcal{L} = - \sum_{t=1}^{T} \log P(w_t \mid w_{1:t-1})
\]

–¶–µ–ø–Ω–æ–µ –ø—Ä–∞–≤–∏–ª–æ —á–µ—Ä–µ–∑ –≤—Ä–µ–º—è

---

## 9) Backpropagation Through Time (BPTT)

RNN —Ä–∞–∑–≤–æ—Ä–∞—á–∏–≤–∞–µ—Ç—Å—è –ø–æ –≤—Ä–µ–º–µ–Ω–∏:

\[
h_t = f(...h_{t-1}...)
\]

–ì—Ä–∞–¥–∏–µ–Ω—Ç –∫–æ–ø–∏—Ç—Å—è —á–µ—Ä–µ–∑ –≤—Å–µ —à–∞–≥–∏

**–ü—Ä–æ–±–ª–µ–º–∞:**
- *vanishing gradients*
- *exploding gradients*

–†–µ—à–µ–Ω–∏—è:
- gradient clipping
- LSTM / GRU (–¥–∞–ª—å—à–µ –≤ –∫—É—Ä—Å–µ)

---

## 10) –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ RNN

1) –≤–∑—è—Ç—å —Å—Ç–∞—Ä—Ç–æ–≤—ã–π —Ç–æ–∫–µ–Ω `<BOS>`
2) –ø–æ \(P(w)\) –≤—ã–±—Ä–∞—Ç—å —Å–ª–µ–¥—É—é—â–µ–µ —Å–ª–æ–≤–æ
3) –ø–æ–¥–∞—Ç—å –µ–≥–æ –æ–±—Ä–∞—Ç–Ω–æ –≤ –º–æ–¥–µ–ª—å
4) –ø–æ–≤—Ç–æ—Ä—è—Ç—å

–ú–æ–∂–Ω–æ –¥–µ–ª–∞—Ç—å sampling, top-k, temperature, nucleus (–ø–æ–∑–∂–µ –≤ –∫—É—Ä—Å–µ).

---

## 11) –ü–µ—Ä–ø–ª–µ–∫c–∏—è (Perplexity)

–ú–µ—Ç—Ä–∏–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ LM:

\[
\text{PPL} = \exp\left( \frac{1}{N} \sum_{t=1}^N -\log P(w_t) \right)
\]

–ù–∏–∂–µ = –ª—É—á—à–µ.

---

## 12) –ó–∞—á–µ–º –Ω–∞–º LM –∏ RNN?

- –≥–µ–Ω–µ—Ä–∞—Ü–∏—è
- –∞–≤—Ç–æ–¥–æ–ø–æ–ª–Ω–µ–Ω–∏–µ
- tagging (POS, NER)
- sentiment classification
- encoder –≤ seq2seq (MT, QA)
- speech recognition
- summarization

---

## 13) –ö—Ä–∞—Ç–∫–∏–π –∏—Ç–æ–≥

| –ú–æ–¥–µ–ª—å | –ü–ª—é—Å—ã | –ú–∏–Ω—É—Å—ã |
|---|---|---|
| n-gram | –ø—Ä–æ—Å—Ç–æ | –Ω–µ—Ç –ø–∞–º—è—Ç–∏, —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ—Å—Ç—å |
| FFNN-LM | —É—á–∏—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ | —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –æ–∫–Ω–æ |
| RNN-LM | **–¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç** | BPTT, –º–µ–¥–ª–µ–Ω–Ω–æ, vanishing grads |

---
## 14) –ú–∏–Ω–∏-–ø—Ä–∏–º–µ—Ä: RNN Language Model (PyTorch, LSTM)

–ù–∏–∂–µ ‚Äî –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Å–∫–µ–ª–µ—Ç **LSTM-—è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏** —É—Ä–æ–≤–Ω—è —Å–ª–æ–≤:
- —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –∏ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ —Å–ª–æ–≤–∞—Ä—è;
- —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–∞—Ä (–≤—Ö–æ–¥-–ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å ‚Üí —Å–¥–≤–∏–Ω—É—Ç—ã–µ —Ü–µ–ª–∏);
- –º–æ–¥–µ–ª—å: `Embedding ‚Üí LSTM ‚Üí Linear`;
- –ª–æ—Å—Å: `CrossEntropyLoss` –ø–æ–≤–µ—Ä—Ö –ª–æ–≥–∏—Ç–æ–≤ `[B*T, V]`.

```python
import math
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader

# ===== 1) –ò–≥—Ä—É—à–µ—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ (–∑–∞–º–µ–Ω–∏ —Å–≤–æ–∏–º–∏) =====
corpus = """
the cat sat on the mat
the dog sat on the log
the cat chased the dog
the dog chased the cat
""".strip().lower().split()

# ===== 2) –°–ª–æ–≤–∞—Ä—å =====
vocab = sorted(set(corpus))
stoi = {w:i for i,w in enumerate(vocab)}
itos = {i:w for w,i in stoi.items()}
V = len(vocab)

# ===== 3) –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è =====
tokens = torch.tensor([stoi[w] for w in corpus], dtype=torch.long)

# ===== 4) –°–µ–∫–≤–µ–Ω—Å—ã –¥–ª—è LM: –≤—Ö–æ–¥ = w[0:T-1], —Ü–µ–ª—å = w[1:T] =====
# —Ä–∞–∑–æ–±—å—ë–º –¥–ª–∏–Ω–Ω—É—é –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –Ω–∞ –±–∞—Ç—á–∏ —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –¥–ª–∏–Ω—ã
def make_sequences(tokens, seq_len):
    xs, ys = [], []
    for i in range(0, len(tokens) - seq_len):
        x = tokens[i : i + seq_len]
        y = tokens[i + 1 : i + seq_len + 1]
        xs.append(x)
        ys.append(y)
    return torch.stack(xs), torch.stack(ys)

SEQ_LEN = 6
X, Y = make_sequences(tokens, SEQ_LEN)  # [N, T], [N, T]
dataset = torch.utils.data.TensorDataset(X, Y)
loader = DataLoader(dataset, batch_size=8, shuffle=True, drop_last=True)

# ===== 5) –ú–æ–¥–µ–ª—å: Embedding ‚Üí LSTM ‚Üí Linear =====
class LSTMLM(nn.Module):
    def __init__(self, vocab_size, d_emb=64, d_hid=128, n_layers=1, dropout=0.1):
        super().__init__()
        self.emb = nn.Embedding(vocab_size, d_emb)
        self.lstm = nn.LSTM(d_emb, d_hid, num_layers=n_layers, batch_first=True, dropout=0.0 if n_layers==1 else dropout)
        self.drop = nn.Dropout(dropout)
        self.fc = nn.Linear(d_hid, vocab_size)

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è (–•–µ–π/–ö–∞–π–º–∏–Ω–≥ –¥–ª—è fc –Ω–µ –æ–±—è–∑–∞—Ç–µ–ª–µ–Ω, –Ω–æ –º–æ–∂–Ω–æ)
        for name, param in self.lstm.named_parameters():
            if "weight_ih" in name:
                nn.init.xavier_uniform_(param)
            elif "weight_hh" in name:
                nn.init.orthogonal_(param)
            elif "bias" in name:
                nn.init.zeros_(param)

    def forward(self, x, h=None):
        # x: [B, T]
        e = self.emb(x)                 # [B, T, d_emb]
        out, h = self.lstm(e, h)        # out: [B, T, d_hid]
        out = self.drop(out)
        logits = self.fc(out)           # [B, T, V]
        return logits, h

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = LSTMLM(V).to(device)
optim = torch.optim.AdamW(model.parameters(), lr=2e-3, weight_decay=1e-2)

# ===== 6) –¢—Ä–µ–Ω–∏—Ä–æ–≤–∫–∞ =====
EPOCHS = 30
for epoch in range(1, EPOCHS+1):
    model.train()
    total_loss = 0.0
    for xb, yb in loader:
        xb = xb.to(device)  # [B, T]
        yb = yb.to(device)  # [B, T]
        optim.zero_grad()
        logits, _ = model(xb)                 # [B, T, V]
        loss = F.cross_entropy(logits.reshape(-1, V), yb.reshape(-1))
        loss.backward()
        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        optim.step()
        total_loss += loss.item()
    ppl = math.exp(total_loss / len(loader))
    if epoch % 5 == 0 or epoch == 1:
        print(f"epoch {epoch:02d} | loss {total_loss/len(loader):.4f} | ppl {ppl:.2f}")

# ===== 7) –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ (greedy; –º–æ–∂–Ω–æ –∑–∞–º–µ–Ω–∏—Ç—å –Ω–∞ top-k/temperature) =====
@torch.no_grad()
def generate(model, seed_tokens, max_new_tokens=10):
    model.eval()
    x = torch.tensor([seed_tokens], dtype=torch.long, device=device)  # [1, T]
    h = None
    for _ in range(max_new_tokens):
        logits, h = model(x[:, -SEQ_LEN:], h)  # –±–µ—Ä–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ SEQ_LEN —Ç–æ–∫–µ–Ω–æ–≤
        next_logit = logits[:, -1, :]          # [1, V] ‚Äî –ª–æ–≥–∏—Ç—ã –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–≥–æ —à–∞–≥–∞
        next_id = torch.argmax(next_logit, dim=-1)  # greedy
        x = torch.cat([x, next_id.unsqueeze(1)], dim=1)
    return [itos[int(i)] for i in x[0].tolist()]

seed = ["the", "cat", "sat", "on", "the", "mat"]
seed_ids = [stoi[w] for w in seed]
print("SEED:", " ".join(seed))
print("GEN :", " ".join(generate(model, seed_ids, max_new_tokens=8)))
```

---

## 15) –ü—Ä–æ–±–ª–µ–º—ã RNN

### 1) Vanishing gradients (–∑–∞—Ç—É—Ö–∞—é—â–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã)
–ü—Ä–∏ –¥–ª–∏–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—è—Ö –≤–∫–ª–∞–¥ —Ä–∞–Ω–Ω–∏—Ö —à–∞–≥–æ–≤ –∏—Å—á–µ–∑–∞–µ—Ç:

\[
\frac{\partial \mathcal{L}}{\partial h_0}
\rightarrow 0
\]

‚Üí –º–æ–¥–µ–ª—å –∑–∞–±—ã–≤–∞–µ—Ç –¥–∞–ª—å–Ω–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç.

### 2) Exploding gradients (–≤–∑—Ä—ã–≤–∞—é—â–∏–µ—Å—è –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã)
\[
\frac{\partial \mathcal{L}}{\partial h_t}
\rightarrow \infty
\]

‚Üí –≤–µ—Å–æ–≤—ã–µ –º–∞—Ç—Ä–∏—Ü—ã ‚Äú–≤–∑—Ä—ã–≤–∞—é—Ç—Å—è‚Äù, loss = NaN

### –†–µ—à–µ–Ω–∏—è
| –ü—Ä–æ–±–ª–µ–º–∞ | –†–µ—à–µ–Ω–∏–µ |
|---|---|
vanishing | gating (LSTM / GRU), skip-connections |
exploding | gradient clipping |
–º–µ–¥–ª–µ–Ω–Ω–æ | minibatches, CuDNN, truncation (TBPTT) |

**Gradient clipping** —É–∂–µ –±—ã–ª –≤ –∫–æ–¥–µ:
\[
\text{clip}(\lVert g \rVert_2) \le 1.0
\]

---

## 16) Teacher forcing

–ü—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏: –Ω–∞ –≤—Ö–æ–¥ RNN –ø–æ–¥–∞—ë–º **–∏—Å—Ç–∏–Ω–Ω—ã–µ** —Ç–æ–∫–µ–Ω—ã
\[
x_t = \text{embed}(w_t)
\]

–ü—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: –ø–æ–¥–∞—ë–º **–ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ**
\[
x_t = \text{embed}(\hat{w}_t)
\]

Teacher forcing —É—Å–∫–æ—Ä—è–µ—Ç –æ–±—É—á–µ–Ω–∏–µ, –Ω–æ –≤—ã–∑—ã–≤–∞–µ—Ç **exposure bias**:
–º–æ–¥–µ–ª—å –Ω–µ –ø—Ä–∏–≤—ã–∫–∞–µ—Ç –∫ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–º –æ—à–∏–±–∫–∞–º.

---

## 17) Sampling —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏

### Greedy decoding
\[
\hat{w}_t = \arg\max p(w_t)
\]
+ –±—ã—Å—Ç—Ä–æ  
‚àí –º–æ–∂–µ—Ç –∑–∞—Å—Ç—Ä—è—Ç—å –≤ –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏—è—Ö

### Temperature
\[
p_i = \frac{\exp(z_i / T)}{\sum_j \exp(z_j / T)}
\]

- \(T < 1\) ‚Üí —É–≤–µ—Ä–µ–Ω–Ω—ã–π, –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç  
- \(T > 1\) ‚Üí –±–æ–ª—å—à–µ —Ç–≤–æ—Ä—á–µ—Å—Ç–≤–∞

### Top-k
–°–Ω–∞—á–∞–ª–∞ –æ—Å—Ç–∞–≤–ª—è–µ–º k —Å–∞–º—ã—Ö –≤–µ—Ä–æ—è—Ç–Ω—ã—Ö —Å–ª–æ–≤, –ø–æ—Ç–æ–º –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º softmax

### Nucleus / Top-p
–í—ã–±–∏—Ä–∞–µ–º **–º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –Ω–∞–±–æ—Ä —Å–ª–æ–≤**, —Å—É–º–º–∞ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π –∫–æ—Ç–æ—Ä—ã—Ö ‚â• p

p ‚âà 0.9 ‚Üí —Ä–∞–±–æ—Ç–∞–µ—Ç –ª—É—á—à–µ –≤—Å–µ–≥–æ –Ω–∞ –ø—Ä–∞–∫—Ç–∏–∫–µ

| –ú–µ—Ç–æ–¥ | –ö–æ–≥–¥–∞ |
|---|---|
Greedy | –ø—Ä–æ—Å—Ç—ã–µ –∑–∞–¥–∞—á–∏ |
Top-k | –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º—ã–π —Ä–∞–Ω–¥–æ–º |
Top-p | —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–π –¥–µ—Ñ–æ–ª—Ç (GPT-3/4/5 style) |

---

## 18) RNN vs FFNN LM

| –°–≤–æ–π—Å—Ç–≤–æ | FFNN LM | RNN LM |
|---|---|---|
–ö–æ–Ω—Ç–µ–∫—Å—Ç | —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –æ–∫–Ω–æ | –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π, –ø–æ—Ç–µ–Ω—Ü. –±–µ—Å–∫–æ–Ω–µ—á–Ω—ã–π |
–ü–∞–º—è—Ç—å | –Ω–µ—Ç | –µ—Å—Ç—å |
–û–±—É—á–µ–Ω–∏–µ | –±—ã—Å—Ç—Ä–æ | –º–µ–¥–ª–µ–Ω–Ω–µ–µ |
–ü—Ä–æ–±–ª–µ–º—ã | –º–∞–ª–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ | vanishing / exploding |
–ú–æ–¥–µ–ª–∏ | historical | –≤—Å—ë –µ—â—ë —Å–∏–ª—å–Ω—ã–µ, –Ω–æ –∑–∞–º–µ–Ω–µ–Ω—ã —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞–º–∏ |

---

## 19) RNN –≤ NLP ‚Äî –≥–¥–µ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è

- language modeling
- text classification
- POS tagging
- NER (–¥–æ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤)
- speech (–¥–æ attention-based –º–æ–¥–µ–ª–µ–π)
- encoder –≤ seq2seq MT (–¥–æ Transformers)

–°–µ–π—á–∞—Å: **RNN –≤—Å—ë –µ—â—ë –≤—Å—Ç—Ä–µ—á–∞—é—Ç—Å—è –≤ speech**, –Ω–æ **Transformers –ø–æ—á—Ç–∏ –ø–æ–ª–Ω–æ—Å—Ç—å—é –≤—ã—Ç–µ—Å–Ω–∏–ª–∏ –∏—Ö –≤ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –∑–∞–¥–∞—á–∞—Ö.**

---

## 20) –ü–µ—Ä–µ—Ö–æ–¥ –∫ LSTM/GRU

–ü–æ—á–µ–º—É –Ω–∞–º –Ω—É–∂–Ω—ã gate-–º–µ—Ö–∞–Ω–∏–∑–º—ã?

RNN —Å–∫—Ä—ã—Ç–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ –æ–±–Ω–æ–≤–ª—è–µ—Ç **–≤—Å—ë —Å—Ä–∞–∑—É**
\[
h_t = f(W_{xh} x_t + W_{hh} h_{t-1})
\]

–î–ª–∏–Ω–Ω—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ —Ç—Ä—É–¥–Ω–æ —Ö—Ä–∞–Ω–∏—Ç—å ‚Üí –Ω—É–∂–µ–Ω –º–µ—Ö–∞–Ω–∏–∑–º **–∫–æ–Ω—Ç—Ä–æ–ª—è –ø–æ—Ç–æ–∫–∞ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞ –∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏** ‚Üí LSTM, GRU.

**–ù–∞ —Å–ª–µ–¥—É—é—â–µ–π –ª–µ–∫—Ü–∏–∏:**
- LSTM: input/forget/output gate, cell state
- GRU: update/reset gate
- truncated BPTT
- seq2seq encoder-decoder

---

## 21) Summary

- LM —É—á–∏—Ç —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –Ω–∞–¥ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—è–º–∏ —Å–ª–æ–≤
- n-gram ‚Üí FFNN ‚Üí RNN‚Üí(LSTM/GRU)‚Üí Transformers
- RNN –æ–±–Ω–æ–≤–ª—è—é—Ç —Å–æ—Å—Ç–æ—è–Ω–∏–µ —Ä–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω–æ
- BPTT –æ–±—É—á–∞–µ—Ç —á–µ—Ä–µ–∑ —Ä–∞–∑–≤–æ—Ä–∞—á–∏–≤–∞–Ω–∏–µ –≤–æ –≤—Ä–µ–º–µ–Ω–∏
- –ì–ª–∞–≤–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã: vanishing & exploding gradients
- –†–µ—à–µ–Ω–∏—è: LSTM/GRU, gradient clipping
- –ú–µ—Ç—Ä–∏–∫–∏: cross-entropy, perplexity
- –ì–µ–Ω–µ—Ä–∞—Ü–∏—è: sampling, temperature, top-k, top-p

---
## 22) LSTM ‚Äî Long Short-Term Memory

### –ó–∞—á–µ–º –Ω—É–∂–µ–Ω LSTM?

–û–±—ã—á–Ω—ã–π RNN:
- –±—ã—Å—Ç—Ä–æ –∑–∞–±—ã–≤–∞–µ—Ç –¥–∞–ª—å–Ω–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç (vanishing gradients)
- —Å–ª–æ–∂–Ω–æ —É—á–∏—Ç –¥–æ–ª–≥–∏–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏

**–ò–¥–µ—è LSTM:** –¥–æ–±–∞–≤–∏—Ç—å –æ—Ç–¥–µ–ª—å–Ω–æ–µ ¬´–¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ¬ª (cell state) –∏ –º–µ—Ö–∞–Ω–∏–∑–º—ã (gates), –∫–æ—Ç–æ—Ä—ã–µ —Ä–µ—à–∞—é—Ç, —á—Ç–æ –∑–∞–ø–æ–º–∏–Ω–∞—Ç—å, –∑–∞–±—ã–≤–∞—Ç—å –∏ –≤—ã–¥–∞–≤–∞—Ç—å.

---

## 23) –§–æ—Ä–º—É–ª—ã LSTM

–û–±–æ–∑–Ω–∞—á–µ–Ω–∏—è:

- \(x_t\) ‚Äî embedding –≤—Ö–æ–¥–Ω–æ–≥–æ —Ç–æ–∫–µ–Ω–∞  
- \(h_t\) ‚Äî —Å–∫—Ä—ã—Ç–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ (hidden state)  
- \(c_t\) ‚Äî **—è—á–µ–π–∫–∞** (cell state) ‚Äî –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω–∞—è –ø–∞–º—è—Ç—å  
- \(\sigma\) ‚Äî sigmoid  
- \(*\) ‚Äî –ø–æ–∫–æ–º–ø–æ–Ω–µ–Ω—Ç–Ω–æ–µ —É–º–Ω–æ–∂–µ–Ω–∏–µ  

**–í—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –Ω–∞ —à–∞–≥–µ t:**
\[
x_t, \; h_{t-1}, \; c_{t-1}
\]

**LSTM-gates:**

Forget gate (—á—Ç–æ –∑–∞–±—ã—Ç—å):
\[
f_t = \sigma(W_f x_t + U_f h_{t-1} + b_f)
\]

Input gate (—á—Ç–æ –¥–æ–±–∞–≤–∏—Ç—å):
\[
i_t = \sigma(W_i x_t + U_i h_{t-1} + b_i)
\]

Candidate values (–Ω–æ–≤–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è):
\[
\tilde{c}_t = \tanh(W_c x_t + U_c h_{t-1} + b_c)
\]

–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ cell state:
\[
c_t = f_t * c_{t-1} + i_t * \tilde{c}_t
\]

Output gate (—á—Ç–æ –≤—ã–≤–µ—Å—Ç–∏ –Ω–∞—Ä—É–∂—É):
\[
o_t = \sigma(W_o x_t + U_o h_{t-1} + b_o)
\]

Hidden state:
\[
h_t = o_t * \tanh(c_t)
\]

---

## 24) –ò–Ω—Ç—É–∏—Ü–∏—è

| Gate | –ß—Ç–æ –¥–µ–ª–∞–µ—Ç | –í–æ–ø—Ä–æ—Å |
|---|---|---|
Forget | –æ—á–∏—â–∞–µ—Ç –ø–∞–º—è—Ç—å | —á—Ç–æ –±–æ–ª—å—à–µ –Ω–µ–≤–∞–∂–Ω–æ? |
Input | —Ä–µ—à–∞–µ—Ç, —á—Ç–æ –∑–∞–ø–∏—Å–∞—Ç—å | –∫–∞–∫—É—é –Ω–æ–≤—É—é –∏–Ω—Ñ—É –¥–æ–±–∞–≤–∏—Ç—å? |
Cell update | –æ–±–Ω–æ–≤–ª—è–µ—Ç –¥–æ–ª–≥—É—é –ø–∞–º—è—Ç—å | —á—Ç–æ —Ö—Ä–∞–Ω–∏—Ç—å –Ω–∞–¥–æ–ª–≥–æ? |
Output | —Ä–µ—à–∞–µ—Ç, —á—Ç–æ –≤–µ—Ä–Ω—É—Ç—å –Ω–∞—Ä—É–∂—É | —á—Ç–æ –≤–∞–∂–Ω–æ –ø—Ä—è–º–æ —Å–µ–π—á–∞—Å? |

LSTM = **–≥–∏–±—Ä–∏–¥ –∫—Ä–∞—Ç–∫–æ—Å—Ä–æ—á–Ω–æ–π –∏ –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω–æ–π –ø–∞–º—è—Ç–∏**.

---

## 25) LSTM –±–ª–æ–∫ ‚Äî ASCII —Å—Ö–µ–º–∞

  c_{t-1} ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
     ‚ñ≤                          ‚îÇ
     ‚îÇ                          ‚ñº
x_t ‚Üí[ gates ]‚Üí[ update ]‚Üí c_t ‚Üí tanh ‚Üí * ‚Üí h_t
‚ñ≤ ‚ñ≤
‚îÇ ‚îÇ
f,i,cÃÑ,o o_t

–∏–ª–∏ –ø—Ä–æ—â–µ:

[f_t, i_t, o_t, cÃÉ_t] = linear([x_t, h_{t-1}])
c_t = f_t * c_{t-1} + i_t * cÃÉ_t
h_t = o_t * tanh(c_t)


---

## 26) –ü–æ—á–µ–º—É LSTM —Ä–∞–±–æ—Ç–∞–µ—Ç —Ö–æ—Ä–æ—à–æ?

- cell state \(c_t\) –ø–æ–∑–≤–æ–ª—è–µ—Ç –ø–µ—Ä–µ–¥–∞–≤–∞—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é **—á–µ—Ä–µ–∑ —Å–æ—Ç–Ω–∏ —à–∞–≥–æ–≤**
- –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –∏–¥—É—Ç ¬´–º–∞–≥–∏—Å—Ç—Ä–∞–ª—å—é¬ª –ø–æ \(c_t\) ‚Üí –º–µ–Ω—å—à–µ –∑–∞—Ç—É—Ö–∞—é—Ç
- gating = –≤—ã–±–æ—Ä–æ—á–Ω–∞—è –ø–∞–º—è—Ç—å (—á—Ç–æ —Ö—Ä–∞–Ω–∏—Ç—å / –≤—ã–±—Ä–∞—Å—ã–≤–∞—Ç—å)

**–≠—Ç–æ —Å–ø–∞—Å–∞–µ—Ç –æ—Ç vanishing gradients.**

---

## 27) –ú–∏–Ω–∏-–ø—Ä–∏–º–µ—Ä LSTM —à–∞–≥–∞ —Ä—É–∫–∞–º–∏ (Python-–ø—Å–µ–≤–¥–æ–∫–æ–¥)

```python
def lstm_step(x_t, h_prev, c_prev, params):
    Wf, Wi, Wc, Wo, Uf, Ui, Uc, Uo, bf, bi, bc, bo = params

    f_t = sigmoid(Wf @ x_t + Uf @ h_prev + bf)
    i_t = sigmoid(Wi @ x_t + Ui @ h_prev + bi)
    c_hat = tanh(Wc @ x_t + Uc @ h_prev + bc)
    c_t = f_t * c_prev + i_t * c_hat
    o_t = sigmoid(Wo @ x_t + Uo @ h_prev + bo)
    h_t = o_t * tanh(c_t)
    return h_t, c_t
```

## 28) GRU ‚Äî Gated Recurrent Unit (–∫—Ä–∞—Ç–∫–æ –∏ –ø–æ–Ω—è—Ç–Ω–æ)

GRU ‚Äî —É–ø—Ä–æ—â—ë–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è LSTM:
- –Ω–µ—Ç –æ—Ç–¥–µ–ª—å–Ω–æ–≥–æ cell state
- –º–µ–Ω—å—à–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
- –±—ã—Å—Ç—Ä–µ–µ —É—á–∏—Ç—Å—è
- —á—É—Ç—å —Ö—É–∂–µ –Ω–∞ –æ—á–µ–Ω—å –¥–ª–∏–Ω–Ω—ã—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—è—Ö

**–§–æ—Ä–º—É–ª—ã**

–û–±–Ω–æ–≤–ª—è—é—â–∏–π gate:
\[
z_t = \sigma(W_z x_t + U_z h_{t-1})
\]

–°–±—Ä–∞—Å—ã–≤–∞—é—â–∏–π gate:
\[
r_t = \sigma(W_r x_t + U_r h_{t-1})
\]

–ö–∞–Ω–¥–∏–¥–∞—Ç–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ:
\[
\tilde{h}_t = \tanh(W_h x_t + U_h (r_t * h_{t-1}))
\]

–§–∏–Ω–∞–ª—å–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ:
\[
h_t = z_t * h_{t-1} + (1 - z_t) * \tilde{h}_t
\]

**–ò–Ω—Ç—É–∏—Ü–∏—è**

| Gate | –ß—Ç–æ –¥–µ–ª–∞–µ—Ç |
|---|---|
\(z_t\) | —Ä–µ—à–∞–µ—Ç, —Å–∫–æ–ª—å–∫–æ —Å—Ç–∞—Ä–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å |
\(r_t\) | —Ä–µ—à–∞–µ—Ç, —Å–∫–æ–ª—å–∫–æ –ø—Ä–æ—à–ª–æ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è —Å–±—Ä–æ—Å–∏—Ç—å |
\(\tilde{h}_t\) | –∫–∞–Ω–¥–∏–¥–∞—Ç–Ω–∞—è –Ω–æ–≤–∞—è –ø–∞–º—è—Ç—å |

---

## 29) –°—Ä–∞–≤–Ω–µ–Ω–∏–µ: RNN vs GRU vs LSTM

| –ú–æ–¥–µ–ª—å | –ü–∞–º—è—Ç—å | Gate-—ã | –ü–∞—Ä–∞–º–µ—Ç—Ä—ã | –°–∫–æ—Ä–æ—Å—Ç—å | –ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å |
|---|---|---|---|---|---|
RNN | —Å–ª–∞–±–∞—è | –Ω–µ—Ç | –º–∞–ª–æ | –±—ã—Å—Ç—Ä—ã–π | –æ—á–µ–Ω—å –∫–æ—Ä–æ—Ç–∫–∏–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ / baseline |
GRU | —Ö–æ—Ä–æ—à–∞—è | 2 | –º–µ–Ω—å—à–µ | –±—ã—Å—Ç—Ä–µ–µ LSTM | –∫–æ–≥–¥–∞ —Å–∫–æ—Ä–æ—Å—Ç—å –≤–∞–∂–Ω–µ–µ |
LSTM | –ª—É—á—à–∞—è | 3 + cell | –±–æ–ª—å—à–µ | –º–µ–¥–ª–µ–Ω–Ω–µ–µ | –¥–ª–∏–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç, NLP –∫–ª–∞—Å—Å–∏–∫–∞ |

**–∏–Ω—Ç—É–∏—Ç–∏–≤–Ω–æ:**

- LSTM = *–ø–∞–º—è—Ç—å-–º–∞–≥–∏—Å—Ç—Ä–∞–ª—å + —É–∑–ª—ã –∫–æ–Ω—Ç—Ä–æ–ª—è*
- GRU = *–ø–æ—á—Ç–∏ LSTM, –Ω–æ –∫–æ–º–ø–∞–∫—Ç–Ω–µ–µ*
- RNN = *–æ—á–µ–Ω—å –∫–æ—Ä–æ—Ç–∫–∞—è –ø–∞–º—è—Ç—å*

---

## 30) Truncated Backpropagation Through Time (TBPTT)

–ü–æ–ª–Ω—ã–π BPTT = —Ä–∞–∑–≤–æ—Ä–∞—á–∏–≤–∞–µ–º –≤—Å—é –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∏ —Å—á–∏—Ç–∞–µ–º –≥—Ä–∞–¥–∏–µ–Ω—Ç.  
–≠—Ç–æ **–¥–æ—Ä–æ–≥–æ** –∏ **–º–µ–¥–ª–µ–Ω–Ω–æ**, –æ—Å–æ–±–µ–Ω–Ω–æ –¥–ª—è –¥–ª–∏–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤.

–ò–¥–µ—è **truncated BPTT**: —Ä–µ–∂–µ–º –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –Ω–∞ –∫—É—Å–∫–∏.

–ù–∞–ø—Ä–∏–º–µ—Ä:

–ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç: w1 w2 w3 w4 w5 w6 w7 w8 w9 ‚Ä¶
–æ–±—É—á–µ–Ω–∏–µ BPTT: [w1..w4] ‚Üí —à–∞–≥ ‚Üí [w5..w8] ‚Üí —à–∞–≥ ‚Üí ‚Ä¶


- hidden state –ø–µ—Ä–µ–Ω–æ—Å–∏–º –º–µ–∂–¥—É –±–∞—Ç—á–∞–º–∏
- –≥—Ä–∞–¥–∏–µ–Ω—Ç –≤–Ω—É—Ç—Ä–∏ –æ–∫–Ω–∞

\[
h_{t_0} \to h_{t_1} \to ... \to h_{t_k}
\]

**–ó–∞—á–µ–º?**

| –ü–æ–ª–Ω—ã–π BPTT | Truncated BPTT |
|---|---|
—Ç–æ—á–Ω–æ | –±—ã—Å—Ç—Ä–µ–µ |
–¥–æ–ª–≥–æ | —Ö—É–∂–µ –ø–æ–º–Ω–∏—Ç —Å–≤–µ—Ä—Ö-–¥–ª–∏–Ω–Ω—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ |
–¥–æ—Ä–æ–≥–æ –ø–æ –ø–∞–º—è—Ç–∏ | —Å—Ç–∞–Ω–¥–∞—Ä—Ç –≤ –ø—Ä–∞–∫—Ç–∏–∫–µ |

---

## 31) Padding (–≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π)

–¢–µ–∫—Å—Ç—ã —Ä–∞–∑–Ω–æ–π –¥–ª–∏–Ω—ã ‚Üí –Ω—É–∂–Ω–æ –≤—ã—Ä–æ–≤–Ω—è—Ç—å batch.

–ü—Ä–∏–º–µ—Ä: —Ö–æ—Ç–∏–º batch —Ä–∞–∑–º–µ—Ä–æ–º 3

Input:
["cat sat", "the dog barked", "hello"]

–ü–æ—Å–ª–µ padding (PAD):
["cat sat ",
"the dog barked",
"hello "]


–ú—ã –¥–æ–±–∞–≤–∏–ª–∏ `<PAD>` —Ç–æ–∫–µ–Ω—ã, —á—Ç–æ–±—ã –¥–ª–∏–Ω—ã —Å–æ–≤–ø–∞–ª–∏.

---

## 32) Masking (–º–∞—Å–∫–∞ –¥–ª—è PAD —Ç–æ–∫–µ–Ω–æ–≤)

–ö–æ–≥–¥–∞ —Å—á–∏—Ç–∞–µ–º loss, –Ω–µ –¥–æ–ª–∂–Ω—ã —à—Ç—Ä–∞—Ñ–æ–≤–∞—Ç—å –º–æ–¥–µ–ª—å –∑–∞ `<PAD>`.

**Mask = 0 –¥–ª—è PAD, 1 –¥–ª—è –ø–æ–ª–µ–∑–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤**

\[
\mathcal{L} = -\sum_t m_t \log P(w_t)
\]

–≥–¥–µ \( m_t = 0 \) –µ—Å–ª–∏ —ç—Ç–æ PAD.

PyTorch —Å–Ω–∏–∂–∞–µ—Ç –±–æ–ª—å: `ignore_index=PAD_ID` –¥–µ–ª–∞–µ—Ç –º–∞—Å–∫—É —Å–∞–º:

```python
loss = F.cross_entropy(logits.reshape(-1, V), y.reshape(-1), ignore_index=pad_id)
```

## 33) Packed sequences –≤ RNN (PyTorch)

–ü—Ä–æ–±–ª–µ–º–∞: –µ—Å–ª–∏ –ø—Ä–æ—Å—Ç–æ –¥–æ–±–∞–≤–∏—Ç—å padding ‚Äî RNN –±—É–¥–µ—Ç –∑—Ä—è —Å—á–∏—Ç–∞—Ç—å `<PAD>` —Ç–æ–∫–µ–Ω—ã.

**–†–µ—à–µ–Ω–∏–µ:** –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å packing, —á—Ç–æ–±—ã RNN –∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞–ª pad –≤–Ω—É—Ç—Ä–∏ –≤—ã—á–∏—Å–ª–µ–Ω–∏–π:

–§—É–Ω–∫—Ü–∏–∏:
- `pad_sequence` ‚Äî –¥–æ–±–∞–≤–ª—è–µ—Ç padding
- `pack_padded_sequence` ‚Äî —É–±–∏—Ä–∞–µ—Ç –ø–∞–¥-—Ç–æ–∫–µ–Ω—ã –ø–µ—Ä–µ–¥ RNN
- `pad_packed_sequence` ‚Äî –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ–±—Ä–∞—Ç–Ω–æ —Ñ–æ—Ä–º–∞—Ç padded

### –ó–∞—á–µ–º —ç—Ç–æ –Ω—É–∂–Ω–æ?
| –ë–µ–∑ pack | –° pack |
|---|---|
RNN —Å—á–∏—Ç–∞–µ—Ç –≤—Å–µ —Ç–æ–∫–µ–Ω—ã, –≤–∫–ª—é—á–∞—è PAD | –°—á–∏—Ç–∞–µ—Ç —Ç–æ–ª—å–∫–æ —Ä–µ–∞–ª—å–Ω—ã–µ |
–º–µ–¥–ª–µ–Ω–Ω–µ–µ | –±—ã—Å—Ç—Ä–µ–µ |
–±–æ–ª—å—à–µ –ø–∞–º—è—Ç—å | –º–µ–Ω—å—à–µ –ø–∞–º—è—Ç—å |

---

## 34) –ö–∞–∫ —ç—Ç–æ –≤—ã–≥–ª—è–¥–∏—Ç (–ª–æ–≥–∏—á–µ—Å–∫–∏)

tokens (—Ä–∞–∑–Ω–æ–π –¥–ª–∏–Ω—ã)
‚Üì pad_sequence
padded batch + lengths
‚Üì pack_padded_sequence
packed batch (–±–µ–∑ PAD)
‚Üì RNN
packed outputs
‚Üì pad_packed_sequence
aligned outputs


**–í–∞–∂–Ω–æ:** –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã –ø–æ –¥–ª–∏–Ω–µ (PyTorch —Ç—Ä–µ–±—É–µ—Ç —ç—Ç–æ–≥–æ, –µ—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞—Ç—å `enforce_sorted=False`).

---

## 35) RNN batching & masking ‚Äî summary

| –¢–µ—Ö–Ω–∏–∫–∞ | –ó–∞—á–µ–º |
|---|---|
padding | –≤—ã—Ä–æ–≤–Ω—è—Ç—å –¥–ª–∏–Ω—ã |
mask / ignore_index | –Ω–µ —Å—á–∏—Ç–∞—Ç—å PAD –≤ loss |
packed sequence | –±—ã—Å—Ç—Ä–µ–µ, –ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã |
truncated BPTT | –¥–ª–∏–Ω–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã –æ–±—É—á–∞–µ–º —á–∞—Å—Ç—è–º–∏ |
gradient clipping | –æ—Ç –≤–∑—Ä—ã–≤–∞ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ |

**–ì–ª–∞–≤–Ω–∞—è –∏–¥–µ—è:**  
RNN-—ã –ª—é–±—è—Ç **–±–∞—Ç—á–∏, –º–∞—Å–∫—É, pack, –∫–æ—Ä–æ—Ç–∫–∏–µ –æ–∫–Ω–∞, clipping**.

---

## 36) –ü–µ—Ä–µ—Ö–æ–¥ –∫ Seq2Seq

–ú—ã —É–∂–µ –∑–Ω–∞–µ–º:
- LM (language modeling)
- RNN / LSTM / GRU
- BPTT & truncated BPTT
- padding + masking + packing

–¢–µ–ø–µ—Ä—å ‚Äî **Encoder-Decoder**:
- RNN —á–∏—Ç–∞–µ—Ç –≤—Ö–æ–¥ —Ü–µ–ª–∏–∫–æ–º ‚Üí –¥–µ–ª–∞–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç
- –¥—Ä—É–≥–æ–π RNN –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –≤—ã—Ö–æ–¥
- –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ MT (machine translation), summarization –∏ —Ç.–¥.

**–ù–æ:** –ø—Ä–æ—Å—Ç–æ–π Seq2Seq —Å—Ç—Ä–∞–¥–∞–µ—Ç ‚Üí –ø–æ—è–≤–∏—Ç—Å—è Attention ‚Üí –ø–æ—Ç–æ–º Transformers.

---

## 37) Seq2Seq (Sequence-to-Sequence) –º–æ–¥–µ–ª—å

–ó–∞–¥–∞—á–∞: –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å –æ–¥–Ω—É –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –≤ –¥—Ä—É–≥—É—é.

–ü—Ä–∏–º–µ—Ä—ã:
- machine translation:   *‚ÄúI love cats‚Äù ‚Üí ‚ÄúIch liebe Katzen‚Äù*
- summarization
- dialogue generation
- parsing
- speech ‚Üí text

---

## 38) –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ Encoder-Decoder (–±–µ–∑ attention)

**Encoder** —á–∏—Ç–∞–µ—Ç –≤—Ö–æ–¥ –∏ –ø—Ä–µ–≤—Ä–∞—â–∞–µ—Ç –µ–≥–æ –≤ —Å–∫—Ä—ã—Ç–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ (–∫–æ–Ω—Ç–µ–∫—Å—Ç).  
**Decoder** –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –≤—ã—Ö–æ–¥ –ø–æ –æ–¥–Ω–æ–º—É —Ç–æ–∫–µ–Ω—É.

–°—Ö–µ–º–∞:

Input sentence ‚Üí Encoder RNN ‚Üí Context vector ‚Üí Decoder RNN ‚Üí Output tokens


–§–æ—Ä–º–∞–ª—å–Ω–æ:

Encoder:
\[
h_t = \text{RNN}_{enc}(x_t, h_{t-1})
\]

–ö–æ–Ω—Ç–µ–∫—Å—Ç (–æ–¥–Ω–æ –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ):
\[
c = h_T
\]

Decoder:
\[
s_t = \text{RNN}_{dec}(y_{t-1}, s_{t-1}, c)
\]
\[
P(y_t) = \operatorname{softmax}(W s_t)
\]

---

## 39) –ü—Ä–æ–±–ª–µ–º–∞ –ø—Ä–æ—Å—Ç–æ–π Seq2Seq

–ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –≤–µ–∫—Ç–æ—Ä **–æ–¥–∏–Ω –Ω–∞ –≤—Å—ë –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ**.

–ï—Å–ª–∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –¥–ª–∏–Ω–Ω–æ–µ ‚Üí  
–º–æ–¥–µ–ª—å **–∑–∞–±—ã–≤–∞–µ—Ç –Ω–∞—á–∞–ª–æ, —Ç–µ—Ä—è–µ—Ç —Å–º—ã—Å–ª**.

| –°–∏–º–ø—Ç–æ–º | –ü—Ä–∏—á–∏–Ω–∞ |
|---|---|
–ø–ª–æ—Ö–æ–π –ø–µ—Ä–µ–≤–æ–¥ –¥–ª–∏–Ω–Ω—ã—Ö —Ñ—Ä–∞–∑ | —É–∑–∫–æ–µ ¬´–±—É—Ç—ã–ª–æ—á–Ω–æ–µ –≥–æ—Ä–ª—ã—à–∫–æ¬ª |
—Ç–µ—Ä—è—é—Ç—Å—è –¥–µ—Ç–∞–ª–∏ | –æ–¥–∏–Ω –≤–µ–∫—Ç–æ—Ä –Ω–µ —Ö—Ä–∞–Ω–∏—Ç –≤—Å—ë |
–≥—Ä—É–±–æ | —ç–Ω–∫–æ–¥–µ—Ä ‚Äú—Å–∂–∏–º–∞–µ—Ç‚Äù –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ |

**–≠—Ç–æ –∏ –ø—Ä–∏–≤–µ–ª–æ –∫ –ø–æ—è–≤–ª–µ–Ω–∏—é Attention.**

---

## 40) Teacher forcing –≤ Seq2Seq

–ü—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ decoder –ø–æ–ª—É—á–∞–µ—Ç **–ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –ø—Ä–µ–¥—ã–¥—É—â–∏–π —Ç–æ–∫–µ–Ω**, –∞ –Ω–µ —Å–≤–æ—ë –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ.

\[
\text{input to decoder at step t} = y_{t-1}^{\text{true}}
\]

–ü–ª—é—Å—ã:
- –±—ã—Å—Ç—Ä–µ–µ —Å—Ö–æ–¥–∏–º–æ—Å—Ç—å
- —Å—Ç–∞–±–∏–ª—å–Ω–µ–µ –æ–±—É—á–µ–Ω–∏–µ

–ú–∏–Ω—É—Å—ã:
- **exposure bias** (–º–æ–¥–µ–ª—å –Ω–µ —Ç—Ä–µ–Ω–∏—Ä—É–µ—Ç—Å—è –Ω–∞ —Å–≤–æ–∏—Ö –æ—à–∏–±–∫–∞—Ö)

---

## 41) Seq2Seq ‚Äî –≤—ã–≤–æ–¥ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π

–î–µ–∫–æ–¥–µ—Ä —Ä–∞–±–æ—Ç–∞–µ—Ç –∞–≤—Ç–æ-—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω–æ:

<BOS> ‚Üí y1 ‚Üí y2 ‚Üí ... ‚Üí <EOS>


–ú–æ–∂–Ω–æ:
- greedy decoding
- sampling
- beam search (—á–∞—â–µ –≤—Å–µ–≥–æ –≤ MT)

---

## 42) –ü—Å–µ–≤–¥–æ–∫–æ–¥ Seq2Seq

```python
# Encoder pass
h = enc_init()
for x_t in input_tokens:
    h = enc_rnn(x_t, h)
context = h

# Decoder pass
s = context
y_prev = <BOS>
for step in range(max_len):
    s = dec_rnn(y_prev, s, context)
    y_pred = softmax(W @ s)
    y_prev = argmax(y_pred)   # or sampling / beam search
```

## 43) –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è Seq2Seq –±–µ–∑ attention

| –ü—Ä–æ–±–ª–µ–º–∞ | –ß—Ç–æ –∑–Ω–∞—á–∏—Ç | –ü–æ—á–µ–º—É –ø–ª–æ—Ö–æ |
|---|---|---|
–û–¥–∏–Ω –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –≤–µ–∫—Ç–æ—Ä | –í—Å—è —Ñ—Ä–∞–∑–∞ ‚Üí –æ–¥–Ω—É —Ç–æ—á–∫—É | –ü–æ—Ç–µ—Ä—è –¥–µ—Ç–∞–ª–µ–π, –æ—Å–æ–±–µ–Ω–Ω–æ –≤ –¥–ª–∏–Ω–Ω—ã—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è—Ö |
–î–ª–∏–Ω–Ω—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ | –ú–æ–¥–µ–ª—å –ø–ª–æ—Ö–æ –ø–æ–º–Ω–∏—Ç –Ω–∞—á–∞–ª–æ | –£—Ö—É–¥—à–∞–µ—Ç—Å—è —Å–º—ã—Å–ª –ø–µ—Ä–µ–≤–æ–¥–∞ |
–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è ¬´—Å–∂–∞—Ç–∞¬ª | Encoder –æ–±—è–∑–∞–Ω —É–º–µ—Å—Ç–∏—Ç—å –≤—Å—ë | –£–∑–∫–æ–µ –±—É—Ç—ã–ª–æ—á–Ω–æ–µ –≥–æ—Ä–ª—ã—à–∫–æ |
–¢–æ—á–Ω–æ—Å—Ç—å | –•–æ—Ä–æ—à–æ –Ω–∞ –∫–æ—Ä–æ—Ç–∫–∏—Ö —Ñ—Ä–∞–∑–∞—Ö | –ü–∞–¥–∞–µ—Ç –Ω–∞ –¥–ª–∏–Ω–Ω—ã—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è—Ö |

**–ò–Ω—Ç—É–∏—Ü–∏—è:**  
–ú–æ–¥–µ–ª—å –∫–∞–∫ –±—É–¥—Ç–æ –ø—Ä–æ—á–∏—Ç–∞–ª–∞ –¥–ª–∏–Ω–Ω–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ,  
–∏ –µ–π –¥–∞—é—Ç `–æ–¥–∏–Ω` –≤–µ–∫—Ç–æ—Ä –∏ –≥–æ–≤–æ—Ä—è—Ç: *–Ω—É –¥–∞–≤–∞–π, –ø–µ—Ä–µ–≤–æ–¥–∏ —Ç–µ–ø–µ—Ä—å –≤—Å—ë –ø–æ –ø–∞–º—è—Ç–∏*.

‚Üí –≠—Ç–æ **–±–æ–ª—å—à–æ–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ**, –ø–æ—ç—Ç–æ–º—É –ø–æ–Ω–∞–¥–æ–±–∏–ª—Å—è –º–µ—Ö–∞–Ω–∏–∑–º **Attention**.

## 44) Attention ‚Äî –∏–¥–µ—è

–ú—ã —Ö–æ—Ç–∏–º, —á—Ç–æ–±—ã –≤–æ –≤—Ä–µ–º—è –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª—å –º–æ–≥–ª–∞ **—Å–º–æ—Ç—Ä–µ—Ç—å –Ω–∞ —Ä–∞–∑–Ω—ã–µ —á–∞—Å—Ç–∏ –≤—Ö–æ–¥–∞**, –∞ –Ω–µ —Ç–æ–ª—å–∫–æ –Ω–∞ –æ–¥–∏–Ω –∫–æ–Ω—Ç–µ–∫—Å—Ç.

–ò–¥–µ—è:
- —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –≤—Å–µ —Å–∫—Ä—ã—Ç—ã–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è encoder-–∞: \(h_1, h_2, ‚Ä¶, h_T\)
- –Ω–∞ –∫–∞–∂–¥–æ–º —à–∞–≥–µ decoder –±–µ—Ä—ë—Ç **–≤–∑–≤–µ—à–µ–Ω–Ω—É—é —Å—É–º–º—É** —ç—Ç–∏—Ö —Å–æ—Å—Ç–æ—è–Ω–∏–π

–ú–æ–¥–µ–ª—å —Å–∞–º–∞ —É—á–∏—Ç—Å—è, –Ω–∞ –∫–∞–∫–∏–µ —Å–ª–æ–≤–∞ –æ–±—Ä–∞—â–∞—Ç—å –≤–Ω–∏–º–∞–Ω–∏–µ.

---

## 45) –§–æ—Ä–º—É–ª—ã Attention (soft attention)

–ü—É—Å—Ç—å —É –Ω–∞—Å:
- encoder-states: \(h_1, \dots, h_T\)
- decoder state: \(s_t\)

**1) –í—ã—á–∏—Å–ª—è–µ–º –≤–µ—Å–∞ –≤–Ω–∏–º–∞–Ω–∏—è**
\[
e_{t,i} = s_t^\top h_i
\]
(–º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥—Ä—É–≥–∏–µ score functions ‚Äî additive, dot, etc.)

**2) –ü—Ä–µ–≤—Ä–∞—â–∞–µ–º –≤ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏**
\[
\alpha_{t,i} = \frac{\exp(e_{t,i})}{\sum_j \exp(e_{t,j})}
\]

**3) –ö–æ–Ω—Ç–µ–∫—Å—Ç = –≤–∑–≤–µ—à–µ–Ω–Ω–∞—è —Å—É–º–º–∞**
\[
c_t = \sum_{i=1}^{T} \alpha_{t,i} \, h_i
\]

**4) –ü–æ–¥–∞—ë–º –≤ decoder**
\[
s_t = \text{RNN}(y_{t-1}, s_{t-1}, c_t)
\]

---

## 46) –ò–Ω—Ç—É–∏—Ü–∏—è Attention

–ö–∞–∫ —á–∏—Ç–∞—Ç–µ–ª—å —Ç–µ–∫—Å—Ç–∞:
- –º—ã –Ω–µ –¥–µ—Ä–∂–∏–º –≤—Å—ë –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –≤ –æ–¥–Ω–æ–π —Ç–æ—á–∫–µ
- –º—ã ¬´—Å–∫–∞–Ω–∏—Ä—É–µ–º¬ª –Ω—É–∂–Ω—ã–µ —Å–ª–æ–≤–∞, –∫–æ–≥–¥–∞ —Å—Ç—Ä–æ–∏–º –ø–µ—Ä–µ–≤–æ–¥

Attention = **–¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∞—è –ø–∞–º—è—Ç—å**.

---

## 47) –ü—Ä–∏–º–µ—Ä –≤–Ω–∏–º–∞–Ω–∏—è (–∏–Ω—Ç—É–∏—Ç–∏–≤–Ω–æ)

Input: the cat sat on the mat
Weights: 0.1 0.7 0.2 0.0 0.0 0.0 ‚Üí –º–æ–¥–µ–ª—å ¬´—Å–º–æ—Ç—Ä–∏—Ç¬ª –Ω–∞ cat


–°–ª–µ–¥—É—é—â–µ–µ —Å–ª–æ–≤–æ –¥–ª—è –≤—ã–≤–æ–¥–∞ –±—É–¥–µ—Ç —Å–∏–ª—å–Ω–æ –∑–∞–≤–∏—Å–µ—Ç—å –æ—Ç `cat`.

---

## 48) –ü–æ—á–µ–º—É attention —Ä–∞–±–æ—Ç–∞–µ—Ç

| –ë–µ–∑ attention | –° attention |
|---|---|
–û–¥–∏–Ω –∫–æ–Ω—Ç–µ–∫—Å—Ç | –ö–æ–Ω—Ç–µ–∫—Å—Ç –Ω–∞ –∫–∞–∂–¥–æ–º —à–∞–≥–µ |
–¢–µ—Ä—è–µ—Ç—Å—è –Ω–∞—á–∞–ª–æ | –ú–æ–∂–Ω–æ –≤–µ—Ä–Ω—É—Ç—å—Å—è –∫ –ª—é–±–æ–º—É —Å–ª–æ–≤—É |
–ü–ª–æ—Ö —Å –¥–ª–∏–Ω–Ω—ã–º–∏ —Ñ—Ä–∞–∑–∞–º–∏ | –†–∞–±–æ—Ç–∞–µ—Ç –Ω–∞ –¥–ª–∏–Ω–Ω—ã—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è—Ö |
Seq2Seq —Å–ª–∞–±—ã–π | Seq2Seq —Å—Ç–∞–Ω–æ–≤–∏—Ç—Å—è —Å–∏–ª—å–Ω—ã–º |

## 49) Connection to Transformers

**–ö–ª—é—á–µ–≤–æ–π –º–æ–º–µ–Ω—Ç –∫—É—Ä—Å–∞:**

RNN: —Å—á–∏—Ç—ã–≤–∞–µ—Ç —Ç–æ–∫–µ–Ω—ã *–ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ*  
Transformers: —á–∏—Ç–∞—é—Ç *–≤—Å–µ —Ç–æ–∫–µ–Ω—ã —Å—Ä–∞–∑—É*, –∏—Å–ø–æ–ª—å–∑—É—é—Ç **self-attention**

| RNN | Transformer |
|---|---|
–ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã–π | –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π |
–≥—Ä–∞–¥–∏–µ–Ω—Ç—ã —á–µ—Ä–µ–∑ –≤—Ä–µ–º—è | attention –Ω–∞–ø—Ä—è–º—É—é |
–ø–ª–æ—Ö–æ —Å –¥–ª–∏–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º | —Ö–æ—Ä–æ—à–æ –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ—Ç—Å—è |
LSTM/GRU | self-attention blocks |

**Attention –≤–ø–µ—Ä–≤—ã–µ –ø–æ—è–≤–∏–ª—Å—è –≤ Seq2Seq,**  
–∞ –ø–æ—Ç–æ–º —Å—Ç–∞–ª –æ—Å–Ω–æ–≤–æ–π Transformers.

---

## 50) Self-Attention (–æ—á–µ–Ω—å –∫—Ä–∞—Ç–∫–æ)

–ï—Å–ª–∏ –≤ –æ–±—ã—á–Ω–æ–º attention decoder —Å–º–æ—Ç—Ä–∏—Ç –Ω–∞ encoder,  
—Ç–æ **self-attention** ‚Äî –º–æ–¥–µ–ª—å —Å–º–æ—Ç—Ä–∏—Ç –Ω–∞ *—Å–∞–º—É —Å–µ–±—è* –≤–Ω—É—Ç—Ä–∏ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏.

\[
\text{SelfAttention}(X) = \text{Attention}(X, X, X)
\]

–≠—Ç–æ –∑–∞–º–µ–Ω–∏–ª–æ —Ä–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω–æ—Å—Ç—å ‚Üí –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º ‚Üí –±—ã—Å—Ç—Ä–µ–µ ‚Üí GPT, BERT, T5, LLaMA –∏ —Ç.–¥.

(–ü–æ–¥—Ä–æ–±–Ω–æ –≤ —Å–ª–µ–¥—É—é—â–µ–π –ª–µ–∫—Ü–∏–∏.)

---

## 51) –ü–∞–º—è—Ç–∫–∞ –ø–æ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞–º

| –≠—Ç–∞–ø | –ò–¥–µ—è | –ù–µ–¥–æ—Å—Ç–∞—Ç–æ–∫ | –ß–µ–º –∑–∞–º–µ–Ω–∏–ª–∏ |
|---|---|---|---|
n-gram | —á–∞—Å—Ç–æ—Ç—ã | –Ω–µ—Ç –æ–±–æ–±—â–µ–Ω–∏—è | –Ω–µ–π—Ä–æ—Å–µ—Ç–∏ |
FFNN-LM | embeddings | —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –æ–∫–Ω–æ | RNN |
RNN | –ø–∞–º—è—Ç—å | vanishing gradients | LSTM/GRU |
LSTM / GRU | gates, –ø–∞–º—è—Ç—å | –º–µ–¥–ª–µ–Ω–Ω—ã–µ, –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã–µ | Attention |
Attention | —Ñ–æ–∫—É—Å | ‚Äî | Self-Attention |
Transformer | –≤—Å—ë –≤–Ω–∏–º–∞–Ω–∏–µ | ‚Äî | SOTA |

---

## 52) Key formulas recap

**RNN update**
\[
h_t = f(Wx_t + Uh_{t-1})
\]

**LSTM**
\[
\begin{aligned}
f_t &= \sigma(W_f x_t + U_f h_{t-1})\\
i_t &= \sigma(W_i x_t + U_i h_{t-1})\\
\tilde{c}_t &= \tanh(W_c x_t + U_c h_{t-1})\\
c_t &= f_t * c_{t-1} + i_t * \tilde{c}_t\\
o_t &= \sigma(W_o x_t + U_o h_{t-1})\\
h_t &= o_t * \tanh(c_t)
\end{aligned}
\]

**Attention**
\[
\alpha_{t,i} = \frac{\exp(s_t^\top h_i)}{\sum_j \exp(s_t^\top h_j)}, \quad
c_t = \sum_i \alpha_{t,i} h_i
\]

---

## 53) –ß—Ç–æ –Ω—É–∂–Ω–æ —É–º–µ—Ç—å –ø–æ—Å–ª–µ —ç—Ç–æ–π –ª–µ–∫—Ü–∏–∏

- –û–±—ä—è—Å–Ω–∏—Ç—å language modeling –∏ perplexity
- –ó–∞–ø–∏—Å–∞—Ç—å —Ñ–æ—Ä–º—É–ª—É —Ü–µ–ø–Ω–æ–≥–æ —Ä–∞–∑–ª–æ–∂–µ–Ω–∏—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏
- –û–±—ä—è—Å–Ω–∏—Ç—å BPTT –∏ truncated BPTT
- –ù–∞–ø–∏—Å–∞—Ç—å —Ñ–æ—Ä–º—É–ª—ã RNN/LSTM/GRU
- –ü–æ–Ω–∏–º–∞—Ç—å teacher forcing –∏ exposure bias
- –û–±—ä—è—Å–Ω–∏—Ç—å –∏–¥–µ—é Seq2Seq
- –ü–æ–Ω—è—Ç—å –º–æ—Ç–∏–≤–∞—Ü–∏—é attention

---

## 54) –¢–∏–ø–∏—á–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã –Ω–∞ –ø—Ä–æ–≤–µ—Ä–∫–µ / —ç–∫–∑–∞–º–µ–Ω–µ

- –ß—Ç–æ —Ç–∞–∫–æ–µ language model?
- –ß—Ç–æ —Ç–∞–∫–æ–µ perplexity –∏ –ø–æ—á–µ–º—É PPL ‚Üì –æ–∑–Ω–∞—á–∞–µ—Ç –ª—É—á—à–µ?
- –í —á—ë–º –ø—Ä–æ–±–ª–µ–º–∞ n-gram LM?
- –í —á—ë–º –æ—Ç–ª–∏—á–∏–µ RNN –æ—Ç FFNN –¥–ª—è LM?
- –ß—Ç–æ —Ç–∞–∫–æ–µ vanishing/exploding gradients?
- –ö–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç LSTM? (–Ω–∞–∑–æ–≤–∏ gate-—ã)
- –ó–∞—á–µ–º –Ω—É–∂–µ–Ω teacher forcing?
- –ß—Ç–æ –¥–µ–ª–∞–µ—Ç attention –∏ –∑–∞—á–µ–º –æ–Ω –ø–æ—è–≤–∏–ª—Å—è?

---

## 55) Mini-summary

- LM –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç —Å–ª–µ–¥—É—é—â–µ–µ —Å–ª–æ–≤–æ
- RNN –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—é—Ç —Ç–µ–∫—Å—Ç —Å–ª–µ–≤–∞ –Ω–∞–ø—Ä–∞–≤–æ
- LSTM/GRU —Ä–µ—à–∞—é—Ç –ø—Ä–æ–±–ª–µ–º—É –ø–∞–º—è—Ç–∏
- Seq2Seq –ø–µ—Ä–µ–≤–æ–¥–∏—Ç –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
- Attention = —É–º–µ–Ω–∏–µ —Å–º–æ—Ç—Ä–µ—Ç—å –Ω–∞ –Ω—É–∂–Ω—ã–µ —Å–ª–æ–≤–∞
- Transformers —É–±—Ä–∞–ª–∏ —Ä–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω–æ—Å—Ç—å ‚Üí –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º ‚Üí –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ

---

üéØ **–î–∞–ª—å—à–µ –ø–æ –∫—É—Ä—Å—É: Self-Attention & Transformers**





