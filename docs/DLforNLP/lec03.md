# üìò Lecture 3 ‚Äî Classification & Neural Networks (2024W)

> **–¢–µ–º–∞:** –º–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è (**multiclass classification**), softmax-–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä, **cross-entropy loss**, –ø–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã–µ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏ (**feed-forward neural networks**), backpropagation, autodiff, —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è, –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è.

---

## 0) –ü–æ—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–¥–∞—á–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ (classification setup & notation)

- –î–∞–Ω–æ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ \( \mathcal{X}\subseteq \mathbb{R}^d \), –º–µ—Ç–∫–∏ –∫–ª–∞—Å—Å–æ–≤ \( \mathcal{Y}=\{1,\dots,K\} \).
- –î–∞—Ç–∞—Å–µ—Ç \( \mathcal{D}=\{(x^{(i)},y^{(i)})\}_{i=1}^N \), –≥–¥–µ \( y^{(i)} \in \{1,\dots,K\} \).
- –¶–µ–ª—å: –ø–æ—Å—Ç—Ä–æ–∏—Ç—å —Å–∫–æ—Ä–∏–Ω–≥–æ–≤—É—é —Ñ—É–Ω–∫—Ü–∏—é \( f: \mathbb{R}^d \to \mathbb{R}^K \) –∏ –ø—Ä–∞–≤–∏–ª–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∫–ª–∞—Å—Å–∞  
  \[
  \hat{y}(x) = \arg\max_{k\in\{1,\dots,K\}} f_k(x).
  \]

---

## 1) –ò–Ω—Ç—É–∏—Ü–∏—è softmax-–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞ (softmax classifier intuition)

- –î–ª—è –º–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤–æ–≥–æ —Å–ª—É—á–∞—è —É–¥–æ–±–Ω–æ –Ω–æ—Ä–º–∏—Ä–æ–≤–∞—Ç—å —Å–∫–æ—Ä \( s_k(x) \) –≤ **—Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π** –ø–æ –∫–ª–∞—Å—Å–∞–º:
  \[
  p_\theta(y=k\mid x)=\frac{\exp(s_k(x))}{\sum_{j=1}^K \exp(s_j(x))},\quad
  s(x)=W x + b,\; W\in\mathbb{R}^{K\times d}.
  \]
- **Softmax** –ø—Ä–µ–≤—Ä–∞—â–∞–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω—ã–µ ¬´—Å—ã—Ä—ã–µ¬ª —Å—á—ë—Ç—ã (**logits**) –≤ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏.

---

## 2) –î–µ—Ç–∞–ª–∏ softmax –∏ –∫—Ä–æ—Å—Å-—ç–Ω—Ç—Ä–æ–ø–∏–∏ (details & cross-entropy)

### 2.1 Softmax
\[
\operatorname{softmax}(z)_k=\frac{e^{z_k}}{\sum_{j=1}^K e^{z_j}},\qquad z\in\mathbb{R}^K.
\]

–°–¥–≤–∏–≥ –Ω–∞ –∫–æ–Ω—Å—Ç–∞–Ω—Ç—É –Ω–µ –º–µ–Ω—è–µ—Ç softmax: \(\operatorname{softmax}(z)=\operatorname{softmax}(z+c\mathbf{1})\) ‚Üí –ø–æ–ª–µ–∑–Ω–æ –¥–ª—è —á–∏—Å–ª–µ–Ω–Ω–æ–π —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏.

### 2.2 One-hot –∏ –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ –ª–æ–≥-–ø—Ä–∞–≤–¥–æ–ø–æ–¥–æ–±–∏–µ
–ü—É—Å—Ç—å –∏—Å—Ç–∏–Ω–Ω–∞—è –º–µ—Ç–∫–∞ –∫–∞–∫ one-hot \(y\in\{0,1\}^K\), —Ç–æ–≥–¥–∞ –ª–æ–≥-–ø—Ä–∞–≤–¥–æ–ø–æ–¥–æ–±–∏–µ:
\[
\log p_\theta(y\mid x) = \sum_{k=1}^K y_k \log \operatorname{softmax}(s(x))_k.
\]

### 2.3 –ü–æ—Ç–µ—Ä—è –∫—Ä–æ—Å—Å-—ç–Ω—Ç—Ä–æ–ø–∏–∏ (cross-entropy loss)
\[
\mathcal{L}_{\text{CE}}(x,y)
= -\sum_{k=1}^K y_k \log p_\theta(y=k\mid x)
= -\log p_\theta(y^\star\mid x),
\]
–≥–¥–µ \(y^\star\) ‚Äî –∏—Å—Ç–∏–Ω–Ω—ã–π –∫–ª–∞—Å—Å. –ù–∞ –≤—Å—ë–º –¥–∞—Ç–∞—Å–µ—Ç–µ:
\[
J(\theta) = \frac{1}{N}\sum_{i=1}^{N} \mathcal{L}_{\text{CE}}\big(x^{(i)},y^{(i)}\big).
\]

---

## 3) –ì—Ä–∞–¥–∏–µ–Ω—Ç—ã –¥–ª—è softmax + CE (–∫–ª—é—á–µ–≤–∞—è —Ñ–æ—Ä–º—É–ª–∞)

–û–±–æ–∑–Ω–∞—á–∏–º \(z = s(x)=Wx+b\), \(p=\operatorname{softmax}(z)\).
–î–ª—è –æ–¥–Ω–æ–≥–æ –ø—Ä–∏–º–µ—Ä–∞ –∏ one-hot \(y\):

- –ü–æ –ª–æ–≥–∏—Ç–∞–º:
  \[
  \frac{\partial \mathcal{L}}{\partial z_k} = p_k - y_k.
  \]
- –ü–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º:
  \[
  \frac{\partial \mathcal{L}}{\partial W} = (p - y)\, x^\top,\qquad
  \frac{\partial \mathcal{L}}{\partial b} = p - y.
  \]
- –ü–æ –≤—Ö–æ–¥—É (–¥–ª—è backprop —Å–∫–≤–æ–∑—å —Å–ª–æ–∏):
  \[
  \frac{\partial \mathcal{L}}{\partial x} = W^\top (p - y).
  \]

> –ò–º–µ–Ω–Ω–æ —ç—Ç–∞ –∫–æ–º–ø–∞–∫—Ç–Ω–∞—è —Ñ–æ—Ä–º–∞ ¬´\(p-y\)¬ª ‚Äî –∫—Ä–∞–µ—É–≥–æ–ª—å–Ω—ã–π –∫–∞–º–µ–Ω—å –æ–±—É—á–µ–Ω–∏—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–≤.

**PyTorch-—Å–∫–µ–ª–µ—Ç (–æ–¥–∏–Ω –±–∞—Ç—á):**
```python
import torch
logits = X @ W.T + b        # [B, K]
loss = torch.nn.functional.cross_entropy(logits, y_true)  # y_true: [B] —Å –∏–Ω–¥–µ–∫—Å–∞–º–∏ –∫–ª–∞—Å—Å–æ–≤
loss.backward()             # –∞–≤—Ç–æ–¥–∏—Ñ –ø–æ—Å—á–∏—Ç–∞–µ—Ç p - y –∏ –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã
```
---

## 4) –ù–µ–π—Ä–æ–Ω –∏ –Ω–µ–ª–∏–Ω–µ–π–Ω–æ—Å—Ç—å (artificial neuron & nonlinearity)

### 4.1 –õ–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è –∫–∞–∫ –Ω–µ–π—Ä–æ–Ω (binary)
\[
\hat{y}=\sigma(w^\top x + b),\quad \sigma(t)=\frac{1}{1+e^{-t}}.
\]

### 4.2 –ü–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã–π —Å–ª–æ–π (fully connected / affine layer)
\[
h = f(Wx+b),\quad f\ \text{‚Äî –Ω–µ–ª–∏–Ω–µ–π–Ω–æ—Å—Ç—å (activation)}.
\]

### 4.3 –ó–∞—á–µ–º –Ω—É–∂–Ω–∞ –Ω–µ–ª–∏–Ω–µ–π–Ω–æ—Å—Ç—å
–ë–µ–∑ \(f\) –∫–æ–º–ø–æ–∑–∏—Ü–∏—è —Å–ª–æ—ë–≤ ‚Äî –ª–∏–Ω–µ–π–Ω–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ ‚Üí **–Ω–µ —É–≤–µ–ª–∏—á–∏–≤–∞–µ—Ç –≤—ã—Ä–∞–∑–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å**.
–ü–æ–ø—É–ª—è—Ä–Ω—ã–µ \(f\): **ReLU**, **tanh**, **GELU**, **SiLU/Swish**.

---

## 5) –ü–æ–ª–Ω–æ—Å–≤—è–∑–Ω–∞—è –Ω–µ–π—Ä–æ—Å–µ—Ç—å (feed-forward neural network)

### 5.1 –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ —Å –æ–¥–Ω–∏–º —Å–∫—Ä—ã—Ç—ã–º —Å–ª–æ–µ–º
\[
\begin{aligned}
h &= f(W_1 x + b_1),\quad h\in\mathbb{R}^m,\\
z &= W_2 h + b_2,\quad z\in\mathbb{R}^K,\\
p &= \operatorname{softmax}(z),\quad
\mathcal{L}(x,y)=-\log p_{y^\star}.
\end{aligned}
\]

### 5.2 Backprop (—Ü–µ–ø–Ω–æ–µ –ø—Ä–∞–≤–∏–ª–æ)
–ü—É—Å—Ç—å \(g_z=\partial \mathcal{L}/\partial z = p-y\). –¢–æ–≥–¥–∞
\[
\begin{aligned}
\frac{\partial \mathcal{L}}{\partial W_2} &= g_z\, h^\top,\qquad
\frac{\partial \mathcal{L}}{\partial b_2} = g_z,\\[4pt]
g_h &= W_2^\top g_z,\\
\frac{\partial \mathcal{L}}{\partial W_1} &= (g_h \odot f'(W_1 x+b_1))\, x^\top,\\
\frac{\partial \mathcal{L}}{\partial b_1} &= g_h \odot f'(W_1 x+b_1).
\end{aligned}
\]

### 5.3 PyTorch: –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π MLP-–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä
```python
import torch, torch.nn as nn, torch.nn.functional as F

class MLP(nn.Module):
    def __init__(self, d_in, d_hid, n_classes):
        super().__init__()
        self.fc1 = nn.Linear(d_in, d_hid)
        self.fc2 = nn.Linear(d_hid, n_classes)

    def forward(self, x):
        h = F.relu(self.fc1(x))
        logits = self.fc2(h)
        return logits

model = MLP(d_in=300, d_hid=256, n_classes=10)
opt = torch.optim.Adam(model.parameters(), lr=1e-3)

for Xb, yb in loader:
    opt.zero_grad()
    logits = model(Xb)                # [B, K]
    loss = F.cross_entropy(logits, yb)
    loss.backward()
    opt.step()
```

---

## 6) –ü—Ä–∏–º–µ—Ä: NER –∫–∞–∫ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –ø–æ –æ–∫–Ω—É (windowed NER)

–í **Named Entity Recognition (NER)** –º—ã –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ–º –∫–∞–∂–¥—ã–π —Ç–æ–∫–µ–Ω: –Ω–∞–ø—Ä–∏–º–µ—Ä, LOCATION vs NOT-LOCATION –∏–ª–∏ —Ñ–æ—Ä–º–∞—Ç IOB (B-LOC, I-LOC, O).

**–û–∫–æ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ (window classifier):**

1) –ë–µ—Ä—ë–º –æ–∫–Ω–æ \(2C+1\) —Å–ª–æ–≤ –≤–æ–∫—Ä—É–≥ —Ç–µ–∫—É—â–µ–≥–æ —Ç–æ–∫–µ–Ω–∞ \(w_t\)
2) –ö–æ–Ω–∫–∞—Ç–µ–Ω–∏—Ä—É–µ–º –∏—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
3) –ü–æ–¥–∞—ë–º –≤ MLP + softmax

\[
x_t = [\,e(w_{t-C});\dots;e(w_t);\dots;e(w_{t+C})\,]
\in \mathbb{R}^{(2C+1)d}
\]

\[
p(y_t \mid x_t) = \operatorname{softmax}(W_2 \, f(W_1 x_t + b_1) + b_2)
\]

–ü—Ä–æ—Å—Ç–æ–π, –Ω–æ –º–æ—â–Ω—ã–π –±–∞–∑–æ–≤—ã–π –º–µ—Ç–æ–¥ (–¥–æ RNN/Transformer —ç–ø–æ—Ö–∏).

---

## 7) –í—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–µ –≥—Ä–∞—Ñ—ã –∏ Backprop

–ú–æ–¥–µ–ª—å ‚Äî —ç—Ç–æ **–≥—Ä–∞—Ñ –≤—ã—á–∏—Å–ª–µ–Ω–∏–π**: —É–∑–ª—ã = –æ–ø–µ—Ä–∞—Ü–∏–∏ (matmul, add, exp, log), —Ä—ë–±—Ä–∞ = —Ç–µ–Ω–∑–æ—Ä—ã.

**–¶–µ–ø–Ω–æ–µ –ø—Ä–∞–≤–∏–ª–æ:**

\[
\frac{dq}{dx} = 
\frac{\partial q}{\partial u}\frac{du}{dx} +
\frac{\partial q}{\partial v}\frac{dv}{dx}
\]

**Backpropagation** = –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —Ü–µ–ø–Ω–æ–≥–æ –ø—Ä–∞–≤–∏–ª–∞ –æ—Ç –≤—ã—Ö–æ–¥–∞ –∫ –≤—Ö–æ–¥–∞–º.

### –ê–≤—Ç–æ–¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏—Ä–æ–≤–∞–Ω–∏–µ (autodiff)

- PyTorch/TF/JAX –∏—Å–ø–æ–ª—å–∑—É—é—Ç **reverse-mode AD**
- `loss.backward()` —Å—Ç—Ä–æ–∏—Ç –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏

---

## 8) –ù–µ–ª–∏–Ω–µ–π–Ω–æ—Å—Ç–∏ (activation functions)

| –§—É–Ω–∫—Ü–∏—è | –§–æ—Ä–º—É–ª–∞ | –ü–ª—é—Å—ã | –ú–∏–Ω—É—Å—ã |
|---|---|---|---|
| ReLU | \(f(x)=\max(0,x)\) | –±—ã—Å—Ç—Ä–æ, —Å—Ç–∞–±–∏–ª—å–Ω–æ | dying ReLU |
| tanh | \(\frac{e^x - e^{-x}}{e^x + e^{-x}}\) | —Ü–µ–Ω—Ç—Ä 0 | –∑–∞—Ç—É—Ö–∞—é—â–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã |
| GELU | smooth ReLU-like | —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–∞—è –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞ | –¥–æ—Ä–æ–∂–µ |
| SiLU/Swish | \(x \sigma(x)\) | —á–∞—Å—Ç–æ ‚Üë –∫–∞—á–µ—Å—Ç–≤–æ | –¥–æ—Ä–æ–∂–µ |

---

## 9) –†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è (regularization)

- **L2 / weight decay**: \(\frac{\lambda}{2}\lVert\theta\rVert_2^2\)
- **Dropout**: –º–∞—Å–∫–∏—Ä—É–µ–º —á–∞—Å—Ç—å –∞–∫—Ç–∏–≤–∞—Ü–∏–π

–¢—Ä–µ–Ω–∏—Ä–æ–≤–∫–∞:  
\[
h' = \frac{m \odot h}{1-p},\quad m \sim \text{Bernoulli}(1-p)
\]

–ò–Ω—Ñ–µ—Ä–µ–Ω—Å:  
\[
h' = h
\]

**PyTorch –ø—Ä–∏–º–µ—Ä Dropout:**
```python
class MLPDrop(nn.Module):
    def __init__(self, d_in, d_hid, n_classes, p=0.5):
        super().__init__()
        self.fc1 = nn.Linear(d_in, d_hid)
        self.drop = nn.Dropout(p)
        self.fc2 = nn.Linear(d_hid, n_classes)

    def forward(self, x):
        h = F.relu(self.fc1(x))
        h = self.drop(h)
        return self.fc2(h)
```
---

## 10) –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è (vectorization)

–ò–∑–±–µ–≥–∞–µ–º Python-—Ü–∏–∫–ª–æ–≤ –ø–æ –æ–¥–Ω–æ–º—É –ø—Ä–∏–º–µ—Ä—É ‚Üí –∏—Å–ø–æ–ª—å–∑—É–µ–º **batch**-–≤—ã—á–∏—Å–ª–µ–Ω–∏—è.

**–ú–µ–¥–ª–µ–Ω–Ω–æ (–ø–ª–æ—Ö–æ):**
```python
import torch
import torch.nn.functional as F

N, d, K = 1024, 300, 10
X = torch.randn(N, d)
W = torch.randn(K, d)
b = torch.randn(K)
y = torch.randint(0, K, (N,))

loss_sum = 0.0
for i in range(N):
    logits_i = X[i] @ W.T + b
    loss_i = F.cross_entropy(logits_i.unsqueeze(0), y[i].unsqueeze(0))
    loss_sum += loss_i.item()
```

**–ë—ã—Å—Ç—Ä–æ (–ø—Ä–∞–≤–∏–ª—å–Ω–æ):**

```python
import torch
import torch.nn.functional as F

N, d, K = 1024, 300, 10
X = torch.randn(N, d)
W = torch.randn(K, d)
b = torch.randn(K)
y = torch.randint(0, K, (N,))

logits = X @ W.T + b           # [N, K]
loss = F.cross_entropy(logits, y)
loss.backward()
```

## 11) –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ (parameter initialization)

–°–ª–∏—à–∫–æ–º –º–∞–ª–µ–Ω—å–∫–∞—è –¥–∏—Å–ø–µ—Ä—Å–∏—è ‚Üí –∑–∞—Ç—É—Ö–∞—é—â–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã.  
–°–ª–∏—à–∫–æ–º –±–æ–ª—å—à–∞—è ‚Üí –≤–∑—Ä—ã–≤—ã –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ / –Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è.

### Xavier / Glorot (–¥–ª—è tanh/linear)

\[
W_{ij} \sim \mathcal{U}\!\Big(
-\sqrt{\frac{6}{n_{\text{in}} + n_{\text{out}}}},
\ \sqrt{\frac{6}{n_{\text{in}} + n_{\text{out}}}}
\Big)
\]

### He / Kaiming (–¥–ª—è ReLU)

```python
import torch.nn as nn

def init_kaiming_relu(m: nn.Module):
    if isinstance(m, nn.Linear):
        nn.init.kaiming_uniform_(m.weight, nonlinearity='relu')
        nn.init.zeros_(m.bias)
```

### –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:
```python
model.apply(init_kaiming_relu)
```

## 12) –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è (optimizers & LR schedules)

–û—Å–Ω–æ–≤–Ω—ã–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä—ã:

- **SGD + momentum**
- **Adam / AdamW** (—á–∞—Å—Ç–æ –ª—É—á—à–∏–π —Å—Ç–∞—Ä—Ç)
- **RMSProp**

–ü–æ–ª–µ–∑–Ω—ã–µ —Ñ–∏—á–∏ –æ–±—É—á–µ–Ω–∏—è:

- **Weight decay** (L2-—Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è)
- **Gradient clipping**
- **Learning rate scheduling**
  - cosine decay
  - warmup
  - step decay

### AdamW + Cosine LR –ø—Ä–∏–º–µ—Ä

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.optim import AdamW
from torch.optim.lr_scheduler import CosineAnnealingLR

class MLP(nn.Module):
    def __init__(self, d_in, d_hid, n_classes):
        super().__init__()
        self.fc1 = nn.Linear(d_in, d_hid)
        self.fc2 = nn.Linear(d_hid, n_classes)

    def forward(self, x):
        x = F.relu(self.fc1(x))
        return self.fc2(x)

model = MLP(d_in=300, d_hid=256, n_classes=10)
opt = AdamW(model.parameters(), lr=2e-3, weight_decay=1e-2)
sched = CosineAnnealingLR(opt, T_max=50)

for epoch in range(50):
    for Xb, yb in loader:
        opt.zero_grad()
        logits = model(Xb)
        loss = F.cross_entropy(logits, yb)
        loss.backward()
        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        opt.step()
    sched.step()
```

## 13) –ü—Ä–∞–∫—Ç–∏–∫—É–º: –æ—Ç –Ω—É–ª—è –¥–æ —Ä–∞–±–æ—Ç–∞—é—â–µ–≥–æ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞

–®–∞–≥–∏ –ø–∞–π–ø–ª–∞–π–Ω–∞:

1. –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞—Ç—å –≤—Ö–æ–¥—ã (standardization)
2. –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞: `d_in ‚Üí d_hid ‚Üí K`, ReLU, dropout
3. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è He (Kaiming) –¥–ª—è ReLU
4. Optimizer: **AdamW** + **cosine LR scheduler**
5. –°–ª–µ–¥–∏—Ç—å –∑–∞:
   - `train loss`
   - `val loss`
   - `accuracy`
   - **early stopping**
6. –ü—Ä–æ–≤–µ—Ä–∏—Ç—å:
   - confusion matrix
   - –¥–∏—Å–±–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤ (class imbalance)
     - weighted loss / oversampling / focal loss

### –§—É–Ω–∫—Ü–∏—è –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è accuracy

```python
import torch
from sklearn.metrics import accuracy_score

def evaluate(model, loader):
    model.eval()
    preds, gold = [], []
    with torch.no_grad():
        for Xb, yb in loader:
            logits = model(Xb)
            preds.append(logits.argmax(dim=1).cpu())
            gold.append(yb.cpu())

    preds = torch.cat(preds).numpy()
    gold = torch.cat(gold).numpy()
    return accuracy_score(gold, preds)
```

## 14) –§–æ—Ä–º—É–ª—ã (cheat-sheet)

**Softmax**
\[
\operatorname{softmax}(z)_k \;=\; \frac{e^{z_k}}{\sum_{j=1}^{K} e^{z_j}}
\]

**–ö—Ä–æ—Å—Å-—ç–Ω—Ç—Ä–æ–ø–∏—è (one-hot)**
\[
\mathcal{L}(x,y) \;=\; -\sum_{k=1}^{K} y_k \,\log \operatorname{softmax}(z)_k
\;=\; -\log p_{y^\star}
\]

**–ö–ª—é—á–µ–≤–æ–π –≥—Ä–∞–¥–∏–µ–Ω—Ç –ø–æ –ª–æ–≥–∏—Ç–∞–º**
\[
\frac{\partial \mathcal{L}}{\partial z} \;=\; p - y
\]

**–õ–∏–Ω–µ–π–Ω—ã–π —Å–ª–æ–π**
\[
z = Wx + b, \qquad
\frac{\partial \mathcal{L}}{\partial W} = (p-y)\,x^{\top}, \qquad
\frac{\partial \mathcal{L}}{\partial b} = p-y, \qquad
\frac{\partial \mathcal{L}}{\partial x} = W^{\top}(p-y)
\]

**Backprop —á–µ—Ä–µ–∑ ReLU**
\[
h = \operatorname{ReLU}(a)=\max(0,a), \quad
\frac{\partial \mathcal{L}}{\partial a} \;=\; \frac{\partial \mathcal{L}}{\partial h}\;\odot\;\mathbf{1}_{a>0}
\]

**–ü–æ–ª–Ω–æ—Å–≤—è–∑–Ω–∞—è —Å–µ—Ç—å (1 —Å–∫—Ä—ã—Ç—ã–π —Å–ª–æ–π)**
\[
\begin{aligned}
h &= f(W_1 x + b_1),\\
z &= W_2 h + b_2,\\
p &= \operatorname{softmax}(z),\\
\mathcal{L} &= -\log p_{y^\star}
\end{aligned}
\]

–ì—Ä–∞–¥–∏–µ–Ω—Ç—ã:
\[
\begin{aligned}
g_z &= \frac{\partial \mathcal{L}}{\partial z} = p - y,\\
\frac{\partial \mathcal{L}}{\partial W_2} &= g_z\, h^\top,\qquad
\frac{\partial \mathcal{L}}{\partial b_2} = g_z,\\
g_h &= W_2^\top g_z,\\
\frac{\partial \mathcal{L}}{\partial W_1} &= \big(g_h \odot f'(W_1x+b_1)\big)\,x^\top, \qquad
\frac{\partial \mathcal{L}}{\partial b_1} = g_h \odot f'(W_1x+b_1)
\end{aligned}
\]

**L2-—Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è (weight decay)**
\[
\mathcal{L}_{\text{total}} \;=\; \mathcal{L}_{\text{task}} \;+\; \frac{\lambda}{2}\,\lVert \theta \rVert_2^2
\]

**Cosine similarity (–Ω–∞ –≤—Å—è–∫–∏–π —Å–ª—É—á–∞–π)**
\[
\cos(\mathbf{a},\mathbf{b}) \;=\; \frac{\mathbf{a}^\top \mathbf{b}}{\lVert \mathbf{a}\rVert_2\,\lVert \mathbf{b}\rVert_2}
\]

## 15) –õ–∏—Ç–µ—Ä–∞—Ç—É—Ä–∞ –∏ –ø–æ–ª–µ–∑–Ω—ã–µ –º–∞—Ç–µ—Ä–∏–∞–ª—ã

**–ë–∞–∑–æ–≤—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏**

- **Rumelhart, Hinton, Williams (1986)**  
  *Learning Representations by Backpropagating Errors*  
  ‚Äî –ö–ª–∞—Å—Å–∏—á–µ—Å–∫–∞—è —Å—Ç–∞—Ç—å—è, –≥–¥–µ –≤–ø–µ—Ä–≤—ã–µ –æ–ø–∏—Å–∞–Ω backprop –¥–ª—è –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π.

- **Collobert, Weston et al. (2011)**  
  *Natural Language Processing (Almost) from Scratch*  
  ‚Äî –ü–µ—Ä–≤–∞—è –º–æ—â–Ω–∞—è –Ω–µ–π—Ä–æ—Å–µ—Ç–æ—á–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –¥–ª—è NLP, –±–µ–∑ —Ä—É—á–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤.

- **Baydin et al. (2015)**  
  *Automatic Differentiation in Machine Learning: A Survey*  
  ‚Äî –ì–ª—É–±–æ–∫–∏–π –æ–±–∑–æ—Ä –º–µ—Ç–æ–¥–æ–≤ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏—Ä–æ–≤–∞–Ω–∏—è (autodiff).

- **Karpathy (2016)**  
  *Yes, you should understand backprop*  
  ‚Äî –û—Ç–ª–∏—á–Ω–æ–µ –∏–Ω—Ç—É–∏—Ç–∏–≤–Ω–æ–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ backprop, MUST READ.

---

**–ß—Ç–æ –µ—â—ë –ø–æ—á–∏—Ç–∞—Ç—å/–ø–æ—Å–º–æ—Ç—Ä–µ—Ç—å**

- Goodfellow, Bengio, Courville ‚Äî *Deep Learning Book*, –≥–ª. 6‚Äì8.
- CS231n (Stanford): –ª–µ–∫—Ü–∏–∏ –ø–æ backprop, softmax, SGD.
- PyTorch Tutorials: *Autograd & Optimization*
- Karpathy‚Äôs micrograd ‚Äî –º–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è backprop:
  https://github.com/karpathy/micrograd  

---

**–ö–ª—é—á–µ–≤—ã–µ takeaway-–ø—É–Ω–∫—Ç—ã –ª–µ–∫—Ü–∏–∏**

- Softmax + Cross-entropy ‚Äî —Å—Ç–∞–Ω–¥–∞—Ä—Ç –¥–ª—è –º–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
- Backprop = –ø—Ä–æ—Å—Ç–æ —Ü–µ–ø–Ω–æ–µ –ø—Ä–∞–≤–∏–ª–æ –Ω–∞ –≥—Ä–∞—Ñ–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏–π
- ReLU —Ä–∞–±–æ—Ç–∞–µ—Ç –ª—É—á—à–µ tanh –¥–ª—è –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–∞ –∑–∞–¥–∞—á
- Weight decay && dropout ‚Üí –∫–æ–Ω—Ç—Ä–æ–ª—å –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è
- –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è > —Ü–∏–∫–ª—ã —Ä—É–∫–∞–º–∏
- AdamW + cosine schedule ‚Äî —Å–∏–ª—å–Ω—ã–π –±–∞–∑–æ–≤—ã–π —Å–µ—Ç–∞–ø
- –•–æ—Ä–æ—à–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è = —Å—Ç–∞–±–∏–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ

---

**–ß—Ç–æ –∑–Ω–∞—Ç—å –∫ —Å–ª–µ–¥—É—é—â–µ–π –ª–µ–∫—Ü–∏–∏**

- –ö–∞–∫ —Å—á–∏—Ç–∞–µ—Ç—Å—è gradient = (p ‚àí y)
- –ö–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç `loss.backward()` –≤ PyTorch
- –ß—Ç–æ —Ç–∞–∫–æ–µ hidden layer –∏ –∑–∞—á–µ–º –Ω–µ–ª–∏–Ω–µ–π–Ω–æ—Å—Ç–∏
- –í —á—ë–º —Å–º—ã—Å–ª weight decay –∏ dropout
- –ü–æ—á–µ–º—É –±–∞—Ç—á–∏ —É—Å–∫–æ—Ä—è—é—Ç –æ–±—É—á–µ–Ω–∏–µ –¥–∞–∂–µ –Ω–∞ CPU
- –ö–∞–∫ –ø–æ–Ω—è—Ç—å, —á—Ç–æ –º–æ–¥–µ–ª—å overfitting/underfitting

---
